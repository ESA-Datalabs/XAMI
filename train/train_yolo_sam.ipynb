{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "eotMLol5O5G0",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0, 1, 2, 3\" # replace with the GPU IDs that are available\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = \"1\"\n",
    "\n",
    "# to help with reproducibility (inspired from YOLO init_seeds function)\n",
    "seed=0\n",
    "import torch.backends.cudnn as cudnn \n",
    "random.seed(seed) \n",
    "np.random.seed(seed) \n",
    "torch.manual_seed(seed) \n",
    "cudnn.benchmark, cudnn.deterministic = (False, True) if seed == 0 else (True, False) \n",
    "\n",
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "np.set_printoptions(precision=9)\n",
    "\n",
    "sys.path.append('../') # workaround for lack of relative import in notebooks\n",
    "from sam_predictor import predictor_utils, astro_sam #, residualAttentionBlock\n",
    "from losses import metrics_utils\n",
    "from dataset import dataset_utils, coco_to_yolo_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_iter=0\n",
    "\n",
    "device_id=3\n",
    "batch_size=8\n",
    "lr=3e-5\n",
    "wd=0.0005\n",
    "wandb_track=True\n",
    "# torch.cuda.set_device(device_id)\n",
    "\n",
    "if wandb_track:\n",
    "    from datetime import datetime\n",
    "    # !pip install wandb\n",
    "    # !wandb login --relogin\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"yolo-sam\", name=f\"yolo-sam {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert = True # if True, convert coco to yolo format\n",
    "\n",
    "if convert: \n",
    "    coco_to_yolo_converter.convert_coco_to_yolo(\n",
    "    dir_absolute_path='/workspace/raid/test/XAMI/',\n",
    "    dataset_path='../xami_dataset/',\n",
    "    yolo_dataset_path='../xami_dataset_YOLO/'\n",
    ")\n",
    "else:\n",
    "    yolo_dataset_path = f'../xami_dataset_YOLO/' # replace with the path to the YOLO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(yolo_dataset_path+f\"data.yaml\", 'r') as stream:\n",
    "    yam_data = yaml.safe_load(stream) # dictionary with keys 'names', 'nc', 'train', 'val'\n",
    "\n",
    "classes = {i:name for i, name in enumerate(yam_data['names'])}\n",
    "train_path = yam_data['train']\n",
    "val_path = yam_data['val']\n",
    "print(classes)\n",
    "\n",
    "coco_data_path = f'../../XAMI-dataset/notebooks/mskf_{kfold_iter}/'\n",
    "annotations_file = '_annotations.coco.json'\n",
    "\n",
    "with open(coco_data_path+'train/'+annotations_file, 'r') as f1, open(coco_data_path+'valid/'+annotations_file, 'r') as f2:\n",
    "    train_coco_data = json.load(f1)\n",
    "    valid_coco_data = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = yolo_dataset_path+f'train/images/'\n",
    "valid_dir = yolo_dataset_path+f'valid/images/'\n",
    "\n",
    "train_image_files = os.listdir(train_dir)\n",
    "valid_image_files = os.listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(sys.path[0]+'/../mobile_sam/')\n",
    "from mobile_sam import sam_model_registry, SamPredictor\n",
    "\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobile_sam_checkpoint = '../train/weights/sam_weights/sam_0_best.pth'\n",
    "# mobile_sam_checkpoint = '../train/weights/yolo_sam_final.pth'\n",
    "\n",
    "yolov8_pretrained_model = YOLO(f'../train/weights/yolo_weights/best.pt');\n",
    "yolov8_pretrained_model.to(f'cuda:{device_id}');\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device)\n",
    "predictor = SamPredictor(mobile_sam_model)\n",
    "\n",
    "# # Residual Attention Block\n",
    "# residual_block = residualAttentionBlock.ResidualAttentionBlock(d_model=256, n_head=8, mlp_ratio=4.0).to(device)\n",
    "# residual_block.load_state_dict(torch.load('../output_sam/residual_attn_blk_2024-05-10 08:05:54.498084_best.pth'))\n",
    "\n",
    "# for name, param in residual_block.named_parameters():\n",
    "# \tparam.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astrosam_model = astro_sam.AstroSAM(mobile_sam_model, device, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    params_to_train = ['mask_tokens', 'output_upscaling', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
    "    if 'mask_decoder' in name: # and any(s in name for s in params_to_train):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "print(f\"ðŸš€ The model has {sum(p.numel() for p in astrosam_model.model.parameters() if p.requires_grad)} trainable parameters.\")\n",
    "# print(f\"ðŸš€ The residual attention block has {sum(p.numel() for p in astrosam_model.residualAttentionBlock.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "\n",
    "# predictor_utils.check_requires_grad(astrosam_model.model)\n",
    "# predictor_utils.check_requires_grad(astrosam_model.residualAttentionBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_num_batches = len(train_image_files) // batch_size\n",
    "valid_num_batches = len(valid_image_files) // batch_size\n",
    "parameters_to_optimize = [param for param in mobile_sam_model.mask_decoder.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(parameters_to_optimize, lr=lr, weight_decay=wd) if len(parameters_to_optimize) > 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "\n",
    "metric_thresholds = [[0.5], [0.75], [0.5, 0.9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10 \n",
    "n_epochs_stop = 100 # 5+ num_epochs//10\n",
    "all_metrics = defaultdict(dict)\n",
    "compute_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Intro\n",
    "predictor_utils.print_training_intro(\n",
    "    train_image_files, valid_image_files, device, metric_thresholds, num_epochs, \n",
    "    batch_size, lr, wd, wandb_track, mobile_sam_model, 'AdamW')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train\n",
    "    astrosam_model.model.train()\n",
    "    # astrosam_model.model.eval()\n",
    "    \n",
    "    # astrosam_model.residualAttentionBlock.train()\n",
    "    train_results = astrosam_model.run_yolo_sam_epoch(\n",
    "        yolov8_pretrained_model,\n",
    "        phase='train',\n",
    "        batch_size=batch_size, \n",
    "        image_files=train_image_files, \n",
    "        images_dir=train_dir, \n",
    "        num_batches=train_num_batches,\n",
    "        optimizer=optimizer) \n",
    "    \n",
    "    epoch_sam_loss_train, train_preds, train_gts = train_results[:3]\n",
    "    train_gt_classes, train_pred_classes, train_all_iou_scores, train_mask_areas, _ = train_results[3:]\n",
    "\n",
    "    # Validate\n",
    "    astrosam_model.model.eval()\n",
    "    # astrosam_model.residualAttentionBlock.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_results = astrosam_model.run_yolo_sam_epoch(\n",
    "            yolov8_pretrained_model,\n",
    "            phase='val',\n",
    "            batch_size=batch_size, \n",
    "            image_files=valid_image_files, \n",
    "            images_dir=valid_dir, \n",
    "            num_batches=valid_num_batches,\n",
    "            optimizer=None) \n",
    "\n",
    "    epoch_sam_loss_val, valid_preds, valid_gts = valid_results[:3]\n",
    "    valid_gt_classes, valid_pred_classes, valid_all_iou_scores, valid_mask_areas, pred_images = valid_results[3:]\n",
    "\n",
    "    # Early stopping                                                                                \n",
    "    if epoch_sam_loss_val < best_valid_loss:\n",
    "        best_valid_loss = epoch_sam_loss_val\n",
    "        best_model = astrosam_model.model\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print(\"Early stopping initiated.\")\n",
    "            early_stop = True\n",
    "            break\n",
    "        \n",
    "    # Metrics\n",
    "    if compute_metrics:\n",
    "        for threshold in tqdm(metric_thresholds, desc=\"(Metrics) Processing thresholds\", bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'):\n",
    "            metric = MeanAveragePrecision(\n",
    "            iou_type = \"segm\", \n",
    "            iou_thresholds = threshold, \n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True,\n",
    "            extended_summary=False)\n",
    "            \n",
    "            train_metrics = metrics_utils.mAP_metrics(metric,\n",
    "            train_preds,\n",
    "            train_gts, \n",
    "            train_gt_classes, \n",
    "            train_pred_classes, \n",
    "            train_all_iou_scores,\n",
    "            train_mask_areas,\n",
    "            show_metrics=False) \n",
    "                \n",
    "            valid_metrics = metrics_utils.mAP_metrics(metric,\n",
    "                    valid_preds,\n",
    "                    valid_gts, \n",
    "                    valid_gt_classes, \n",
    "                    valid_pred_classes, \n",
    "                    valid_all_iou_scores,\n",
    "                    valid_mask_areas,\n",
    "                    show_metrics=False)\n",
    "        \n",
    "            all_metrics[tuple(threshold)] = {'train': train_metrics, 'valid': valid_metrics}\n",
    "\n",
    "        # Prints\n",
    "        if not wandb_track:\n",
    "            wandb = None\n",
    "        predictor_utils.prints_and_wandb(\n",
    "            epoch, \n",
    "            epoch_sam_loss_train, \n",
    "            epoch_sam_loss_val, \n",
    "            all_metrics, \n",
    "            metric_thresholds,\n",
    "            wandb)\n",
    "\n",
    "#     # Checkpoint save\n",
    "#     torch.save(best_model.state_dict(), f'./output_sam/yolo_sam_{epoch}.pth')\n",
    "\n",
    "# torch.save(best_model.state_dict(), f'./output_sam/yolo_sam_final.pth')\n",
    "\n",
    "# Finish wandb run\n",
    "if wandb_track:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 12. Train loss: 0.390036\n",
    "# Epoch 12. Validation loss: 0.3961745\n",
    "# Train mAP50: tensor(0.9316) Train mAP75: tensor(0.7344) Train mAP50-90: tensor(0.6263)\n",
    "# Valid mAP50: tensor(0.8694) Valid mAP75: tensor(0.6692) Valid mAP50-90: tensor(0.5737)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 16. Train loss: 0.3887573\n",
    "# Epoch 16. Validation loss: 0.3952929\n",
    "# Train mAP50: tensor(0.9238) Train mAP75: tensor(0.7429) Train mAP50-90: tensor(0.6288)\n",
    "# Valid mAP50: tensor(0.8665) Valid mAP75: tensor(0.6799) Valid mAP50-90: tensor(0.5777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 12. Train loss: 0.3895514\n",
    "# Epoch 12. Validation loss: 0.3958057\n",
    "# Train mAP50: tensor(0.9384) Train mAP75: tensor(0.7349) Train mAP50-90: tensor(0.6285)\n",
    "# Valid mAP50: tensor(0.8759) Valid mAP75: tensor(0.6678) Valid mAP50-90: tensor(0.5741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ious_pred_vs_gt(gts, preds):\n",
    "    all_ious_pred_vs_gt_flatten = []\n",
    "    all_ious_pred_vs_gt = []\n",
    "    \n",
    "    for i in range(len(gts)):\n",
    "        iou_image_scores = []\n",
    "        for gt_mask, pred_mask in zip(gts[i], preds[i]):\n",
    "            intersection = np.logical_and(gt_mask, pred_mask)\n",
    "            union = np.logical_or(gt_mask, pred_mask)\n",
    "            iou_score = np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 0\n",
    "            iou_image_scores.append(iou_score)\n",
    "            all_ious_pred_vs_gt_flatten.append(iou_score)\n",
    "        all_ious_pred_vs_gt.append(iou_image_scores)\n",
    "        \n",
    "    return all_ious_pred_vs_gt_flatten, all_ious_pred_vs_gt\n",
    "\n",
    "train_all_ious_pred_vs_gt, train_ious_pred_vs_gt = ious_pred_vs_gt(train_gts, train_preds)\n",
    "valid_all_ious_pred_vs_gt, valid_ious_pred_vs_gt = ious_pred_vs_gt(valid_gts, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IoUs frequency\n",
    "\n",
    "train_all_ious_flatten, train_mask_areas_flatten = metrics_utils.flatten_ious_areas(train_pred_classes, train_all_iou_scores, train_mask_areas)\n",
    "valid_all_ious_flatten, valid_mask_areas_flatten = metrics_utils.flatten_ious_areas(valid_pred_classes, valid_all_iou_scores, valid_mask_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(valid_all_ious_pred_vs_gt, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Segmentation Predicted IoUs')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams.update({'font.size': 14, 'axes.labelsize': 16, 'axes.titlesize': 18, 'xtick.labelsize': 12, 'ytick.labelsize': 12, 'legend.fontsize': 14, 'font.family':'sans-serif'})\n",
    "\n",
    "ious_train = train_all_ious_pred_vs_gt\n",
    "ious_valid = valid_all_ious_pred_vs_gt\n",
    "areas_train = train_mask_areas_flatten\n",
    "areas_valid = valid_mask_areas_flatten\n",
    "\n",
    "vals_train, base_train = np.histogram(ious_train, bins=1000, range=(0, 1), density=True)\n",
    "cumulative_train = np.cumsum(vals_train * np.diff(base_train))\n",
    "vals_valid, base_valid = np.histogram(ious_valid, bins=1000, range=(0, 1), density=True)\n",
    "cumulative_valid = np.cumsum(vals_valid * np.diff(base_valid))\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(base_train[:-1], cumulative_train, label='Train', color='magenta')\n",
    "plt.plot(base_valid[:-1], cumulative_valid, label='Validation', color='blue')\n",
    "\n",
    "quartiles_y = np.quantile(cumulative_train, [0.8, 0.9])\n",
    "quartiles_x = np.interp(quartiles_y, cumulative_train, base_train[:-1])\n",
    "\n",
    "for qx, qy in zip(quartiles_x, quartiles_y):\n",
    "    plt.hlines(qy, xmin=0, xmax=qx, colors='grey', linestyles='dashed')\n",
    "    plt.vlines(qx, ymin=0, ymax=qy, colors='grey', linestyles='dashed')\n",
    "    plt.text(qx, qy, f'({qx:.2f}, {qy:.2f})', fontsize=18, verticalalignment='bottom', horizontalalignment='right')\n",
    "\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel('IoU', fontsize=22)\n",
    "plt.ylabel('Cumulative Frequency', fontsize=22)\n",
    "plt.legend(fontsize=18, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/cdf_ious.png', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "#D81B60\n",
    "#1E88E5\n",
    "\n",
    "#1A85FF\n",
    "#D41159\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(np.log(areas_train), ious_train, alpha=0.5, label='Train', color='magenta')\n",
    "plt.scatter(np.log(areas_valid), ious_valid, alpha=0.4, label='Validation', color='blue')\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "# x_ticks_positions = np.array([20000, 40000, 60000])*(np.max(np.log(areas_train))- np.min(np.log(areas_train)))/(np.max(areas_train)-np.min(areas_train))   # show orignal ticks but log plot\n",
    "# x_ticks_labels = ['20000', '40000', '60000']\n",
    "\n",
    "# plt.xticks(x_ticks_positions, x_ticks_labels, fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.xlabel('Mask area [log(px)]', fontsize=22)\n",
    "plt.ylabel('IoU', fontsize=22)\n",
    "plt.legend(fontsize=18, loc='lower right', bbox_to_anchor=(1, 0.04))\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/ious_and_areas_log.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(areas_train), np.max(areas_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ious_train), np.min(ious_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ious_train), np.max(ious_train), np.min(ious_valid), np.max(ious_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU TP FP FN plot (~29s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_range(gt_masks, pred_masks, iou_thresholds, image=None):\n",
    "    \"\"\"Compute the True Positive, False Positive, and False Negative BINARY masks for multiple segmentations.\"\"\"\n",
    "    \n",
    "    combined_gt_mask = np.zeros_like(gt_masks[0][0], dtype=bool)\n",
    "    combined_pred_mask = np.zeros_like(pred_masks[0], dtype=bool)\n",
    "    filtered_pred_masks = np.zeros_like(pred_masks, dtype=bool)\n",
    "      \n",
    "    for gt_mask in gt_masks:\n",
    "        combined_gt_mask = np.logical_or(combined_gt_mask, gt_mask)\n",
    "\n",
    "    for pred_mask in pred_masks:\n",
    "        combined_pred_mask = np.logical_or(combined_pred_mask, pred_mask.astype(bool))\n",
    "\n",
    "    intersection = np.sum(np.logical_and(combined_gt_mask, combined_pred_mask))\n",
    "    union = np.sum(np.logical_or(combined_gt_mask, combined_pred_mask))\n",
    "\n",
    "    iou = intersection/union\n",
    "    \n",
    "    true_positive_mask = np.logical_and(combined_gt_mask, combined_pred_mask)\n",
    "    false_negative_mask = np.logical_and(combined_gt_mask, np.logical_not(combined_pred_mask))\n",
    "    false_positive_mask = np.logical_and(combined_pred_mask, np.logical_not(combined_gt_mask))\n",
    "\n",
    "    return true_positive_mask, false_positive_mask, false_negative_mask, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import style\n",
    "\n",
    "random_indices = random.sample(range(len(pred_images)), 8)\n",
    "# random_indices = [247, 77,232, 111,     66, 204, 64, 79] #111, 133, 205\n",
    "# random_indices = [140, 106, 136, 44, 197, 152, 7, 46]\n",
    "# 140 106 136 44\n",
    "#   7 \n",
    "images = [pred_images[i] for i in random_indices]\n",
    "selected_gts = [valid_gts[i] for i in random_indices]\n",
    "selected_preds = [valid_preds[i] for i in random_indices]\n",
    "selected_ious = [valid_ious_pred_vs_gt[i] for i in random_indices]\n",
    "\n",
    "print(\"Indices:\", random_indices, \"Images:\", images)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(36, 18))  # Only 2 rows, 4 columns\n",
    "\n",
    "for j, (image_name, gt_mask, pred_mask, selected_iou) in enumerate(zip(images, selected_gts, selected_preds, selected_ious)):\n",
    "    row, col = divmod(j, 4)\n",
    "    image_id = [img['id'] for img in valid_coco_data['images'] if img['file_name'].startswith(image_name.split(\".\")[0])][0]\n",
    "    dataset_images = valid_coco_data['images']\n",
    "    image_filename = [dataset_images[i]['file_name'] for i in range(len(dataset_images)) if dataset_images[i]['id']==image_id][0]\n",
    "    gt_masks = np.array([dataset_utils.create_mask(valid_coco_data['annotations'][i]['segmentation'][0], (512, 512)) \\\n",
    "                         for i in range(len(valid_coco_data['annotations']))\\\n",
    "                      if valid_coco_data['annotations'][i]['image_id']==image_id])\n",
    "    \n",
    "    left_limit = np.mean(selected_iou) - 0.1\n",
    "    right_limit = np.mean(selected_iou) + 0.05\n",
    "    thresholds = [[0.6, 0.7], [0.8, 0.9]]\n",
    "\n",
    "    gt_masks = np.array(gt_masks)\n",
    "    gt_masks = gt_masks[:, None, :, :]\n",
    "\n",
    "    image = cv2.imread(valid_dir + image_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # _, _, _, iou = compute_metrics_with_range(gt_masks, pred_mask, [0.6, 0.7], image)\n",
    "    # if 0.6 < iou < 0.7 or 0.8 < iou < 0.9:\n",
    "    #     print(j,  iou)\n",
    "    # continue\n",
    "    \n",
    "    # Display the original image\n",
    "    ax_simple = axes[row, col]\n",
    "    ax_simple.imshow(255-image)\n",
    "    ax_simple.set_xticks([])\n",
    "    ax_simple.set_yticks([])\n",
    "    ax_simple.set_title(f'{image_name.split(\".\")[0].replace(\"_png\", \"\")}', fontsize=45, pad=10)\n",
    "\n",
    "    for spine in ax_simple.spines.values():\n",
    "        spine.set_linewidth(0)\n",
    "        spine.set_edgecolor('white')\n",
    "\n",
    "    # Display the true positive, false positive, and false negative masks\n",
    "    tp_color = np.array([0/255, 255/255, 0/255, 0.4])\n",
    "    fp_color = np.array([255/255, 0/255, 0/255, 0.4])\n",
    "    fn_color = np.array([0/255, 0/255, 255/255, 0.4])\n",
    "    \n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        true_positive_mask, false_positive_mask, false_negative_mask, iou = compute_metrics_with_range(gt_masks, pred_mask, threshold, image)\n",
    "        print(iou)\n",
    "        if j<4: \n",
    "            left_limit = 0.6\n",
    "            right_limit = 0.7\n",
    "        else: \n",
    "            left_limit = 0.8\n",
    "            right_limit = 0.9\n",
    "            \n",
    "        # if left_limit < iou and iou < right_limit:\n",
    "        h, w = true_positive_mask.shape[-2:]\n",
    "        true_positive_mask = true_positive_mask.reshape(h, w, 1) * tp_color.reshape(1, 1, -1)\n",
    "        false_positive_mask = false_positive_mask.reshape(h, w, 1) * fp_color.reshape(1, 1, -1)\n",
    "        false_negative_mask = false_negative_mask.reshape(h, w, 1) * fn_color.reshape(1, 1, -1)\n",
    "        ax_simple.imshow(false_positive_mask)\n",
    "        ax_simple.imshow(false_negative_mask)\n",
    "        ax_simple.imshow(true_positive_mask)\n",
    "        ax_simple.text(20, 480, f'IoU ~ {iou:.2f}', fontsize=30, color='white', bbox=dict(facecolor='black', alpha=0.8))\n",
    "\n",
    "        if j==0 or j==4:\n",
    "            ax_simple.set_ylabel(f'{np.round(left_limit, 2)} < IoU < {np.round(right_limit, 2)}', fontsize=45)\n",
    "\n",
    "print(\"\\nPlotting...\")\n",
    "tp_patch = mpatches.Patch(color=tp_color, label='True Positive', alpha=0.6)\n",
    "fp_patch = mpatches.Patch(color=fp_color, label='False Positive', alpha=0.6)\n",
    "fn_patch = mpatches.Patch(color=fn_color, label='False Negative', alpha=0.6)\n",
    "\n",
    "fig.legend(handles=[tp_patch, fp_patch, fn_patch], loc='lower center', bbox_to_anchor=(0.5, -0.003), ncol=3, fontsize=30)\n",
    "plt.tight_layout(pad=0.1)\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.99, bottom=0.05, hspace=0.01, wspace=0.01)\n",
    "plt.savefig('../plots/iou_tp_fp_fn_images.png', bbox_inches='tight', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import matplotlib.patches as mpatches\n",
    "# from matplotlib import style\n",
    "\n",
    "# random_indices = random.sample(range(len(pred_images)), 5)\n",
    "# random_indices = [45, 75, 141, 116, 87] # 6,\n",
    "# images = [pred_images[i] for i in random_indices]\n",
    "# selected_gts = [valid_gts[i] for i in random_indices]\n",
    "# selected_preds = [valid_preds[i] for i in random_indices]\n",
    "# selected_ious = [valid_all_ious_pred_vs_gt[i] for i in random_indices]\n",
    "\n",
    "# print(\"Indices:\", random_indices, \"Images:\", images)\n",
    "# mean_s = r'$\\bar{x}$' \n",
    "\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=len(images), figsize=(30, 15))\n",
    "\n",
    "# for j, (image_name, gt_mask, pred_mask, selected_iou) in enumerate(zip(images, selected_gts, selected_preds, selected_ious)):\n",
    "\n",
    "#     image_id = [img['id'] for img in valid_coco_data['images'] if img['file_name'].startswith(image_name.split(\".\")[0])][0]\n",
    "#     dataset_images = valid_coco_data['images']\n",
    "#     image_filename = [dataset_images[i]['file_name'] for i in range(len(dataset_images)) if dataset_images[i]['id']==image_id][0]\n",
    "#     gt_masks = np.array([dataset_utils.create_mask(valid_coco_data['annotations'][i]['segmentation'][0], (512, 512)) \\\n",
    "#                          for i in range(len(valid_coco_data['annotations']))\\\n",
    "#                       if valid_coco_data['annotations'][i]['image_id']==image_id])\n",
    "\n",
    "#     print('Mean IoU:', np.mean(selected_iou), 'std:', np.std(selected_iou))\n",
    "#     gt_masks = np.array(gt_masks)\n",
    "#     gt_masks = gt_masks[:, None, :, :]\n",
    "#     ax_simple = axes[0, j]\n",
    "#     image = cv2.imread(valid_dir + image_name)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
    "#     ax_simple.imshow(255-image)\n",
    "#     ax_simple.set_xticks([])\n",
    "#     ax_simple.set_yticks([])\n",
    "#     ax_simple.set_title(f'{image_name.split(\".\")[0].replace(\"_png\", \"\")}', fontsize=30, pad=10)\n",
    "#     for spine in ax_simple.spines.values():\n",
    "#             spine.set_linewidth(0)\n",
    "#             spine.set_edgecolor('white')\n",
    "        \n",
    "#     # Display the image in all rows\n",
    "#     for row in range(2):\n",
    "#         ax = axes[row+1, j]\n",
    "#         ax.imshow(255-image)\n",
    "#         ax.set_xticks([])\n",
    "#         ax.set_yticks([])\n",
    "\n",
    "#         for spine in ax.spines.values():\n",
    "#             spine.set_linewidth(0)\n",
    "#             spine.set_edgecolor('white')\n",
    "\n",
    "#     tp_color = np.array([0/255, 255/255, 0/255, 0.4]) \n",
    "#     fp_color = np.array([255/255, 0/255, 0/255, 0.4]) \n",
    "#     fn_color = np.array([0/255, 0/255, 255/255, 0.4]) \n",
    "        \n",
    "#     thresholds = [0.3, 0.8] \n",
    "#     for i, threshold in enumerate(thresholds):\n",
    "#         ax = axes[i+1, j]\n",
    "#         true_positive_mask, false_positive_mask, false_negative_mask = predictor_utils.compute_metrics(\n",
    "#             gt_masks, \n",
    "#             pred_mask, \n",
    "#             threshold, \n",
    "#             image)\n",
    "\n",
    "#         h, w = true_positive_mask.shape[-2:]\n",
    "#         true_positive_mask = true_positive_mask.reshape(h, w, 1) * tp_color.reshape(1, 1, -1)\n",
    "#         false_positive_mask = false_positive_mask.reshape(h, w, 1) * fp_color.reshape(1, 1, -1)\n",
    "#         false_negative_mask = false_negative_mask.reshape(h, w, 1) * fn_color.reshape(1, 1, -1)\n",
    "\n",
    "#         # Overlay the TP, FP, FN masks with respective colors\n",
    "#         ax.imshow(false_positive_mask)\n",
    "#         ax.imshow(false_negative_mask)\n",
    "#         ax.imshow(true_positive_mask)\n",
    "#         if j == 0:\n",
    "#             ax.set_ylabel(f'IoU > {threshold}', fontsize=36)\n",
    "            \n",
    "# print(\"\\nPlotting...\")\n",
    "# tp_patch = mpatches.Patch(color=tp_color, label='True Positive', alpha=0.8)\n",
    "# fp_patch = mpatches.Patch(color=fp_color, label='False Positive', alpha=0.8)\n",
    "# fn_patch = mpatches.Patch(color=fn_color, label='False Negative', alpha=0.8)\n",
    "\n",
    "# # Add a legend for TP, FP, FN\n",
    "# fig.legend(handles=[tp_patch, fp_patch, fn_patch], loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=3, fontsize=30)\n",
    "# plt.tight_layout(pad=0.1)\n",
    "# plt.subplots_adjust(left=0.1, right=0.9, top=0.99, bottom=0.05, hspace=0.01, wspace=0.01)\n",
    "# plt.savefig('../plots/iou_tp_fp_fn_images.png', bbox_inches='tight', dpi=400)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0b8ef4b8092a483c8019c080f9f55ab8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_345020515e194d63a7b09cead7f0e1ad",
       "style": "IPY_MODEL_f65c72e5f4f54305ba68c5e86998d6bd"
      }
     },
     "2afe7fbbbc2649e6aef2f7e3c3b97b1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_e3c9ec928dbb4951bc1625262157aade",
       "max": 1,
       "style": "IPY_MODEL_ee1da285ca4546e18418a90ae9dccf85"
      }
     },
     "2bedf6d734494fbf85097e69b3b88de4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0b8ef4b8092a483c8019c080f9f55ab8",
        "IPY_MODEL_2afe7fbbbc2649e6aef2f7e3c3b97b1e"
       ],
       "layout": "IPY_MODEL_aec68aecbcac4b8daf2d8a07eecda345"
      }
     },
     "2c67a1e3054c4385bea591341841a371": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_445f1000e3f7493ca1cd674df2399f02",
       "style": "IPY_MODEL_598b6b6cf8204bc292f8fcc732d0904f",
       "value": "0.061 MB of 0.061 MB uploaded\r"
      }
     },
     "2e2418a4e7704380836198328806a521": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_94b9e20a13dc48558e732472e4028e49",
       "max": 1,
       "style": "IPY_MODEL_84f7649ed86d45f090a8cd56684442cf",
       "value": 1
      }
     },
     "345020515e194d63a7b09cead7f0e1ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "445f1000e3f7493ca1cd674df2399f02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "598b6b6cf8204bc292f8fcc732d0904f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "84f7649ed86d45f090a8cd56684442cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "94b9e20a13dc48558e732472e4028e49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aec68aecbcac4b8daf2d8a07eecda345": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dbf1f7a08b5f4df18dab0e06fec2fa3f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e3c9ec928dbb4951bc1625262157aade": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee1da285ca4546e18418a90ae9dccf85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f65c72e5f4f54305ba68c5e86998d6bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
