{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0515ed",
   "metadata": {},
   "source": [
    "# fine-tune mask-rcnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f1cfa-5599-4443-95a8-3332aa605325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "# from pycocotools.mask import frPyObjects, decode\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.set_device(7)  # ❗️❗️❗️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1def1-7808-4eaa-bdb3-eccc82ae1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictor_utils import *\n",
    "import predictor_utils\n",
    "from dataset_utils import *\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "\n",
    "reload(predictor_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4cf7a-9904-4574-bf53-b0428b25c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CocoDetection(root='/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/train/',\n",
    "                              annFile='/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/train/_annotations.coco.json')\n",
    "val_dataset = CocoDetection(root='/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/valid/',\n",
    "                            annFile='/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/valid/_annotations.coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76141426-1288-4e9e-b163-89da61b212b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    # new_targets = defaultdict(dict)\n",
    "    # for i, target in enumerate(targets):\n",
    "    #     for j, annot in enumerate(target):\n",
    "    #         new_targets[i['masks'] = annot['segmentation']\n",
    "    #         new_targets['boxes'] = annot['segmentation']\n",
    "    #         new_targets['labels'] = annot['segmentation']\n",
    "\n",
    "    images = [transforms.ToTensor()(image) for image in images]\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "def create_mask(points, image_size):\n",
    "    polygon = [(points[i], points[i+1]) for i in range(0, len(points), 2)]\n",
    "    mask = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "    cv2.fillPoly(mask, [np.array(polygon, dtype=np.int32)], 1)\n",
    "    return mask\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2fca9-d53e-4e43-a8bf-0df1db97ac90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 512\n",
    "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "        in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes=14)\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc637b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_targets(images, targets):\n",
    "    for target in targets:\n",
    "        for t in target:\n",
    "            if isinstance(t['bbox'], list):\n",
    "                t['bbox'] = torch.as_tensor(t['bbox'], dtype=torch.float32)\n",
    "\n",
    "    targets_adjusted = []\n",
    "    for target in targets:\n",
    "        if isinstance(target, dict):\n",
    "            target_adjusted = {k: v.to(device) if torch.is_tensor(\n",
    "                v) else v for k, v in target.items()}\n",
    "        # If the target is a list of dictionaries\n",
    "        elif isinstance(target, list) and all(isinstance(t, dict) for t in target):\n",
    "            target_adjusted = [{k: v.to(device) if torch.is_tensor(\n",
    "                v) else v for k, v in t.items()} for t in target]\n",
    "            for t in target:\n",
    "                for k, v in t.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        v = v.to(device)\n",
    "        else:\n",
    "            target_adjusted = target\n",
    "            print(\"Target is neither a dictionary nor a list of dictionaries !\")\n",
    "        targets_adjusted.append(target_adjusted)\n",
    "\n",
    "    for target in targets_adjusted:  # the model expects the target to have the following keys: 'boxes', 'labels', 'masks'\n",
    "        for tt in target:\n",
    "\n",
    "            if 'bbox' in tt.keys():\n",
    "                tt['boxes'] = tt.pop('bbox')\n",
    "                tt['boxes'] = torch.as_tensor(\n",
    "                    tt['boxes'], dtype=torch.float32).reshape(-1, 4)\n",
    "\n",
    "            if 'category_id' in tt.keys():\n",
    "                tt['labels'] = tt.pop('category_id')\n",
    "                tt['labels'] = torch.tensor(\n",
    "                    tt['labels'], dtype=torch.int64, device=device).unsqueeze(0)\n",
    "\n",
    "            if 'segmentation' in tt.keys():\n",
    "                tt['masks'] = create_mask(\n",
    "                    tt['segmentation'][0], (images[0].shape[1], images[0].shape[2]))\n",
    "                tt['masks'] = torch.tensor(\n",
    "                    tt['masks'], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    for target in targets_adjusted:  # XYHW to XYXY\n",
    "        for t in target:\n",
    "            boxes = t['boxes']\n",
    "            boxes[:, 2] += boxes[:, 0]\n",
    "            boxes[:, 3] += boxes[:, 1]\n",
    "            t['boxes'] = boxes\n",
    "    return targets_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e3ea5-3b15-4961-b5f4-2fb79d6665be",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**target** is a list of annotations. each annotation is a list of dictionaries containing the fields:\n",
    "* id\n",
    "* image_id\n",
    "* area\n",
    "* segmentation : (N, 1, H, W)\n",
    "* iscrowd\n",
    "* bbox (N, 4)\n",
    "* category_id : int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb674278-4949-46a5-a3e9-5aa026a7f3d5",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be8b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, targets in tqdm(train_data_loader):\n",
    "        images = torch.stack(images, dim=0).to(device)\n",
    "\n",
    "        targets_adjusted = adjust_targets(images, targets)\n",
    "        batch_loss = []\n",
    "        for i in range(len(targets_adjusted)):\n",
    "            # mask_copy = targets_adjusted[i][0]['masks'][0][0].detach().cpu().numpy()\n",
    "            # mask_copy = cv2.cvtColor(mask_copy, cv2.COLOR_GRAY2BGR)\n",
    "            # mask_copy = mask_copy * 255\n",
    "            # x_min, y_min, x_max, y_max = targets_adjusted[i][0]['boxes'][0].detach().cpu().numpy()\n",
    "            # x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
    "            \n",
    "            # print(x_min, y_min, x_max, y_max)\n",
    "            # cv2.rectangle(mask_copy, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            # plt.imshow(mask_copy)\n",
    "            # plt.show()\n",
    "        \n",
    "            # plt.imshow(images[i].permute(1, 2, 0).cpu().numpy())\n",
    "            # plt.show()\n",
    "            # plt.close()\n",
    "            # for j in targets_adjusted[i]:\n",
    "            #     print(targets_adjusted[i][0]['masks'][0][0].shape)\n",
    "            #     plt.imshow(targets_adjusted[i][0]['masks'][0][0].detach().cpu().numpy())\n",
    "            #     plt.show()\n",
    "            #     plt.close()\n",
    "\n",
    "            loss_dict = model(images[i].unsqueeze(0), targets_adjusted[i])\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            batch_loss.append(losses)\n",
    "        \n",
    "        del targets_adjusted\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        batch_loss = torch.stack(batch_loss)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.sum().backward()\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if parameter.grad is not None: \n",
    "                grad_norm = parameter.grad.norm()\n",
    "                if grad_norm < 1e-8:  # threshold\n",
    "                    print(f'❗️Layer {name} has vanishing gradients: {grad_norm}')\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_train_losses.append(batch_loss.mean().detach().cpu().numpy())\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # val_loss_accumulator = []\n",
    "    # with torch.no_grad():\n",
    "    #     for images, targets in val_data_loader:\n",
    "    #         images = torch.stack(images, dim=0).to(device)\n",
    "    #         targets_adjusted = adjust_targets(images, targets)\n",
    "\n",
    "    #         batch_loss = []\n",
    "    #         for i in range(len(targets_adjusted)):\n",
    "    #             loss_dict = model(images[i].unsqueeze(0), targets_adjusted[i])\n",
    "    #             print(loss_dict[0].keys())\n",
    "    #             losses = sum(loss for loss in loss_dict.values())\n",
    "    #             batch_loss.append(losses)\n",
    "\n",
    "    #         batch_loss = torch.stack(batch_loss).mean()  # Compute mean loss for the batch\n",
    "    #         val_loss_accumulator.append(batch_loss.item())\n",
    "\n",
    "    # epoch_val_loss = sum(val_loss_accumulator) / len(val_loss_accumulator)\n",
    "    # epoch_val_losses.append(epoch_val_loss)\n",
    "    # print(f' Epoch {epoch}. Train loss: {batch_loss.mean()}. Validation loss: {epoch_val_loss}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(images[len(images)-1].unsqueeze(0))\n",
    "        predicted_masks = []\n",
    "        for pred in prediction:\n",
    "            predicted_masks.append(pred['masks'][0][0].detach().cpu().numpy())\n",
    "    \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(images[len(images)-1].permute(1, 2, 0).detach().cpu().numpy())\n",
    "        show_masks(predicted_masks, plt.gca(), random_color=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # for mask in predicted_masks:\n",
    "        # plt.imshow(mask[0].detach().cpu().numpy())\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "        # predicted_labels = prediction[0]['labels']\n",
    "        # predicted_scores = prediction[0]['scores']\n",
    "    \n",
    "        # predicted_boxes = prediction[0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064b8bc-72a9-4a9f-a26a-8818585a823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'mask_rcnn_resent_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb1e78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(epoch_train_losses))),\n",
    "         epoch_train_losses, label='Training Loss')\n",
    "# plt.plot(list(range(len(valid_bboxes_losses))), valid_bboxes_losses, label='Validation Loss')\n",
    "plt.title('Mean epoch loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/mask_rcnn.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
