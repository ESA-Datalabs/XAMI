{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDq7cE9WlA28"
   },
   "source": [
    "### 00 Install segment anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://maxjoas.medium.com/finetune-segment-anything-sam-for-images-with-multiple-masks-34514ee811bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gfocM8zbj1m",
    "outputId": "e99cf6e5-9b1f-42a3-e054-1eee7937256c"
   },
   "outputs": [],
   "source": [
    "# !pip install segment_anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzlnytrWlHS5"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2mNb494bCQS"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import json\n",
    "import toml\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm2M1Pqxl8nv",
    "outputId": "e4dc62b7-0408-4529-f9d8-30055f232414"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.types import Device\n",
    "#data\n",
    "global train\n",
    "global test\n",
    "global annot\n",
    "\n",
    "# TODO: Put your path here !!!!\n",
    "# train_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/train\"\n",
    "# test = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/test\"\n",
    "\n",
    "train_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-2/train\"\n",
    "test = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-2/train/\"\n",
    "\n",
    "annot_train = \"_annotations.coco.json\"\n",
    "annot_test = \"_annotations.coco.json\"\n",
    "\n",
    "#model\n",
    "global batch_size\n",
    "global epochs\n",
    "global lr\n",
    "global weight_decay\n",
    "global DEVICE\n",
    "\n",
    "# TODOD adjust, if needed!!!!\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "lr = 0.001\n",
    "weight_decay = 0.00005\n",
    "DEVICE = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# loss\n",
    "global alpha\n",
    "global gamma\n",
    "alpha = 0.8\n",
    "gamma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_1_repr_images = [#'S0111200201_S', \n",
    "                                'S0414380901_V_png.rf.af973c8a0bf59679a9537a68e1af3b2d.jpg', \n",
    "                                'S0728360101_M_png.rf.53711bf6706b927adbcfc5c8b14100d4.jpg', \n",
    "                                'S0651790101_M_png.rf.db68eba9e94fd6505eefacb9fa3301aa.jpg', \n",
    "                                'S0844430201_L_png.rf.b761534d487edaec220bc29ce2e5cb9f.jpg',\n",
    "                                'S0205510401_M_png.rf.6f256ac1e8b7d36259a57ffde6208aab.jpg',\n",
    "                                'S0725290157_L_png.rf.1eb28d89ee137c739c48d39fd20e4fa8.jpg',\n",
    "                                'S0112260201_L_png.rf.6a52b5c55a524a577af1d51a6a894f5c.jpg',\n",
    "                                'S0065820601_M_png.rf.71af28e2772b1bab657c6e585d840abd.jpg',\n",
    "                                'S0112681301_L_png.rf.db0e33f14ba120bd599ddd9c66f86880.jpg',\n",
    "                                'S0827230401_M_png.rf.073265141f24734da9e6a88f9ebdaee7.jpg', # ROS\n",
    "                                'S0112680801_L_png.rf.9481cb81300d33e79f877d572e6162a6.jpg', # ROS\n",
    "                                'S0402430401_U_png.rf.961308f9c5796636182db1455c6ddaed.jpg', # ROS\n",
    "                                'S0506340101_L_png.rf.7511ae9b99d0a63a0c954763530917ae.jpg', #ROS\n",
    "                                'S0506340101_L_png.rf.7511ae9b99d0a63a0c954763530917ae.jpg', \n",
    "                                'S0655340135_L_png.rf.570a562141d45991c8f4935dfe7b3877.jpg', #ROS\n",
    "                                'S0300890101_U_png.rf.d929d6c991617f4575e4773247aa1c5a.jpg', \n",
    "                                'S0677840141_L_png.rf.7258cc2f40ad3454f04bc581a9c3ca9a.jpg',\n",
    "                                'S0655340140_L_png.rf.1c7ffe7b6168ad9cdb4dde1fbb2a8a3d.jpg', \n",
    "                                'S0201510101_M_png.rf.2e767022dd7c923948a21df7a47a92fa.jpg', \n",
    "                                'S0862090101_U_png.rf.61bf3459a21f2bf38e746c69ccd6db20.jpg', \n",
    "                                'S0110010701_S_png.rf.e4030682302dc037bc7ed14318c42c91.jpg', \n",
    "                                'S0414190701_U_png.rf.14a225508497ebedfb08628f5181ba82.jpg', \n",
    "                                'S0134921101_B_png.rf.4f2fdf08afea257413568ecba13d9421.jpg', \n",
    "                                'S0677820144_L_png.rf.3938bcbe21e6bd27ddc354123a96ec3f.jpg', \n",
    "                                'S0554750401_M_png.rf.ebf8b2f9a1f1e83caa1ce59d94cb6b42.jpg', \n",
    "                                'S0601930601_L_png.rf.a708cd8cb21291b6c47da17bf007dbf1.jpg', \n",
    "                                'S0610000201_L_png.rf.b63fa639aa370f0ab54ee5532b955ab3.jpg', \n",
    "                                'S0125911501_V_png.rf.a615cc89c36c5c43a6eb031da173dc46.jpg', \n",
    "                                'S0725290148_L_png.rf.1ea5685ce324a1c741e727e65486e188.jpg', \n",
    "                                'S0677800134_L_png.rf.4edc692f0dba644be7c82dc07067dea2.jpg',\n",
    "                                'S0203540701_U_png.rf.57fd1b4861d1fe3ea1f56b603a6aec60.jpg', \n",
    "                                'S0741032901_L_png.rf.aaeb5e8a293a1885fab80f116dac8ae7.jpg',\n",
    "                                'S0747420139_L_png.rf.5c4c0bce4e793a2e705ce0641353fd5d.jpg',\n",
    "                                'S0404350101_S_png.rf.1bbcd48cc90226b9f02988dbe870b1a7.jpg', \n",
    "                                'S0105460201_S_png.rf.893cd2920d2e190b3bd4b9b46df9e7b6.jpg', \n",
    "                                'S0840131201_L_png.rf.6f3c86b8e62c8db18d8c291c6a144bf7.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putem them into another file\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# for filename in iter_1_repr_images:\n",
    "#     src_path = os.path.join('./-xmm_om_images_v4-contrast-512-5-2/train/', filename)\n",
    "#     dst_path = os.path.join('./repr_images_1/', filename)\n",
    "    \n",
    "#     shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_1_repr_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(iter_1_repr_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # extract annotations for images given filenames and json annotations file\n",
    "# with open('./-xmm_om_images_v4-contrast-512-5-2/train/_annotations.coco.json', 'r') as f:\n",
    "#     coco_data = json.load(f)\n",
    "\n",
    "# # Filter the images to keep only those with filenames in image_filenames\n",
    "# filtered_images = [image for image in coco_data['images'] if image['file_name'] in iter_1_repr_images]\n",
    "# print(filtered_images)\n",
    "# # Collect IDs of the filtered images\n",
    "# img_ids = [image['id'] for image in filtered_images]\n",
    "\n",
    "# # Filter the annotations to keep only those associated with the filtered image IDs\n",
    "# filtered_annotations = [annotation for annotation in coco_data['annotations'] if annotation['image_id'] in img_ids]\n",
    "\n",
    "# # Update the original COCO data structure with filtered data\n",
    "# coco_data['images'] = filtered_images\n",
    "# coco_data['annotations'] = filtered_annotations\n",
    "\n",
    "# # Optionally, filter categories to keep only those used in the filtered annotations\n",
    "# used_category_ids = set(annotation['category_id'] for annotation in filtered_annotations)\n",
    "# filtered_categories = [category for category in coco_data['categories'] if category['id'] in used_category_ids]\n",
    "# coco_data['categories'] = filtered_categories\n",
    "\n",
    "# # Save the modified data to a new JSON file\n",
    "# with open('./repr_images_1/_annotations.coco.json', 'w') as f:\n",
    "#     json.dump(coco_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhpwnMj986d"
   },
   "source": [
    "### Take a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "i2viHv23KaoP",
    "outputId": "0eafea44-2139-4a71-bf4e-632c86ae3e05"
   },
   "outputs": [],
   "source": [
    "# TODO add your path\n",
    "sample_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-2/train/S0673730101_U_png.rf.fdeb357e1e274430b141f56d811dad49_augm3.jpg\"\n",
    "sample_img = cv2.imread(sample_path)\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37T0xYI4Ke9d"
   },
   "source": [
    "## 01 Segment image with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJBVqzUux7tL"
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "def build_totalmask(pred: List[Dict[str, Any]]) -> np.ndarray:\n",
    "    \"\"\"Builds a total mask from a list of segmentations\n",
    "    ARGS:\n",
    "        pred (list): list of dicts with keys 'segmentation' and others\n",
    "    RETURNS:\n",
    "        total_mask (np.ndarray): total mask\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    total_mask = np.zeros(pred[0]['segmentation'].shape, dtype=np.uint8)\n",
    "    for seg in pred:\n",
    "        total_mask += seg['segmentation']\n",
    "    # use cv2 to make image black and white\n",
    "    _, total_mask = cv2.threshold(total_mask, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "    return total_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD_gWqamK_AA"
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"./sam_vit_h_4b8939.pth\") #TODO your path here\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "sam.to(DEVICE)\n",
    "masks = mask_generator.generate(sample_img)\n",
    "print(type(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YxQNW4P_gkm",
    "outputId": "2283dc50-0244-4b59-cf4a-95c4470ee5f7"
   },
   "outputs": [],
   "source": [
    "print(len(masks))\n",
    "print(type(masks[0]))\n",
    "print(f'keys of dict: {masks[0].keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XT7PeRQSBYdC",
    "outputId": "3648dd4d-2775-49a6-bc0b-bd4ea20e1cc7"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape of masks: {masks[0]['segmentation'].shape}\")\n",
    "print('Value counts in segmentation of first mask:')\n",
    "print(np.unique(masks[0]['segmentation'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7s14BEuiB_tD"
   },
   "outputs": [],
   "source": [
    "total_mask = build_totalmask(masks)\n",
    "plt.imshow(total_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwOBqml4v_l6"
   },
   "source": [
    "When you use the web app of SAM, you might notice, that you need to provide a prompt (i.e. point with your mouse where your object is) to get a result. The mask_generator does this for you, by providing a grid of points over the whole image and creating a mask for each point and then later removing duplicated and low-quality masks. See the point grid below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "RShjKGz0vva4",
    "outputId": "6eff53ae-b5d7-4043-a880-a72bb8fbd416"
   },
   "outputs": [],
   "source": [
    "points = mask_generator.point_grids[0]\n",
    "# plot image and lay points on it\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(sample_img)\n",
    "ax.scatter(x=points[:, 0] *512, y=points[:, 1] *512, c=\"r\", s=10)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22HFyVV-_jFT"
   },
   "source": [
    "### Summary Segment Anything format:\n",
    "\n",
    "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
    "\n",
    "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
    "* `area` - `[int]` - the area of the mask in pixels\n",
    "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
    "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
    "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
    "* `stability_score` - `[float]` - an additional measure of mask quality\n",
    "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format\n",
    "\n",
    "- The mask generator uses a grid of points as prompts and generates masks for each point.[see here](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py)\n",
    "\n",
    "- The coco annotations categories is a list of dictionaries with keys:\n",
    "  * `id` - the id of the class\n",
    "  * `name` - the name corresponding to the id\n",
    "  * `supercategory` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAeC2TPHbCQU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load data from a json file in COCO format.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        the root directory containing the images and annotations\n",
    "    annotation_file : str\n",
    "        name of the json file containing the annotations (in root_dir)\n",
    "    transform : callable\n",
    "        a function/transform to apply to each image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx)\n",
    "        returns the image, image path, and masks for the given index\n",
    "    buid_total_mask(masks)\n",
    "        combines the masks into a single mask\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids_1 = list(self.coco.imgs.keys())\n",
    "        self.categories = self.coco.loadCats(self.coco.getCatIds()) \n",
    "        # Filter out image_ids without any annotations\n",
    "        self.image_ids_1 = [image_id for image_id in self.image_ids_1 if len(self.coco.getAnnIds(imgIds=image_id)) > 0]\n",
    "        self.categories = {self.categories[i]['id']: self.categories[i]['name'] for i in range(len(self.categories))} # id:name dict\n",
    "        self.image_ids = []\n",
    "\n",
    "        for idx in range(len(self.image_ids_1)):\n",
    "            image_id = self.image_ids_1[idx]\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            good_image = False\n",
    "            \n",
    "            for ann in anns:\n",
    "                x, y, w, h = ann['bbox']\n",
    "                if w > 10 or h > 10: # filter out very small masks\n",
    "                    if not self.categories[ann['category_id']].endswith('star'):\n",
    "                        good_image = True\n",
    "                        \n",
    "            if good_image: # filter out images with no annotations again\n",
    "                self.image_ids.append(image_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "\n",
    "            if not self.categories[ann['category_id']].endswith('star'): # this is safe because images with only stars are already filtered out in __init__\n",
    "                bboxes.append([x, y, x + w, y + h])\n",
    "                mask = self.coco.annToMask(ann)\n",
    "                masks.append(mask)\n",
    "      \n",
    "        if self.transform:\n",
    "            image, masks, bboxes = self.transform(image, masks, np.array(bboxes))\n",
    "\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        return image, image_path, torch.tensor(masks).float()\n",
    "\n",
    "    def get_totalmask(self, masks):\n",
    "        \"\"\"get all masks in to one image\n",
    "        ARGS:\n",
    "            masks (List[Tensor]): list of masks\n",
    "        RETURNS:\n",
    "            total_gt (Tensor): all masks in one image\n",
    "\n",
    "        \"\"\"\n",
    "        total_gt = torch.zeros_like(masks[0][0,:,:])\n",
    "        for k in range(len(masks[0])):\n",
    "            total_gt += masks[0][k,:,:]\n",
    "        return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class ResizeAndPad:\n",
    "    \"\"\"\n",
    "    Resize and pad images and masks to a target size.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_size : int\n",
    "        the target size of the image\n",
    "    transform : ResizeLongestSide\n",
    "        a transform to resize the image and masks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.transform = ResizeLongestSide(target_size)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, image, masks, bboxes):\n",
    "        # Resize image and masks\n",
    "        og_h, og_w, _ = image.shape\n",
    "\n",
    "        negative_map = (np.where(image > 0, 1, 0)).astype(np.uint8)\n",
    "        negative_map = torch.from_numpy(negative_map)  \n",
    "        negative_map = negative_map.permute(2, 0, 1)\n",
    "        negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "        negative_map = negative_map.to(torch.uint8)\n",
    "        \n",
    "        # negative_map = negative_map.unsqueeze(0)\n",
    "        \n",
    "        # negative_map = (np.where(image > 0, 1, 0)).astype(np.uint8) # apply_image expects np.uint8\n",
    "        image = self.transform.apply_image(image)\n",
    "        # negative_map = self.transform.apply_image(negative_map)\n",
    "        masks = [torch.tensor(self.transform.apply_image(mask)) for mask in masks]\n",
    "        image = self.to_tensor(image)\n",
    "        # negative_map = self.to_tensor(negative_map)\n",
    "\n",
    "        # Pad image and masks to form a square\n",
    "        _, h, w = image.shape\n",
    "        max_dim = max(w, h)\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        pad_h = (max_dim - h) // 2\n",
    "\n",
    "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
    "        image = transforms.Pad(padding)(image)\n",
    "        # negative_map = transforms.Pad(padding)(negative_map)\n",
    "        masks = [transforms.Pad(padding)(mask) for mask in masks]\n",
    "        # print('transformed image:', image.shape, image.dtype, image.min(), image.max())\n",
    "\n",
    "        image = image * negative_map # mask -ve pixels\n",
    "        # print(negative_map.shape, image.shape, np.unique(negative_map.detach().cpu().numpy()))\n",
    "        # plt.imshow(image[0].detach().cpu().numpy())\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n",
    "        bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n",
    "\n",
    "        return image, masks, bboxes\n",
    "\n",
    "\n",
    "def load_datasets(img_size):\n",
    "    \"\"\" load the training and validation datasets in PyTorch DataLoader objects\n",
    "    ARGS:\n",
    "        img_size (Tuple(int, int)): image size\n",
    "    RETURNS:\n",
    "        train_dataloader (DataLoader): training dataset\n",
    "        val_dataloader (DataLoader): validation dataset\n",
    "\n",
    "    \"\"\"\n",
    "    transform = ResizeAndPad(1024)\n",
    "    traindata = COCODataset(root_dir=train_path,\n",
    "                        annotation_file=os.path.join(train_path, annot_train),\n",
    "                        transform=transform)\n",
    "\n",
    "    valdata = COCODataset(root_dir=test,\n",
    "                      annotation_file=os.path.join(test, annot_test),\n",
    "                      transform=transform)\n",
    "    train_dataloader = DataLoader(traindata,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=1)\n",
    "    val_dataloader = DataLoader(valdata,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1)\n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8syP6e2bCQW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainloader, validloader = load_datasets(1024)\n",
    "sample_img = trainloader.dataset[0][0]\n",
    "sample_mask = trainloader.dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwxmF_hpbCQX",
    "outputId": "e8b0bbdc-45cc-4ba2-d4b4-e835b0252e89"
   },
   "outputs": [],
   "source": [
    "print(f'shape of sample_img: {sample_img.shape}')\n",
    "print(f'shape fo sample_mask: {sample_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i1znVjo0KQg"
   },
   "source": [
    "### Summary functions and classes:\n",
    "Now we have transformed our images and coco annotations to torch tensors, that we can use for training. For training (fine-tuning SAM) we need to define a Neural net with PyTorch first, we do this in the next class. It's pretty well documented, so I'll leave you with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46NcQC-A3jfy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ModelSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the sam model to to fine-tune the model on a new dataset\n",
    "\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    freeze_encoder (bool): freeze the encoder weights\n",
    "    freeze_decoder (bool): freeze the decoder weights\n",
    "    freeze_prompt_encoder (bool): freeze the prompt encoder weights\n",
    "    transform (ResizeLongestSide): resize the images to the model input size\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    setup(): load the model and freeze the weights\n",
    "    forward(images, points): forward pass of the model, returns the masks and iou_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freeze_encoder=True, freeze_decoder=False, freeze_prompt_encoder=True):\n",
    "        super().__init__()\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.freeze_decoder = freeze_decoder\n",
    "        self.freeze_prompt_encoder = freeze_prompt_encoder\n",
    "        # we need this to make the input image size compatible with the model\n",
    "        self.transform = ResizeLongestSide(1024) #This is 1024, because sam was trained on 1024x1024 images\n",
    "\n",
    "    def setup(self, MODEL_TYPE, chekpoint):\n",
    "        self.model = sam_model_registry[MODEL_TYPE](chekpoint)\n",
    "        # to speed up training time, we normally freeze the encoder and decoder\n",
    "        if self.freeze_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.transfrom = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "        \n",
    "    def forward(self, images, negative_maps):\n",
    "\n",
    "        # print('images shape:', images.shape)\n",
    "        _, _, H, W = images.shape # batch, channel, height, width - well, batch_size cannot be greater than 1 apparently\n",
    "        \n",
    "        image_embeddings = self.model.image_encoder(images) # shape: (1, 256, 64, 64)\n",
    "        # get prompt embeddings without acutally any prompts (uninformative)\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "\n",
    "        # get low resolution masks and iou predictions\n",
    "        # mulitmask_output=False means that we only get one mask per image,\n",
    "        # otherwise we would get three masks per image\n",
    "        low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # sparse_embeddings shape: (1, 0, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # dense_embeddings shape: (1, 256, 256)\n",
    "            multimask_output=True,\n",
    "        )\n",
    "        # postprocess the masks to get the final masks and resize them to the original image size\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks, # shape: (1, 1, 256, 256)\n",
    "            (H, W),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        \n",
    "        masks = masks * negative_maps\n",
    "\n",
    "        # print(masks.shape)\n",
    "        # shape masks after interpolate: torch.Size([1, 1, 1024, 1024])\n",
    "        return masks, iou_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2fwU8qI4oKH",
    "outputId": "01753af9-7f4d-4901-abc7-ccdcbe73226c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = ModelSimple()\n",
    "# model.setup('vit_h', './sam_vit_h_4b8939.pth')\n",
    "# img_size = model.model.image_encoder.img_size\n",
    "# print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def compute_iou_tensor(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) of two binary masks represented as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    intersection = torch.logical_and(mask1, mask2).sum().item()\n",
    "    union = torch.logical_or(mask1, mask2).sum().item()\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return intersection / union\n",
    "\n",
    "def associate_masks_tensor(predicted_masks, target_masks):\n",
    "    \"\"\"\n",
    "    Associate each predicted mask with a target mask using the Hungarian algorithm.\n",
    "    Assumes inputs are lists of PyTorch tensors.\n",
    "    \"\"\"\n",
    "    num_predicted = len(predicted_masks)\n",
    "    num_targets = len(target_masks)\n",
    "    \n",
    "    # Calculate the IoU between each pair of predicted and target mask\n",
    "    iou_matrix = torch.zeros(num_predicted, num_targets)\n",
    "    for i, pred_mask in enumerate(predicted_masks):\n",
    "        for j, target_mask in enumerate(target_masks):\n",
    "            iou_matrix[i, j] = compute_iou_tensor(pred_mask, target_mask)\n",
    "    \n",
    "    # Convert IoU matrix to NumPy for linear_sum_assignment\n",
    "    iou_matrix_np = iou_matrix.numpy()\n",
    "    row_ind, col_ind = linear_sum_assignment(1 - iou_matrix_np)\n",
    "    \n",
    "    # Filter out assignments with no overlap\n",
    "    matched_indices = [(r, c) for r, c in zip(row_ind, col_ind) if iou_matrix[r, c] > 0]\n",
    "    \n",
    "    return matched_indices, iou_matrix\n",
    "\n",
    "# # Example usage with tensor masks\n",
    "# predicted_masks = [torch.randint(0, 2, (100, 100), dtype=torch.bool) for _ in range(5)]\n",
    "# target_masks = [torch.randint(0, 2, (100, 100), dtype=torch.bool) for _ in range(4)]\n",
    "\n",
    "# matched_indices, iou_scores_tensor = associate_masks_tensor(predicted_masks, target_masks)\n",
    "\n",
    "# for pred_idx, target_idx in matched_indices:\n",
    "#     print(f\"Predicted Mask {pred_idx} is matched with Target Mask {target_idx} with IoU {iou_scores_tensor[pred_idx, target_idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#train images:', len(trainloader.dataset), '#validation images:', len(validloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPhQN81W3QyN"
   },
   "source": [
    "## Models, classes functions for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GhzOeOFbCQa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_totalmask(masks):\n",
    "    \"\"\"get all masks in to one image\n",
    "    ARGS:\n",
    "        masks (torch.Tensor): shape: (N, H, W) where N is the number of masks\n",
    "                              masks H,W is usually 1024,1024\n",
    "    RETURNS:\n",
    "        total_gt (torch.Tensor): all masks in one image\n",
    "\n",
    "    \"\"\"\n",
    "    total_gt = torch.zeros_like(masks[0,:,:])\n",
    "    for k in range(len(masks)):\n",
    "        total_gt += masks[k,:,:]\n",
    "    return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Computes the Focal loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1 - BCE_EXP)**gamma * BCE\n",
    "\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" Computes the Dice loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / \\\n",
    "            (inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "\n",
    "def criterion(x, y):\n",
    "    \"\"\" Combined dice and focal loss.\n",
    "    ARGS:\n",
    "        x: (torch.Tensor) the model output\n",
    "        y: (torch.Tensor) the target\n",
    "    RETURNS:\n",
    "        (torch.Tensor) the combined loss\n",
    "\n",
    "    \"\"\"\n",
    "    focal, dice = FocalLoss(), DiceLoss()\n",
    "    y = y.to(DEVICE)\n",
    "    x = x.to(DEVICE)\n",
    "    return 1e4 * (20 * focal(x, y) + dice(x, y))\n",
    "\n",
    "\n",
    "def train_one_epoch(model, trainloader, optimizer, epoch_idx):\n",
    "    \"\"\" Runs forward and backward pass for one epoch and returns the average\n",
    "    batch loss for the epoch.\n",
    "    ARGS:\n",
    "        model: (nn.Module) the model to train\n",
    "        trainloader: (torch.utils.data.DataLoader) the dataloader for training\n",
    "        optimizer: (torch.optim.Optimizer) the optimizer to use for training\n",
    "        epoch_idx: (int) the index of the current epoch\n",
    "        tb_writer: (torch.utils.tensorboard.writer.SummaryWriter) the tensorboard writer\n",
    "    RETURNS:\n",
    "        last_loss: (float) the average batch loss for the epoch\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    for i, (image, path, masks) in tqdm(enumerate(trainloader)):\n",
    "        orig_image = cv2.imread(path[0])\n",
    "        orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        negative_map = (np.where(orig_image > 0, 1, 0)).astype(np.uint8)\n",
    "        negative_map = torch.from_numpy(negative_map)  \n",
    "        negative_map = negative_map.permute(2, 0, 1)\n",
    "        negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "        negative_map = negative_map.to(torch.uint8).squeeze(0).to(DEVICE)\n",
    "        del orig_image\n",
    "        image = image.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(image, negative_map)\n",
    "        masks = masks[0].to(DEVICE) # (number of masks, 1024, 1024)\n",
    "\n",
    "        # match predicted with ground truth masks (using the Hungarian method)\n",
    "        matched_indices, iou_scores_tensor = associate_masks_tensor(pred, masks)\n",
    "        print(matched_indices, iou_scores_tensor)\n",
    "\n",
    "        total_mask = get_totalmask(masks)\n",
    "        pred = pred.to(DEVICE)\n",
    "\n",
    "        print(pred.shape, pred[0][0].unsqueeze(0).unsqueeze(0).shape)\n",
    "        loss = criterion(pred[0][0].unsqueeze(0).unsqueeze(0), total_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        binary_pred_mask = ((pred.detach().cpu().numpy()[0]) > 0.0).astype(int)\n",
    "\n",
    "        for name, parameter in model.named_parameters():\n",
    "                if parameter.grad is not None: \n",
    "                    grad_norm = parameter.grad.norm()\n",
    "                    if grad_norm < 1e-8: \n",
    "                        print(f'❗️Layer {name} has vanishing gradients: {grad_norm}')\n",
    "                        \n",
    "        fig, axs = plt.subplots(1, 3)\n",
    "        \n",
    "        axs[0].imshow(image[0].permute(1,2,0).detach().cpu().numpy())\n",
    "        axs[0].axis('off')\n",
    "    \n",
    "        axs[1].imshow(total_mask.detach().cpu().numpy())\n",
    "        axs[1].set_title('Ground truth masks', fontsize=10)\n",
    "        axs[1].axis('off')\n",
    "        \n",
    "        # axs[2].imshow(pred.detach().cpu().numpy()[0][0])\n",
    "        axs[2].imshow(binary_pred_mask[0])\n",
    "        axs[2].set_title('Predicted masks', fontsize=10)\n",
    "        axs[2].axis('off')\n",
    "        # plt.savefig('on_img_train.png', dpi=300)\n",
    "    \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    i = len(trainloader)\n",
    "    last_loss = running_loss / i\n",
    "    # print(f'batch_loss for batch {i}: {last_loss}')\n",
    "    tb_x = epoch_idx * len(trainloader) + i + 1\n",
    "    # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def train(n_epochs_stop):\n",
    "    \"\"\" Trains the model for the given number of epochs.\"\"\"\n",
    "    bestmodel_path = \"\"\n",
    "    model = ModelSimple()\n",
    "    model.setup('vit_h', './sam_vit_h_4b8939.pth')\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load('model_final.pth'))\n",
    "    model.to(DEVICE)\n",
    "    img_size = model.model.image_encoder.img_size\n",
    "    trainloader, validloader = load_datasets(img_size=img_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    train_losses, valid_losses = [], []\n",
    "    for epch in range(epochs): # type: ignore\n",
    "        #train\n",
    "        running_vloss = 0.\n",
    "        model.train(True)\n",
    "        avg_batchloss = train_one_epoch(\n",
    "            model, trainloader, optimizer, epch)\n",
    "        print(f'epoch: {epch}, train loss: {avg_batchloss}.')\n",
    "        train_losses.append(avg_batchloss)\n",
    "        # validate\n",
    "        eval = False\n",
    "        \n",
    "        if not eval: # type: ignore\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            for images, path, masks in validloader:\n",
    "                model.to(DEVICE)\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks[0].to(DEVICE)\n",
    "                total_mask = get_totalmask(masks)\n",
    "                total_mask = total_mask.to(DEVICE)\n",
    "                model.eval()\n",
    "                preds, iou = model(images)\n",
    "                preds = preds.to(DEVICE)\n",
    "                vloss = criterion(preds, total_mask)\n",
    "                running_vloss += vloss.item()\n",
    "        avg_vloss = running_vloss / len(validloader)\n",
    "        valid_losses.append(avg_vloss)\n",
    "        \n",
    "        # save model\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        print(f'best valid loss: {best_valid_loss}')\n",
    "        if running_vloss < best_valid_loss:\n",
    "            best_valid_loss = running_vloss\n",
    "            best_model = model\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print(\"Early stopping initiated.\")\n",
    "                early_stop = True\n",
    "                break\n",
    "     \n",
    "    return model, train_losses, valid_losses\n",
    "\n",
    "# next add train function for prompting model with points (ModelAll)\n",
    "# next unrelated find out how to get tensor output of masks form SamAutomaticMaskGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFWOAdEY7upr"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSqrkj4w7uLh",
    "outputId": "053c9b1b-6609-4980-949d-2f3faf4edf04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, train_losses, val_losses = train(n_epochs_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_losses', train_losses)\n",
    "print('val_losses', val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(train_losses))), train_losses, label='Training Loss')\n",
    "# plt.plot(list(range(len(val_losses))), val_losses, label='Validation Loss')\n",
    "plt.title('Mean epoch loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/amg_losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), f'model_final_on_36_repr_images.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sam = sam_model_registry[\"vit_h\"](checkpoint=\"./sam_vit_h_4b8939.pth\")\n",
    "# mask_generator = SamAutomaticMaskGenerator(model.model)\n",
    "# # sam.to(DEVICE)\n",
    "# predicted_masks = mask_generator.generate(sample_img) # this doesn't really work this way, but rather in the way the model was fine-tuned\n",
    "# print(len(predicted_masks))\n",
    "\n",
    "# total_mask = build_totalmask(predicted_masks)\n",
    "# plt.imshow(total_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Epochs and corresponding train loss values\n",
    "epochs = list(range(10))\n",
    "train_loss = [\n",
    "    2.111152442395687,\n",
    "    1.6615700113773346,\n",
    "    1.6539997470378875,\n",
    "    1.6287594631314277,\n",
    "    1.7573000520467759,\n",
    "    1.7617580753564834,\n",
    "    1.737440549135208,\n",
    "    1.6835789729654789,\n",
    "    1.819552083015442,\n",
    "    1.759515732228756\n",
    "]\n",
    "\n",
    "# Plotting the train loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, marker='o', linestyle='-', color='b', label='Train Loss')\n",
    "plt.title('Train Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xticks(epochs)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FrAGnJX8vwH"
   },
   "source": [
    "## Predict trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf8QC8hobCQe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict trainied model\n",
    "eval = False\n",
    "\n",
    "if eval:\n",
    "    model_trained = ModelSimple()\n",
    "    model_trained.setup()\n",
    "    with torch.no_grad():\n",
    "        model_trained.load_state_dict(torch.load('model_final.pth', map_location=torch.device(\"cuda:7\")))\n",
    "        model_trained.eval()\n",
    "        img_size = model_trained.model.image_encoder.img_size\n",
    "        print(f'img_size: {img_size}')\n",
    "        print(f'image_encoder.img_size: {model_trained.model.image_encoder.img_size}')\n",
    "        running_vloss = 0.\n",
    "        for images, path, masks in validloader:\n",
    "    \n",
    "            orig_image = cv2.imread(path[0])\n",
    "            orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            negative_map = (np.where(orig_image > 0, 1, 0)).astype(np.uint8)\n",
    "            negative_map = torch.from_numpy(negative_map)  \n",
    "            negative_map = negative_map.permute(2, 0, 1)\n",
    "            negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "            negative_map = negative_map.to(torch.uint8).squeeze(0).to(DEVICE)\n",
    "            \n",
    "            # print(images)\n",
    "            model.to(DEVICE)\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks[0].to(DEVICE)\n",
    "            total_mask = get_totalmask(masks)\n",
    "            total_mask = total_mask.to(DEVICE)\n",
    "            model.eval()\n",
    "            preds, iou = model(images, negative_map)\n",
    "            preds = preds.to(DEVICE)\n",
    "    \n",
    "            print(preds.shape, total_mask.shape)\n",
    "            \n",
    "            vloss = criterion(preds[0][0].unsqueeze(0).unsqueeze(0), total_mask) # was criterion(preds, total_mask) before. i changes because the multimask_output is set on True\n",
    "            running_vloss += vloss.item()\n",
    "            np_image = images[0].permute(1,2,0).detach().cpu().numpy()\n",
    "            np_mask = masks.detach().cpu().numpy()\n",
    "            np_pred = preds[0].detach().cpu().numpy()\n",
    "    \n",
    "            # binary_pred_mask = ((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int) * 255\n",
    "            binary_pred_mask = ((preds[0]).cpu().numpy()[0] > 0).astype(int) * 255\n",
    "            \n",
    "            print(np.mean((F.sigmoid(preds[0]).cpu().numpy()[0])), np.min((F.sigmoid(preds[0]).cpu().numpy()[0])), np.max((F.sigmoid(preds[0]).cpu().numpy()[0])))\n",
    "            print(np.unique((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int))\n",
    "            print(\"validation loss: \", running_vloss)\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 3)\n",
    "            \n",
    "            axs[0].imshow(np_image)\n",
    "            axs[0].axis('off')\n",
    "        \n",
    "            axs[1].imshow(get_totalmask(masks).detach().cpu().numpy())\n",
    "            axs[1].set_title('Ground truth masks', fontsize=10)\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            axs[2].imshow(binary_pred_mask)\n",
    "            axs[2].set_title('Predicted masks', fontsize=10)\n",
    "            axs[2].axis('off')\n",
    "            # plt.savefig('on_img_train.png', dpi=300)\n",
    "        \n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU1aS-ow9ZUy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
