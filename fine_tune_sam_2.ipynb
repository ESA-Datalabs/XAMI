{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDq7cE9WlA28"
   },
   "source": [
    "### 00 Install segment anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://maxjoas.medium.com/finetune-segment-anything-sam-for-images-with-multiple-masks-34514ee811bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gfocM8zbj1m",
    "outputId": "e99cf6e5-9b1f-42a3-e054-1eee7937256c"
   },
   "outputs": [],
   "source": [
    "# !pip install segment_anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzlnytrWlHS5"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2mNb494bCQS"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import json\n",
    "import toml\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm2M1Pqxl8nv",
    "outputId": "e4dc62b7-0408-4529-f9d8-30055f232414"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.types import Device\n",
    "#data\n",
    "global train\n",
    "global test\n",
    "global annot\n",
    "\n",
    "# TODO: Put your path here !!!!\n",
    "# train_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/train\"\n",
    "# test = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/dog-2/test\"\n",
    "\n",
    "train_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/one_image_test\"\n",
    "test = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/one_image_test\"\n",
    "\n",
    "annot = \"_annotations.coco.json\"\n",
    "#model\n",
    "global batch_size\n",
    "global epochs\n",
    "global lr\n",
    "global weight_decay\n",
    "global DEVICE\n",
    "\n",
    "# TODOD adjust, if needed!!!!\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "weight_decay = 0.0005\n",
    "DEVICE = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# loss\n",
    "global alpha\n",
    "global gamma\n",
    "alpha = 0.8\n",
    "gamma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # extract annotations for one image only given image filename and json annotations file\n",
    "# with open('./-xmm_om_images_v4-contrast-512-5-2/train/_annotations.coco.json', 'r') as f:\n",
    "#     coco_data = json.load(f)\n",
    "\n",
    "# one_image_filename = 'S0037980401_L_png.rf.c1dea6bc6d11debf603d5832b53c8e89.jpg'\n",
    "\n",
    "# filtered_images = [image for image in coco_data['images'] if image['file_name'] == one_image_filename]\n",
    "# print(filtered_images)\n",
    "\n",
    "# img_id = [image['id'] for image in coco_data['images'] if image['file_name'] == one_image_filename]\n",
    "# filtered_annotations = [annotation for annotation in coco_data['annotations'] if annotation['image_id'] == img_id[0]]\n",
    "\n",
    "# coco_data['images'] = filtered_images\n",
    "# coco_data['annotations'] = filtered_annotations\n",
    "\n",
    "# used_category_ids = set([annotation['category_id'] for annotation in filtered_annotations])\n",
    "# filtered_categories = [category for category in coco_data['categories'] if category['id'] in used_category_ids]\n",
    "# coco_data['categories'] = filtered_categories\n",
    "\n",
    "# with open('./one_image_test/_annotations.coco.json', 'w') as f:\n",
    "#     json.dump(coco_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhpwnMj986d"
   },
   "source": [
    "### Take a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "i2viHv23KaoP",
    "outputId": "0eafea44-2139-4a71-bf4e-632c86ae3e05"
   },
   "outputs": [],
   "source": [
    "# TODO add your path\n",
    "sample_path = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-2/train/S0673730101_U_png.rf.fdeb357e1e274430b141f56d811dad49_augm3.jpg\"\n",
    "sample_img = cv2.imread(sample_path)\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37T0xYI4Ke9d"
   },
   "source": [
    "## 01 Segment image with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJBVqzUux7tL"
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "def build_totalmask(pred: List[Dict[str, Any]]) -> np.ndarray:\n",
    "    \"\"\"Builds a total mask from a list of segmentations\n",
    "    ARGS:\n",
    "        pred (list): list of dicts with keys 'segmentation' and others\n",
    "    RETURNS:\n",
    "        total_mask (np.ndarray): total mask\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    total_mask = np.zeros(pred[0]['segmentation'].shape, dtype=np.uint8)\n",
    "    for seg in pred:\n",
    "        total_mask += seg['segmentation']\n",
    "    # use cv2 to make image black and white\n",
    "    _, total_mask = cv2.threshold(total_mask, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "    return total_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD_gWqamK_AA"
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"./sam_vit_h_4b8939.pth\") #TODO your path here\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "sam.to(DEVICE)\n",
    "masks = mask_generator.generate(sample_img)\n",
    "print(type(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YxQNW4P_gkm",
    "outputId": "2283dc50-0244-4b59-cf4a-95c4470ee5f7"
   },
   "outputs": [],
   "source": [
    "print(len(masks))\n",
    "print(type(masks[0]))\n",
    "print(f'keys of dict: {masks[0].keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XT7PeRQSBYdC",
    "outputId": "3648dd4d-2775-49a6-bc0b-bd4ea20e1cc7"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape of masks: {masks[0]['segmentation'].shape}\")\n",
    "print('Value counts in segmentation of first mask:')\n",
    "print(np.unique(masks[0]['segmentation'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7s14BEuiB_tD"
   },
   "outputs": [],
   "source": [
    "total_mask = build_totalmask(masks)\n",
    "plt.imshow(total_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwOBqml4v_l6"
   },
   "source": [
    "When you use the web app of SAM, you might notice, that you need to provide a prompt (i.e. point with your mouse where your object is) to get a result. The mask_generator does this for you, by providing a grid of points over the whole image and creating a mask for each point and then later removing duplicated and low-quality masks. See the point grid below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "RShjKGz0vva4",
    "outputId": "6eff53ae-b5d7-4043-a880-a72bb8fbd416"
   },
   "outputs": [],
   "source": [
    "points = mask_generator.point_grids[0]\n",
    "# plot image and lay points on it\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(sample_img)\n",
    "ax.scatter(x=points[:, 0] *512, y=points[:, 1] *512, c=\"r\", s=10)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22HFyVV-_jFT"
   },
   "source": [
    "### Summary Segment Anything format:\n",
    "\n",
    "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
    "\n",
    "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
    "* `area` - `[int]` - the area of the mask in pixels\n",
    "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
    "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
    "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
    "* `stability_score` - `[float]` - an additional measure of mask quality\n",
    "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format\n",
    "\n",
    "- The mask generator uses a grid of points as prompts and generates masks for each point.[see here](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAeC2TPHbCQU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load data from a json file in COCO format.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        the root directory containing the images and annotations\n",
    "    annotation_file : str\n",
    "        name of the json file containing the annotations (in root_dir)\n",
    "    transform : callable\n",
    "        a function/transform to apply to each image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx)\n",
    "        returns the image, image path, and masks for the given index\n",
    "    buid_total_mask(masks)\n",
    "        combines the masks into a single mask\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        # Filter out image_ids without any annotations\n",
    "        self.image_ids = [image_id for image_id in self.image_ids if len(self.coco.getAnnIds(imgIds=image_id)) > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image, masks, bboxes = self.transform(image, masks, np.array(bboxes))\n",
    "\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        return image, image_path, torch.tensor(masks).float()\n",
    "\n",
    "    def get_totalmask(self, masks):\n",
    "        \"\"\"get all masks in to one image\n",
    "        ARGS:\n",
    "            masks (List[Tensor]): list of masks\n",
    "        RETURNS:\n",
    "            total_gt (Tensor): all masks in one image\n",
    "\n",
    "        \"\"\"\n",
    "        total_gt = torch.zeros_like(masks[0][0,:,:])\n",
    "        for k in range(len(masks[0])):\n",
    "            total_gt += masks[0][k,:,:]\n",
    "        return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class ResizeAndPad:\n",
    "    \"\"\"\n",
    "    Resize and pad images and masks to a target size.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_size : int\n",
    "        the target size of the image\n",
    "    transform : ResizeLongestSide\n",
    "        a transform to resize the image and masks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.transform = ResizeLongestSide(target_size)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, image, masks, bboxes):\n",
    "        # Resize image and masks\n",
    "        og_h, og_w, _ = image.shape\n",
    "\n",
    "        negative_map = (np.where(image > 0, 1, 0)).astype(np.uint8)\n",
    "        negative_map = torch.from_numpy(negative_map)  \n",
    "        negative_map = negative_map.permute(2, 0, 1)\n",
    "        negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "        negative_map = negative_map.to(torch.uint8)\n",
    "        \n",
    "        # negative_map = negative_map.unsqueeze(0)\n",
    "        \n",
    "        # negative_map = (np.where(image > 0, 1, 0)).astype(np.uint8) # apply_image expects np.uint8\n",
    "        image = self.transform.apply_image(image)\n",
    "        # negative_map = self.transform.apply_image(negative_map)\n",
    "        masks = [torch.tensor(self.transform.apply_image(mask)) for mask in masks]\n",
    "        image = self.to_tensor(image)\n",
    "        # negative_map = self.to_tensor(negative_map)\n",
    "\n",
    "        # Pad image and masks to form a square\n",
    "        _, h, w = image.shape\n",
    "        max_dim = max(w, h)\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        pad_h = (max_dim - h) // 2\n",
    "\n",
    "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
    "        image = transforms.Pad(padding)(image)\n",
    "        # negative_map = transforms.Pad(padding)(negative_map)\n",
    "        masks = [transforms.Pad(padding)(mask) for mask in masks]\n",
    "        print('transformed image:', image.shape, image.dtype, image.min(), image.max())\n",
    "\n",
    "        image = image * negative_map # mask -ve pixels\n",
    "        # print(negative_map.shape, image.shape, np.unique(negative_map.detach().cpu().numpy()))\n",
    "        # plt.imshow(image[0].detach().cpu().numpy())\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n",
    "        bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n",
    "\n",
    "        return image, masks, bboxes\n",
    "\n",
    "\n",
    "def load_datasets(img_size):\n",
    "    \"\"\" load the training and validation datasets in PyTorch DataLoader objects\n",
    "    ARGS:\n",
    "        img_size (Tuple(int, int)): image size\n",
    "    RETURNS:\n",
    "        train_dataloader (DataLoader): training dataset\n",
    "        val_dataloader (DataLoader): validation dataset\n",
    "\n",
    "    \"\"\"\n",
    "    transform = ResizeAndPad(1024)\n",
    "    traindata = COCODataset(root_dir=train_path,\n",
    "                        annotation_file=os.path.join(train_path, annot),\n",
    "                        transform=transform)\n",
    "    valdata = COCODataset(root_dir=test,\n",
    "                      annotation_file=os.path.join(test, annot),\n",
    "                      transform=transform)\n",
    "    train_dataloader = DataLoader(traindata,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=1)\n",
    "    val_dataloader = DataLoader(valdata,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1)\n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8syP6e2bCQW"
   },
   "outputs": [],
   "source": [
    "trainloader, validloader = load_datasets(1024)\n",
    "sample_img = trainloader.dataset[0][0]\n",
    "sample_mask = trainloader.dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwxmF_hpbCQX",
    "outputId": "e8b0bbdc-45cc-4ba2-d4b4-e835b0252e89"
   },
   "outputs": [],
   "source": [
    "print(f'shape of sample_img: {sample_img.shape}')\n",
    "print(f'shape fo sample_mask: {sample_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i1znVjo0KQg"
   },
   "source": [
    "### Summary functions and classes:\n",
    "Now we have transformed our images and coco annotations to torch tensors, that we can use for training. For training (fine-tuning SAM) we need to define a Neural net with PyTorch first, we do this in the next class. It's pretty well documented, so I'll leave you with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46NcQC-A3jfy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ModelSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the sam model to to fine-tune the model on a new dataset\n",
    "\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    freeze_encoder (bool): freeze the encoder weights\n",
    "    freeze_decoder (bool): freeze the decoder weights\n",
    "    freeze_prompt_encoder (bool): freeze the prompt encoder weights\n",
    "    transform (ResizeLongestSide): resize the images to the model input size\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    setup(): load the model and freeze the weights\n",
    "    forward(images, points): forward pass of the model, returns the masks and iou_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freeze_encoder=True, freeze_decoder=False, freeze_prompt_encoder=True):\n",
    "        super().__init__()\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.freeze_decoder = freeze_decoder\n",
    "        self.freeze_prompt_encoder = freeze_prompt_encoder\n",
    "        # we need this to make the input image size compatible with the model\n",
    "        self.transform = ResizeLongestSide(1024) #This is 1024, because sam was trained on 1024x1024 images\n",
    "\n",
    "    def setup(self):\n",
    "        self.model = sam_model_registry['vit_h']('./sam_vit_h_4b8939.pth')\n",
    "        # to speed up training time, we normally freeze the encoder and decoder\n",
    "        if self.freeze_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.transfrom = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "    def forward(self, images, negative_maps):\n",
    "\n",
    "        print('images shape:', images.shape)\n",
    "        _, _, H, W = images.shape # batch, channel, height, width - well, batch_size cannot be greater than 1 apparently\n",
    "        \n",
    "        image_embeddings = self.model.image_encoder(images) # shape: (1, 256, 64, 64)\n",
    "        # get prompt embeddings without acutally any prompts (uninformative)\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "\n",
    "        # get low resolution masks and iou predictions\n",
    "        # mulitmask_output=False means that we only get one mask per image,\n",
    "        # otherwise we would get three masks per image\n",
    "        low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # sparse_embeddings shape: (1, 0, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # dense_embeddings shape: (1, 256, 256)\n",
    "            multimask_output=True,\n",
    "        )\n",
    "        # postprocess the masks to get the final masks and resize them to the original image size\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks, # shape: (1, 1, 256, 256)\n",
    "            (H, W),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        \n",
    "        masks = masks * negative_maps\n",
    "\n",
    "        print(masks.shape)\n",
    "        # shape masks after interpolate: torch.Size([1, 1, 1024, 1024])\n",
    "        return masks, iou_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2fwU8qI4oKH",
    "outputId": "01753af9-7f4d-4901-abc7-ccdcbe73226c"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = ModelSimple()\n",
    "model.setup()\n",
    "img_size = model.model.image_encoder.img_size\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPhQN81W3QyN"
   },
   "source": [
    "## Models, classes functions for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GhzOeOFbCQa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def get_totalmask(masks):\n",
    "    \"\"\"get all masks in to one image\n",
    "    ARGS:\n",
    "        masks (torch.Tensor): shape: (N, H, W) where N is the number of masks\n",
    "                              masks H,W is usually 1024,1024\n",
    "    RETURNS:\n",
    "        total_gt (torch.Tensor): all masks in one image\n",
    "\n",
    "    \"\"\"\n",
    "    total_gt = torch.zeros_like(masks[0,:,:])\n",
    "    for k in range(len(masks)):\n",
    "        total_gt += masks[k,:,:]\n",
    "    return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Computes the Focal loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1 - BCE_EXP)**gamma * BCE\n",
    "\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" Computes the Dice loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / \\\n",
    "            (inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "\n",
    "def criterion(x, y):\n",
    "    \"\"\" Combined dice and focal loss.\n",
    "    ARGS:\n",
    "        x: (torch.Tensor) the model output\n",
    "        y: (torch.Tensor) the target\n",
    "    RETURNS:\n",
    "        (torch.Tensor) the combined loss\n",
    "\n",
    "    \"\"\"\n",
    "    focal, dice = FocalLoss(), DiceLoss()\n",
    "    y = y.to(DEVICE)\n",
    "    x = x.to(DEVICE)\n",
    "    return 20 * focal(x, y) + dice(x, y)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, trainloader, optimizer, epoch_idx):\n",
    "    \"\"\" Runs forward and backward pass for one epoch and returns the average\n",
    "    batch loss for the epoch.\n",
    "    ARGS:\n",
    "        model: (nn.Module) the model to train\n",
    "        trainloader: (torch.utils.data.DataLoader) the dataloader for training\n",
    "        optimizer: (torch.optim.Optimizer) the optimizer to use for training\n",
    "        epoch_idx: (int) the index of the current epoch\n",
    "        tb_writer: (torch.utils.tensorboard.writer.SummaryWriter) the tensorboard writer\n",
    "    RETURNS:\n",
    "        last_loss: (float) the average batch loss for the epoch\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    for i, (image, path, masks) in enumerate(trainloader):\n",
    "        # print(f'train1epch image: {image}')\n",
    "        # print(f'train1epch path: {path}')\n",
    "\n",
    "        orig_image = cv2.imread(path[0])\n",
    "        orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        negative_map = (np.where(orig_image > 0, 1, 0)).astype(np.uint8)\n",
    "        negative_map = torch.from_numpy(negative_map)  \n",
    "        negative_map = negative_map.permute(2, 0, 1)\n",
    "        negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "        negative_map = negative_map.to(torch.uint8).squeeze(0).to(DEVICE)\n",
    "\n",
    "        del orig_image\n",
    "        \n",
    "        image = image.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(image, negative_map)\n",
    "        # print(f'pred shape: {pred.shape}')\n",
    "        masks = masks[0].to(DEVICE)\n",
    "        total_mask = get_totalmask(masks)\n",
    "        pred = pred.to(DEVICE)\n",
    "        print(pred.dtype, pred[0][0].unsqueeze(0).shape)\n",
    "        \n",
    "        loss = criterion(pred[0][0].unsqueeze(0).unsqueeze(0), total_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        binary_pred_mask = ((pred.detach().cpu().numpy()[0]) > 0.0).astype(int)\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 3)\n",
    "        \n",
    "        axs[0].imshow(image[0].permute(1,2,0).detach().cpu().numpy())\n",
    "        axs[0].axis('off')\n",
    "    \n",
    "        axs[1].imshow(total_mask.detach().cpu().numpy())\n",
    "        axs[1].set_title('Ground truth masks', fontsize=10)\n",
    "        axs[1].axis('off')\n",
    "        \n",
    "        # axs[2].imshow(pred.detach().cpu().numpy()[0][0])\n",
    "        axs[2].imshow(binary_pred_mask[0])\n",
    "        axs[2].set_title('Predicted masks', fontsize=10)\n",
    "        axs[2].axis('off')\n",
    "        # plt.savefig('on_img_train.png', dpi=300)\n",
    "    \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    i = len(trainloader)\n",
    "    last_loss = running_loss / i\n",
    "    print(f'batch_loss for batch {i}: {last_loss}')\n",
    "    tb_x = epoch_idx * len(trainloader) + i + 1\n",
    "    # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\" Trains the model for the given number of epochs.\"\"\"\n",
    "    bestmodel_path = \"\"\n",
    "    model = ModelSimple()\n",
    "    model.setup()\n",
    "    model.to(DEVICE)\n",
    "    img_size = model.model.image_encoder.img_size\n",
    "    trainloader, validloader = load_datasets(img_size=img_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_valid_loss = float('inf')\n",
    "    for epch in range(epochs): # type: ignore\n",
    "        running_vloss = 0.\n",
    "        model.train(True)\n",
    "        avg_batchloss = train_one_epoch(\n",
    "            model, trainloader, optimizer, epch)\n",
    "        eval = False\n",
    "        print(f'epoch: {epch}, train loss: {avg_batchloss}')\n",
    "        if not eval: # type: ignore\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            for images, path, masks in validloader:\n",
    "                print(images)\n",
    "                model.to(DEVICE)\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks[0].to(DEVICE)\n",
    "                total_mask = get_totalmask(masks)\n",
    "                total_mask = total_mask.to(DEVICE)\n",
    "                model.eval()\n",
    "                preds, iou = model(images)\n",
    "                preds = preds.to(DEVICE)\n",
    "                vloss = criterion(preds, total_mask)\n",
    "                running_vloss += vloss.item()\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        avg_vloss = running_vloss / len(validloader)\n",
    "        # save model\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        print(f'best valid loss: {best_valid_loss}')\n",
    "        if running_vloss < best_valid_loss:\n",
    "          best_model = model\n",
    "    return model\n",
    "\n",
    "# next add train function for prompting model with points (ModelAll)\n",
    "# next unrelated find out how to get tensor output of masks form SamAutomaticMaskGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFWOAdEY7upr"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSqrkj4w7uLh",
    "outputId": "053c9b1b-6609-4980-949d-2f3faf4edf04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sam = sam_model_registry[\"vit_h\"](checkpoint=\"./sam_vit_h_4b8939.pth\")\n",
    "# mask_generator = SamAutomaticMaskGenerator(model.model)\n",
    "# # sam.to(DEVICE)\n",
    "# predicted_masks = mask_generator.generate(sample_img) # this doesn't really work this way, but rather in the way the model was fine-tuned\n",
    "# print(len(predicted_masks))\n",
    "\n",
    "# total_mask = build_totalmask(predicted_masks)\n",
    "# plt.imshow(total_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FrAGnJX8vwH"
   },
   "source": [
    "## Predict trained model\n",
    "\n",
    "Here I assume that you have fine-tuned SAM with the code above and the fine-tuned model is saved as model_final.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf8QC8hobCQe"
   },
   "outputs": [],
   "source": [
    "# predict trainied model\n",
    "model_trained = ModelSimple()\n",
    "model_trained.setup()\n",
    "with torch.no_grad():\n",
    "    model_trained.load_state_dict(torch.load('model_final.pth', map_location=torch.device(\"cuda:7\")))\n",
    "    model_trained.eval()\n",
    "    img_size = model_trained.model.image_encoder.img_size\n",
    "    print(f'img_size: {img_size}')\n",
    "    print(f'image_encoder.img_size: {model_trained.model.image_encoder.img_size}')\n",
    "    running_vloss = 0.\n",
    "    for images, path, masks in validloader:\n",
    "\n",
    "        orig_image = cv2.imread(path[0])\n",
    "        orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        negative_map = (np.where(orig_image > 0, 1, 0)).astype(np.uint8)\n",
    "        negative_map = torch.from_numpy(negative_map)  \n",
    "        negative_map = negative_map.permute(2, 0, 1)\n",
    "        negative_map = resize(negative_map, [1024, 1024], antialias=True) \n",
    "        negative_map = negative_map.to(torch.uint8).squeeze(0).to(DEVICE)\n",
    "        \n",
    "        # print(images)\n",
    "        model.to(DEVICE)\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks[0].to(DEVICE)\n",
    "        total_mask = get_totalmask(masks)\n",
    "        total_mask = total_mask.to(DEVICE)\n",
    "        model.eval()\n",
    "        preds, iou = model(images, negative_map)\n",
    "        preds = preds.to(DEVICE)\n",
    "\n",
    "        print(preds.shape, total_mask.shape)\n",
    "        \n",
    "        vloss = criterion(preds[0][0].unsqueeze(0).unsqueeze(0), total_mask) # was criterion(preds, total_mask) before. i changes because the multimask_output is set on True\n",
    "        running_vloss += vloss.item()\n",
    "        np_image = images[0].permute(1,2,0).detach().cpu().numpy()\n",
    "        np_mask = masks.detach().cpu().numpy()\n",
    "        np_pred = preds[0].detach().cpu().numpy()\n",
    "\n",
    "        # binary_pred_mask = ((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int) * 255\n",
    "        binary_pred_mask = ((preds[0]).cpu().numpy()[0] > 0).astype(int) * 255\n",
    "        \n",
    "        print(np.mean((F.sigmoid(preds[0]).cpu().numpy()[0])), np.min((F.sigmoid(preds[0]).cpu().numpy()[0])), np.max((F.sigmoid(preds[0]).cpu().numpy()[0])))\n",
    "        print(np.unique((F.sigmoid(preds[0]).cpu().numpy()[0]) > 0.5).astype(int))\n",
    "        print(\"validation loss: \", running_vloss)\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 3)\n",
    "        \n",
    "        axs[0].imshow(np_image)\n",
    "        axs[0].axis('off')\n",
    "    \n",
    "        axs[1].imshow(get_totalmask(masks).detach().cpu().numpy())\n",
    "        axs[1].set_title('Ground truth masks', fontsize=10)\n",
    "        axs[1].axis('off')\n",
    "        \n",
    "        axs[2].imshow(binary_pred_mask)\n",
    "        axs[2].set_title('Predicted masks', fontsize=10)\n",
    "        axs[2].axis('off')\n",
    "        # plt.savefig('on_img_train.png', dpi=300)\n",
    "    \n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU1aS-ow9ZUy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
