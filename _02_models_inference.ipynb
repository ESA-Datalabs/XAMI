{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from pycocotools import mask as maskUtils\n",
    "# import cv2\n",
    "# import supervision as sv\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# !pip install astropy\n",
    "# import astropy\n",
    "# from astropy.io import fits\n",
    "# from scipy.interpolate import interp1d\n",
    "# from astropy.visualization import ZScaleInterval, ImageNormalize\n",
    "import torch\n",
    "import random\n",
    "import sys\n",
    "# import os\n",
    "import numpy as np\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "import supervision as sv\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# import math\n",
    "# from torch import cuda\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my utils\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OM_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'\n",
    "OM_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Roboflow annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0==1:\n",
    "    \n",
    "    def display_masks(image_path, masks):\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "    \n",
    "        for mask in masks:\n",
    "            if isinstance(mask[0], list):  # If the mask is a polygon\n",
    "                polygon_points = np.array(mask[0]).reshape(-1, 2)\n",
    "                polygon = Polygon(polygon_points, edgecolor='g', facecolor='none')\n",
    "                ax.add_patch(polygon)\n",
    "            else:  # If the mask is RLE\n",
    "                binary_mask = maskUtils.decode(mask)\n",
    "                ax.imshow(binary_mask, alpha=0.5, cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    # Load the JSON file\n",
    "    dir_train_path = './xmm_om_images_v4-contrast-512-5-7/train/'\n",
    "    json_file_path = dir_train_path+'_annotations.coco.json'\n",
    "    \n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f) # dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
    "    \n",
    "    # Iterate through each image and its annotations\n",
    "    for image_info in data['images']:\n",
    "        image_id = image_info['id']\n",
    "        image_path = dir_train_path + image_info['file_name']\n",
    "        \n",
    "        # Find annotations for the current image\n",
    "        annotations = [anno for anno in data['annotations'] if anno['image_id'] == image_id]\n",
    "        # annotations: dict_keys(['id', 'image_id', 'category_id', 'bbox', 'area', 'segmentation', 'iscrowd'])\n",
    "        \n",
    "        # Extract and display masks for the image\n",
    "        masks = [anno['segmentation'] for anno in annotations]\n",
    "        display_masks(image_path, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mobile SAM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/') # MobileSAM repo path\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device=device)\n",
    "mobile_sam_model.eval();\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileSAM_predict(image_path: str, \n",
    "                      output_path: Optional[str] = None,\n",
    "                      mask_on_negative: Optional[np.ndarray] = None\n",
    "                     ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Predicts segmentation masks for an input image using the MobileSAM model and optionally annotates the image with the segmentation results.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - mask_on_negative (Optional[np.ndarray], optional): A negative map specifying the bad (-ve) pixel regions used for removing unuseful segmentations masks.\n",
    "    If provided, masks identified as negative based on this parameter are removed from the annotation. Defaults to None.\n",
    "    - output_path (str): The output path for the annotated image. Defaults to None. \n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray] : The annotated image array and the resulted masks.\n",
    "    \n",
    "    Notes:\n",
    "    - The function uses OpenCV to read and preprocess the input image.\n",
    "    - It utilizes a pretrained MobileSAM model for prediction.\n",
    "    - The prediction process involves normalizing the image based on its pixel mean and standard deviation.\n",
    "    - The `mask_on_negative` parameter allows for further processing to remove certain masks based on the provided negative mask.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # (H, W, C)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # using the pixel mean and std specific to each image instead of the standard one (this step can be ignored)\n",
    "        image_T = np.transpose(image, (2, 1, 0))\n",
    "        pixel_mean = torch.as_tensor([np.mean(image_T[0]), np.mean(image_T[1]),np.mean(image_T[2])], dtype=torch.float, device=device)\n",
    "        pixel_std = torch.as_tensor([np.std(image_T[0]), np.std(image_T[1]),np.std(image_T[2])], dtype=torch.float, device=device)\n",
    "        mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "        predictor = SamPredictor(mobile_sam_model)\n",
    "        predictor.set_image(image)\n",
    "        \n",
    "        mask_generator = SamAutomaticMaskGenerator(mobile_sam_model)\n",
    "        mobile_sam_result = mask_generator.generate(image)\n",
    "        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        print(image.shape)\n",
    "        if mask_on_negative is not None:\n",
    "            mobile_sam_result = remove_masks(sam_result=mobile_sam_result, \n",
    "                                             mask_on_negative=mask_on_negative, \n",
    "                                             threshold=image.shape[0]**2/4, \n",
    "                                             remove_big_masks=True, \n",
    "                                             img_shape = image.shape)\n",
    "\n",
    "        detections = sv.Detections.from_sam(mobile_sam_result)\n",
    "        annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "\n",
    "        annotated_image = annotated_image * (image>0).astype(float) # mask negative pixels\n",
    "        \n",
    "        if annotated_image.max() <= 1.0:\n",
    "            annotated_image *= 255\n",
    "\n",
    "        annotated_image = annotated_image.astype(np.uint8)\n",
    "        image = Image.fromarray(annotated_image)\n",
    "        \n",
    "        if output_path:\n",
    "            image.save(output_path)\n",
    "\n",
    "    return image, mobile_sam_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = './xmm_om_images_v4-contrast-512-5-7/train/S0037980401_L_png.rf.17cd9454f2c96e8a3e06676a49f2640b.jpg'\n",
    "image_fits_path = '../XMM_OM_dataset/scaled_raw_512/S0037980401_L.fits'\n",
    "\n",
    "with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "    extracted_bboxes_points = json.load(f)\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if filename in image_path.split(\"/\")[-1]])/255.0\n",
    "mask_on_negative = mask_and_plot_image(image_fits_path)\n",
    "output_path = image_path.split('/')[-1].replace(\".png\", \"_mobile_sam_nonnegative.png\")\n",
    "start_time = time.time()\n",
    "image, mobile_sam_result = MobileSAM_predict(image_path, output_path=output_path, mask_on_negative=mask_on_negative)\n",
    "end_time = time.time()\n",
    "print(f\"Mobile SAM predict time/img: {end_time-start_time} s\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title('Mobile SAM pred. on image input')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN3DPGZSn57p"
   },
   "source": [
    "# 🚀 Segment Anything Model (SAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SAM model and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1H9YruJen0Q8",
    "outputId": "9098dc8e-3476-4166-d48e-d88e3ba76267"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "# !pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VeYIWh1iDWW"
   },
   "source": [
    "## Download SAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aszw1OxBwowI",
    "outputId": "64f3e115-2442-4181-9a7e-4eafdb4464f2"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "# !mkdir {HOME}/weights\n",
    "# %cd {HOME}/weights\n",
    "\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlhbd_f4xfiJ"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6_9PSZupghA"
   },
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "DEVICE = torch.device(f'cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "sam.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 The predictor function (can remove masks on negative pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = \"./xmm_om_images_v4-contrast-512-5-7/train/S0037980401_L_png.rf.17cd9454f2c96e8a3e06676a49f2640b.jpg\"\n",
    "mask_on_negative = mask_and_plot_image(\"../XMM_OM_dataset/scaled_raw_512/S0037980401_L.fits\")\n",
    "start_time = time.time()\n",
    "_, _, annotated_image1 = SAM_predictor(SamAutomaticMaskGenerator, sam, image_path, mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "print(f\"original SAM predict time/img: {time.time()-start_time} s\")\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if filename in image_path.split(\"/\")[-1]])/255\n",
    "if len(img_points)>0: # TODO: solve the error with points format\n",
    "    start_time2 = time.time()\n",
    "    _, _, annotated_image2 = SAM_predictor(SamAutomaticMaskGenerator, sam, image_path, mask_on_negative=mask_on_negative, img_grid_points=img_points)\n",
    "    print(f\"original SAM predict time/img: {time.time()-start_time2} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(annotated_image1)\n",
    "plt.title('SAM pred. on image input')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs= plt.subplots(1, 3, figsize=(10, 10)) \n",
    "\n",
    "# axs[0].imshow(annotated_image1)\n",
    "# axs[0].set_title(f'Original SAM inference \\ntime: {round(end_time-start_time, 3)}s, {1024} grid points', \\\n",
    "#                  fontfamily='monospace', fontsize=10)\n",
    "\n",
    "# axs[1].imshow(annotated_image2)\n",
    "# axs[1].set_title(f'Original SAM inference\\ntime: {round(end_time2-start_time2, 3)}s, {img_points.shape[1]} grid points', \\\n",
    "#                 fontfamily='monospace', fontsize=10)\n",
    "\n",
    "# axs[2].imshow(cv2.cvtColor(cv2.imread(image_path2), cv2.COLOR_BGR2RGB))\n",
    "# axs[2].scatter(img_points[0][:, 0]*255, img_points[0][:, 1]*255, s=10, c='red')\n",
    "# axs[2].set_title(f'Extracted sources grid points', \\\n",
    "#                  fontfamily='monospace', fontsize=10)\n",
    "# axs[2].set_aspect('equal', 'box')  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('plots/sam_grid_points_comparison.png', dpi=1000)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "\n",
    "generate_all_predictions = True\n",
    "\n",
    "inference_times = []\n",
    "if generate_all_predictions:\n",
    "    dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "    for file_ in dir_files:\n",
    "        try:\n",
    "            if '.png' in file_ and 'clahe' in file_:\n",
    "                \n",
    "                mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in file_.split(\"/\")[-1]])\n",
    "                img_points = img_points/255.0\n",
    "                \n",
    "                start_time = time.time()\n",
    "                _, _, annotated_image = SAM_predictor(SamAutomaticMaskGenerator, sam, OM_dir+file_, mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(f\"original SAM predict time/img: {end_time-start_time} s\")\n",
    "                inference_times.append(end_time-start_time)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    # with open('cell_SAM_predict_with_threshold.txt', 'w') as f:\n",
    "    #     f.write(str(cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "\n",
    "generate_all_predictions = True\n",
    "\n",
    "inference_times = []\n",
    "if generate_all_predictions:\n",
    "    dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "    for file_ in dir_files:\n",
    "        try:\n",
    "            if '.png' in file_ and 'clahe' in file_:\n",
    "                \n",
    "                mask_on_negative = mask_and_plot_image(OM_dir+file_.replace('.png', '.fits').replace('clahe_', ''))\n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in file_.split(\"/\")[-1]])\n",
    "                img_points = img_points/255.0\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                if len(img_points)==0:\n",
    "                    img_points = None\n",
    "                    \n",
    "                _, _, annotated_image = SAM_predictor(SamAutomaticMaskGenerator, sam, OM_dir+file_, mask_on_negative=mask_on_negative, \n",
    "                                                           img_grid_points=img_points)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(f\"original SAM predict time/img: {end_time-start_time} s\")\n",
    "                inference_times.append(end_time-start_time)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Generate annotation json file (COCO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    print(box[0], box[1], box[2],  box[3]) \n",
    "    w, h = box[2], box[3] \n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def numpy_to_list(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: numpy_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [numpy_to_list(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Initialize a dictionary to store all images and annotations\n",
    "categories = []\n",
    "\n",
    "# coco_style_annotations = {'categories': categories, 'images': [], 'annotations': []}\n",
    "coco_style_annotations = {'annotations': []}\n",
    "\n",
    "\n",
    "def get_SAM_annotations(IMAGE_PATH, mask_on_negative = None, output_mode=\"binary_mask\"):\n",
    "    \"\"\"\n",
    "    This function calls SAM (Segment Anything) and gets annotations for a given image.\n",
    "    Args:\n",
    "        IMAGE_PATH (str): The path to the image file.\n",
    "        remove_masks_on_negative (bool, optional): If True, masks on negative detections are removed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the SAM results, detections, and the annotated image.\n",
    "    \"\"\"\n",
    "    image_bgr = cv2.imread(IMAGE_PATH)\n",
    "    annotated_image, detections, sam_result = None, None, None\n",
    "\n",
    "    # try:\n",
    "    if True:\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mask_generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode)\n",
    "        \n",
    "        sam_result = mask_generator.generate(image_rgb)\n",
    "        if mask_on_negative is not None:\n",
    "            sam_result = remove_masks(sam_result=sam_result,mask_on_negative=mask_on_negative, threshold=50)\n",
    "            \n",
    "        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "        annotated_image = mask_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "        # plt.imshow(annotated_image)\n",
    "        # plt.show()\n",
    "    # except Exception as e:\n",
    "    #     print(\"Exception:\\n\", IMAGE_PATH, e)\n",
    "    #     pass\n",
    "        \n",
    "    return sam_result, detections, annotated_image\n",
    "\n",
    "\n",
    "# Run SAM for .png files in the directory and create the annotation json file in ~COCO format\n",
    "\n",
    "def generate_json_file(input_dir, coco_style_annotations):\n",
    "    for file_ in os.listdir(input_dir):\n",
    "    # if True:\n",
    "        # file_ = 'S0720251301_L.png'\n",
    "        if \"png\" in file_:\n",
    "\n",
    "            print(file_)\n",
    "            mask_on_negative = mask_and_plot_image(input_dir+file_.replace('.png', '.fits'))\n",
    "            sam_result_i, detections_i, annotated_image_i = get_SAM_annotations(input_dir+file_, mask_on_negative.astype(int))\n",
    "            \n",
    "            sam_result_i = numpy_to_list(sam_result_i)\n",
    "            img = cv2.imread(os.path.join(input_dir, file_))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "            height, width, _ = img.shape\n",
    "            # print(k)\n",
    "            # plt.figure(figsize=(10,10))\n",
    "            # print(type(annotated_image_i))\n",
    "            # plt.imshow(annotated_image_i)\n",
    "            # # show_box(annotation['bbox'], plt.gca())\n",
    "            # # plt.axis('off')\n",
    "            # plt.show()\n",
    "            # coco_style_annotations['images'].append({\n",
    "            #             'id': file_.split('.')[0],\n",
    "            #             'license': 1,\n",
    "            #             'file_name': input_dir+file_,\n",
    "            #             'height':height, \n",
    "            #             'width': width,  \n",
    "            #             'date_captured': datetime.now().isoformat(), \n",
    "            #         })\n",
    "            k=0\n",
    "            for annotation in sam_result_i:\n",
    "                # xyhw = annotation['bbox']\n",
    "                # if (xyhw[2] >2 or xyhw[3] >2) and (xyhw[2]*1.0/height < 0.7 and xyhw[3]*1.0/width < 0.7):\n",
    "                if True:\n",
    "                    coco_style_annotations['annotations'].append({\n",
    "                            'id': f'{file_.split(\".\")[0]}_mask{k}',\n",
    "                            'image_id': file_.split('.')[0], \n",
    "                            'category_id': 0,  \n",
    "                            'segmentation': annotation['segmentation'],\n",
    "                            'area': annotation['area'],\n",
    "                            'bbox': annotation['bbox'],\n",
    "                            'iscrowd': 0,\n",
    "                        })\n",
    "                    k+=1\n",
    "    \n",
    "    with open('SAM_annotations_coco_style_v2.json', 'w') as f:\n",
    "    # with open('SAM_annotations_coco_style_img1.json', 'w') as f:\n",
    "        json.dump(coco_style_annotations, f)\n",
    "\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/'\n",
    "generate_json_file(input_dir, coco_style_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Generate annotation json file (VOC format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"my_apy_key\")\n",
    "upload_project = rf.workspace(\"my_username\").project(\"xmm_om_images_v4-contrast-512-5\") # error if the project doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and export SAM annotations in VOC format to Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import my utils\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils, voc_annotate_and_Roboflow_export\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "reload(voc_annotate_and_Roboflow_export)\n",
    "\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "from voc_annotate_and_Roboflow_export import * \n",
    "\n",
    "input_dir = './temp_images/'\n",
    "\n",
    "k=0\n",
    "if 0==1:\n",
    "    input_fits_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'\n",
    "    for file_ in os.listdir(input_dir):\n",
    "        # if k<1000:\n",
    "        #     k+=1\n",
    "        # else:\n",
    "        #     if k>1030:\n",
    "        #         break\n",
    "        #     k+=1\n",
    "            # if k>3:\n",
    "            #      break\n",
    "            # else:\n",
    "            #     k+=1\n",
    "            mask_on_negative = mask_and_plot_image(input_fits_dir+file_.replace('.png', '.fits'), plot_=False)\n",
    "    \n",
    "            sam_result_i, detections_i, annotated_image_i = SAM_predictor(SamAutomaticMaskGenerator, sam, input_dir+file_, \n",
    "                                                                          mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "            if sam_result_i is not None and detections_i is not None and annotated_image_i is not None:\n",
    "                objects = []\n",
    "                for annotation in sam_result_i: # a mask over an image is a binary array with shape (img_h, img_w)\n",
    "                    polygon = binary_image_to_polygon(annotation['segmentation'])\n",
    "                    # plot_polygon(polygon[0], annotated_image_i) # to see the masks polygons\n",
    "                    objects.append({\n",
    "                        'name': 'star',\n",
    "                        'bbox': annotation['bbox'],\n",
    "                        'segmentations': polygon[0]\n",
    "                    })\n",
    "\n",
    "                print(file_)\n",
    "                    \n",
    "                create_annotation_SAM(filename=file_, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "                image_path = input_dir+file_\n",
    "                annotation_filename = file_.replace(\".png\", \".xml\")\n",
    "                print(annotation_filename)\n",
    "                \n",
    "                upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "                os.remove(annotation_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert annotations from **COCO** format to VOC and mount to Roboflow\n",
    "\n",
    "📍📍📍 - in Roboflow, the SAM annotated files end with **.png** while from COCO json to VOC they end in **_png**! The workaround is that I generate other temp image files with .png not to have duplicates in Roboflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Check the coorrdinates! \n",
      "S0300930301_L.png.rf.bd9aa0ba26185b22de57c2d40f318c46.jpg\n",
      " Check the coorrdinates! \n",
      "S0551640201_M.png.rf.c08ac219c9ec35325916bce986cd8869.jpg\n",
      " Check the coorrdinates! \n",
      "S0109130201_B.png.rf.be4119c9fb3335fbc6e45e50f7abc049.jpg\n",
      " Check the coorrdinates! \n",
      "S0604010201_L.png.rf.bc22ce5dcccd52fcea0fcfd65b6bb36d.jpg\n",
      " Check the coorrdinates! \n",
      "S0700381101_L.png.rf.be6aa3d8694097cae7d09ffeb1c61ef5.jpg\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import json \n",
    "\n",
    "import voc_annotate_and_Roboflow_export\n",
    "reload(voc_annotate_and_Roboflow_export)\n",
    "from voc_annotate_and_Roboflow_export import * # moved the files there\n",
    "\n",
    "input_dir = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-6/train/\"\n",
    "json_file_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/-xmm_om_images_v4-contrast-512-5-6/train/_annotations.coco.json'\n",
    "\n",
    "output_d = \"/workspace/raid/OM_DeepLearning/VOC_xmm_om_images-contrast-512-v5-3/\"\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    data_in = json.load(f)\n",
    "\n",
    "k=0\n",
    "if 1==1:\n",
    "    for im in data_in['images']: \n",
    "            k+=1  \n",
    "            if k>6:\n",
    "                 break  \n",
    "            objects = []\n",
    "            file_ = im['file_name']\n",
    "            extension = \".\"+file_.split(\".\")[-1]\n",
    "            masks = [data_in['annotations'][a] for a in range(len(data_in['annotations'])) if data_in['annotations'][a]['image_id'] == im['id']]\n",
    "            classes = [data_in['annotations'][a]['category_id'] for a in range(len(data_in['annotations'])) if data_in['annotations'][a]['image_id'] == im['id']]\n",
    "            class_categories = {data_in['categories'][a]['id']:data_in['categories'][a]['name'] for a in range(len(data_in['categories']))}\n",
    "\n",
    "            temp_img = cv2.imread(input_dir+im[\"file_name\"])\n",
    "            temp_img = cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(f\"./{file_.replace('_png', '.png')}\", temp_img)\n",
    "\n",
    "            for i in range(len(masks)):\n",
    "                segmentation = masks[i]['segmentation']\n",
    "                if isinstance(segmentation, list):\n",
    "                    if len(segmentation) > 0 and isinstance(segmentation[0], list):\n",
    "                        points = segmentation[0]\n",
    "                        h_img, w_img = temp_img.shape[:2]\n",
    "                            \n",
    "                binary_m = create_mask(points, (h_img, w_img)) # COCO segmentations are polygon points, and must be converted to masks\n",
    "                            \n",
    "                objects.append({\n",
    "                    'name': class_categories[classes[i]],\n",
    "                    'bbox': mask_to_bbox(binary_m),\n",
    "                    'segmentations': segmentation[0]\n",
    "                })\n",
    "\n",
    "            create_annotation(filename=file_.replace('_png', '.png'), width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format \n",
    "            image_path = file_.replace('_png', '.png')\n",
    "            annotation_filename = file_.replace('_png', '.png').replace(extension, \".xml\")\n",
    "\n",
    "            print(image_path)\n",
    "\n",
    "            new_lines = ['<annotation>\\n','\t<folder></folder>\\n']\n",
    "            \n",
    "            with open(annotation_filename, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            del lines[:3]\n",
    "            \n",
    "            modified_lines = new_lines + lines\n",
    "            \n",
    "            with open(annotation_filename, 'w') as file:\n",
    "                file.writelines(modified_lines)\n",
    "\n",
    "            upload_project.upload(image_path, annotation_filename, overwrite=True)\n",
    "            del temp_img\n",
    "            os.remove(annotation_filename)\n",
    "            os.remove(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export images and VOC annotations in VOC format to Roboflow (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"my_api_key\")\n",
    "upload_project = rf.workspace(\"my_username\").project(\"xmm_om_images_v4-contrast-512-3\")\n",
    "\n",
    "dataset_images_folder = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_stretched/'\n",
    "annotations_voc_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-1/train/'\n",
    "annotations_ = []\n",
    "\n",
    "for annot in os.listdir(annotations_voc_dir):\n",
    "    if annot.endswith('.xml'):\n",
    "        annotations_.append(annot)\n",
    "\n",
    "for image_name in os.listdir(dataset_images_folder):\n",
    "    image_path = os.path.join(dataset_images_folder, image_name)\n",
    "    if os.path.isfile(image_path):\n",
    "        print(image_path.split('/')[-1].replace('.', '_'))\n",
    "        annotations_voc_filename = [annotation for annotation in annotations_ if annotation.startswith(image_path.split('/')[-1].replace('.', '_'))]\n",
    "        if len(annotations_voc_filename):\n",
    "            upload_project.upload(image_path, annotations_voc_dir+annotations_voc_filename[0], overwrite=True)\n",
    "\n",
    "print(\"Image upload complete.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
