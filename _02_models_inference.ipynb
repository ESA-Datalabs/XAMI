{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from pycocotools import mask as maskUtils\n",
    "import torch\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import time\n",
    "import cv2\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "import data_preprocess, class_agnostic_sam_predictor\n",
    "from data_preprocess import preprocess_utils as preprocess\n",
    "from class_agnostic_sam_predictor import predictor_utils as predict\n",
    "\n",
    "# from preprocess_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OM_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'\n",
    "OM_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and visualize Roboflow annotations\n",
    "\n",
    "This step assumes that you have downloaded the dataset locally, in COCO format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0==1:\n",
    "    \n",
    "    def display_masks(image_path, masks):\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "    \n",
    "        for mask in masks:\n",
    "            if isinstance(mask[0], list):  # If the mask is a polygon\n",
    "                polygon_points = np.array(mask[0]).reshape(-1, 2)\n",
    "                polygon = Polygon(polygon_points, edgecolor='g', facecolor='none')\n",
    "                ax.add_patch(polygon)\n",
    "            else:  # If the mask is RLE\n",
    "                binary_mask = maskUtils.decode(mask)\n",
    "                ax.imshow(binary_mask, alpha=0.5, cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    dir_train_path = './xmm_om_images_v4-contrast-512-5-7/train/'\n",
    "    json_file_path = dir_train_path+'_annotations.coco.json'\n",
    "    \n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f) \n",
    "    \n",
    "    for image_info in data['images']:\n",
    "        image_id = image_info['id']\n",
    "        image_path = dir_train_path + image_info['file_name']\n",
    "        \n",
    "        annotations = [anno for anno in data['annotations'] if anno['image_id'] == image_id]\n",
    "        # annotations: dict_keys(['id', 'image_id', 'category_id', 'bbox', 'area', 'segmentation', 'iscrowd'])\n",
    "        \n",
    "        masks = [anno['segmentation'] for anno in annotations]\n",
    "        display_masks(image_path, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mobile SAM inference\n",
    "\n",
    "Here, the SAM Auto Mask Generator is used to predict masks on an input image. The predictor generates $32\\times32$ grid points which represent foreground point input prompts for the mask decoder and filteres the best masks for prediciton. However, some methods are have the `torch.no_grad()` decorator thus such predictor cannot be trained. One idea would be to add gradients <i>on the way</i>, but it is risky and is not targeted for this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/') # MobileSAM repo path\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device=device)\n",
    "mobile_sam_model.eval();\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = './roboflow_datasets/xmm_om_images_v4-contrast-512-5-7/train/S0037980401_L_png.rf.17cd9454f2c96e8a3e06676a49f2640b.jpg'\n",
    "image_fits_path = '../XMM_OM_dataset/scaled_raw_512/S0037980401_L.fits'\n",
    "\n",
    "mask_on_negative = predict.mask_and_plot_image(image_fits_path)\n",
    "output_path = image_path.split('/')[-1].replace(\".png\", \"_mobile_sam_nonnegative.png\")\n",
    "start_time = time.time()\n",
    "image, mobile_sam_result = predict.MobileSAM_predict(\n",
    "    image_path, \n",
    "    model=mobile_sam_model,\n",
    "    predictor=SamPredictor,\n",
    "    generator=SamAutomaticMaskGenerator,\n",
    "    device=device, \n",
    "    output_path=output_path, \n",
    "    mask_on_negative=mask_on_negative)\n",
    "end_time = time.time()\n",
    "print(f\"Mobile SAM predict time/img: {end_time-start_time} s\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title('Mobile SAM pred. on image input')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN3DPGZSn57p"
   },
   "source": [
    "# ðŸš€ Segment Anything Model (SAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SAM model and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1H9YruJen0Q8",
    "outputId": "9098dc8e-3476-4166-d48e-d88e3ba76267"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "# !pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VeYIWh1iDWW"
   },
   "source": [
    "## Download SAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aszw1OxBwowI",
    "outputId": "64f3e115-2442-4181-9a7e-4eafdb4464f2"
   },
   "outputs": [],
   "source": [
    "# %cd {HOME}\n",
    "# !mkdir {HOME}/weights\n",
    "# %cd {HOME}/weights\n",
    "\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlhbd_f4xfiJ"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6_9PSZupghA"
   },
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "DEVICE = torch.device(f'cuda:1' if torch.cuda.is_available() else 'cpu') \n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "sam.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ The predictor function (can remove masks on negative pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_on_negative = predict.mask_and_plot_image(\"../XMM_OM_dataset/scaled_raw_512/S0012850201_L.fits\", plot_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMG with extracted and deblended sources centroids\n",
    "\n",
    "Generating 1024 points on an astronomical image may be redundant, as the usual Field of View contains way fewer noticeable sources. In this way, we can reduce inference time by using deblended sources centroids resulted from a source detection algorithm. This approach yields faster result but is sometimes less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_id = 'S0801800201_L'\n",
    "image_path = f\"../XMM_OM_dataset/zscaled_512_stretched/{obs_id}.png\"\n",
    "mask_on_negative = predict.mask_and_plot_image(f\"../XMM_OM_dataset/scaled_raw_512/{obs_id}.fits\")\n",
    "start_time = time.time()\n",
    "\n",
    "with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "    extracted_bboxes_points = json.load(f)\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if obs_id in filename])/255.0\n",
    "\n",
    "_, _, annotated_image1 = predict.SAM_predictor(\n",
    "    SamAutomaticMaskGenerator, \n",
    "    sam, image_path, \n",
    "    mask_on_negative=mask_on_negative, \n",
    "    img_grid_points=None)\n",
    "sam_grid_time = time.time()-start_time\n",
    "\n",
    "img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                       if filename in image_path.split(\"/\")[-1]])/255\n",
    "if len(img_points)>0:\n",
    "    start_time = time.time()\n",
    "    _, _, annotated_image2 = predict.SAM_predictor(\n",
    "        SamAutomaticMaskGenerator, \n",
    "        sam, \n",
    "        image_path, \n",
    "        mask_on_negative=None, \n",
    "        img_grid_points=img_points)\n",
    "    sam_with_detected_sources_time = time.time()-start_time\n",
    "    \n",
    "fig, axs= plt.subplots(1, 3, figsize=(10, 10)) \n",
    "axs[0].imshow(annotated_image1)\n",
    "axs[0].set_title(f'Original SAM inference \\ntime: {round(sam_grid_time, 3)}s, {1024} grid points', \\\n",
    "                 fontfamily='monospace', fontsize=10)\n",
    "\n",
    "axs[1].imshow(annotated_image2)\n",
    "axs[1].set_title(f'SAM inference \\nwith deblended sources centroids \\ntime: '+\\\n",
    "                 f'{round(sam_with_detected_sources_time, 3)}s, {img_points.shape[1]} grid points', \\\n",
    "                fontfamily='monospace', fontsize=10)\n",
    "\n",
    "axs[2].imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "axs[2].scatter(img_points[0][:, 0]*255, img_points[0][:, 1]*255, s=10, c='red')\n",
    "axs[2].set_title(f'Extracted sources grid points', \\\n",
    "                 fontfamily='monospace', fontsize=10)\n",
    "axs[2].set_aspect('equal', 'box')  \n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/sam_grid_points_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_predictions = True\n",
    "\n",
    "inference_times = []\n",
    "if generate_all_predictions:\n",
    "    dir_files = [f for f in os.listdir(OM_dir) if os.path.isfile(os.path.join(OM_dir, f))]\n",
    "    for file_ in dir_files:\n",
    "        try:\n",
    "            if '.png' in file_:\n",
    "                image_path = OM_dir+'/'+file_\n",
    "                mask_on_negative = predict.mask_and_plot_image(image_path.replace('.png', '.fits'))\n",
    "                start_time = time.time()\n",
    "                \n",
    "                with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "                    extracted_bboxes_points = json.load(f)\n",
    "                \n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in image_path])/255.0\n",
    "                \n",
    "                _, _, annotated_image1 = predict.SAM_predictor(\n",
    "                    SamAutomaticMaskGenerator, \n",
    "                    sam, \n",
    "                    image_path, \n",
    "                    mask_on_negative=mask_on_negative, \n",
    "                    img_grid_points=None)\n",
    "                sam_grid_time = time.time()-start_time\n",
    "                \n",
    "                img_points = np.array([point for filename, point in extracted_bboxes_points['points'].items() \n",
    "                                       if filename in image_path.split(\"/\")[-1]])/255\n",
    "                if len(img_points)>0:\n",
    "                    start_time = time.time()\n",
    "                    _, _, annotated_image2 = predict.SAM_predictor(\n",
    "                        SamAutomaticMaskGenerator, \n",
    "                        sam, \n",
    "                        image_path, \n",
    "                        mask_on_negative=None, \n",
    "                        img_grid_points=img_points)\n",
    "                    sam_with_detected_sources_time = time.time()-start_time\n",
    "                    \n",
    "                fig, axs= plt.subplots(1, 3, figsize=(10, 10)) \n",
    "                axs[0].imshow(annotated_image1)\n",
    "                axs[0].set_title(f'Original SAM inference \\ntime: {round(sam_grid_time, 3)}s, {1024} grid points', \\\n",
    "                                 fontfamily='monospace', fontsize=10)\n",
    "                \n",
    "                axs[1].imshow(annotated_image2)\n",
    "                axs[1].set_title(f'SAM inference \\nwith deblended sources centroids \\ntime: '+\\\n",
    "                     f'{round(sam_with_detected_sources_time, 3)}s, {img_points.shape[1]} grid points', \\\n",
    "                fontfamily='monospace', fontsize=10)\n",
    "                \n",
    "                axs[2].imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "                axs[2].scatter(img_points[0][:, 0]*255, img_points[0][:, 1]*255, s=10, c='red')\n",
    "                axs[2].set_title(f'Extracted sources grid points', \\\n",
    "                                 fontfamily='monospace', fontsize=10)\n",
    "                axs[2].set_aspect('equal', 'box')  \n",
    "                plt.tight_layout()\n",
    "                # plt.savefig('plots/sam_grid_points_comparison.png', dpi=300)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    # with open('cell_SAM_predict_with_threshold.txt', 'w') as f:\n",
    "    #     f.write(str(cap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Generate annotation json file (COCO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    print(box[0], box[1], box[2],  box[3]) \n",
    "    w, h = box[2], box[3] \n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def numpy_to_list(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: numpy_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [numpy_to_list(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "categories = []\n",
    "\n",
    "# coco_style_annotations = {'categories': categories, 'images': [], 'annotations': []}\n",
    "coco_style_annotations = {'annotations': []}\n",
    "\n",
    "def get_SAM_annotations(IMAGE_PATH, mask_on_negative = None, output_mode=\"binary_mask\"):\n",
    "    \"\"\"\n",
    "    This function calls SAM (Segment Anything) and gets annotations for a given image.\n",
    "    Args:\n",
    "        IMAGE_PATH (str): The path to the image file.\n",
    "        remove_masks_on_negative (bool, optional): If True, masks on negative detections are removed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the SAM results, detections, and the annotated image.\n",
    "    \"\"\"\n",
    "    image_bgr = cv2.imread(IMAGE_PATH)\n",
    "    annotated_image, detections, sam_result = None, None, None\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode)\n",
    "    \n",
    "    sam_result = mask_generator.generate(image_rgb)\n",
    "    if mask_on_negative is not None:\n",
    "        sam_result = remove_masks(sam_result=sam_result,mask_on_negative=mask_on_negative, threshold=50)\n",
    "        \n",
    "    mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "    annotated_image = mask_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "    \n",
    "    return sam_result, detections, annotated_image\n",
    "\n",
    "# Run SAM for image files in the directory and create the annotation json file in COCO format\n",
    "def generate_json_file(input_dir, coco_style_annotations):\n",
    "    for file_ in os.listdir(input_dir):\n",
    "        if \"png\" in file_:\n",
    "            mask_on_negative = mask_and_plot_image(input_dir+file_.replace('.png', '.fits'))\n",
    "            sam_result_i, detections_i, annotated_image_i = get_SAM_annotations(input_dir+file_, mask_on_negative.astype(int))\n",
    "            sam_result_i = numpy_to_list(sam_result_i)\n",
    "            img = cv2.imread(os.path.join(input_dir, file_))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            height, width, _ = img.shape\n",
    "            \n",
    "            for annotation in sam_result_i:\n",
    "                xyhw = annotation['bbox']\n",
    "                \n",
    "                # filter out very small/big objects\n",
    "                if (xyhw[2] >2 or xyhw[3] >2) and (xyhw[2]*1.0/height < 0.7 and xyhw[3]*1.0/width < 0.7):\n",
    "                    coco_style_annotations['annotations'].append({\n",
    "                            'id': f'{file_.split(\".\")[0]}_mask{k}',\n",
    "                            'image_id': file_.split('.')[0], \n",
    "                            'category_id': 0,  \n",
    "                            'segmentation': annotation['segmentation'],\n",
    "                            'area': annotation['area'],\n",
    "                            'bbox': annotation['bbox'],\n",
    "                            'iscrowd': 0,\n",
    "                        })\n",
    "                    k+=1\n",
    "    \n",
    "    with open('SAM_annotations_coco_style_v2.json', 'w') as f:\n",
    "        json.dump(coco_style_annotations, f)\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw/'\n",
    "generate_json_file(input_dir, coco_style_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Generate annotation json file (VOC format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"my_apy_key\")\n",
    "upload_project = rf.workspace(\"my_username\").project(\"xmm_om_images_v4-contrast-512-5\") # error if the project doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and export SAM annotations in VOC format to Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dataset, class_agnostic_sam_predictor\n",
    "from class_agnostic_sam_predictor import predictor_utils as predict\n",
    "from dataset import voc_annotate_and_Roboflow_export as voc\n",
    "\n",
    "input_dir = './temp_images/'\n",
    "\n",
    "k=0\n",
    "if 1==1:\n",
    "    input_fits_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'\n",
    "    for file_ in os.listdir(input_dir):\n",
    "        if k>1:\n",
    "            break\n",
    "        else:\n",
    "            k+=1\n",
    "            mask_on_negative = predict.mask_and_plot_image(input_fits_dir+file_.replace('.png', '.fits'), plot_=False)\n",
    "            sam_result_i, detections_i, annotated_image_i = predict.SAM_predictor(SamAutomaticMaskGenerator, sam, input_dir+file_, \n",
    "                                                                          mask_on_negative=mask_on_negative, img_grid_points=None)\n",
    "            if sam_result_i is not None and detections_i is not None and annotated_image_i is not None:\n",
    "                objects = []\n",
    "                for annotation in sam_result_i: # a mask over an image is a binary array with shape (img_h, img_w)\n",
    "                    polygon = voc.binary_image_to_polygon(annotation['segmentation'])\n",
    "                    # plot_polygon(polygon[0], annotated_image_i) # to see the masks polygons\n",
    "                    objects.append({\n",
    "                        'name': 'star',\n",
    "                        'bbox': annotation['bbox'],\n",
    "                        'segmentations': polygon[0]\n",
    "                    })\n",
    "\n",
    "                print(file_)\n",
    "                voc.create_annotation_SAM(filename=file_, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "                image_path = input_dir+file_\n",
    "                annotation_filename = file_.replace(\".png\", \".xml\")\n",
    "                \n",
    "                upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "                os.remove(annotation_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert annotations from **COCO** format to VOC and mount to Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import json \n",
    "import dataset\n",
    "from dataset import voc_annotate_and_Roboflow_export as voc\n",
    "\n",
    "input_dir = \"/workspace/raid/OM_DeepLearning/XMM_OM_code_git/roboflow_datasets/xmm_om_images_512_SG_SR_CR_only-10-COCO/train/\"\n",
    "json_file_path = input_dir+'_annotations.coco.json'\n",
    "\n",
    "output_d = \"/workspace/raid/OM_DeepLearning/VOC_xmm_om_images-contrast-512-v5-3/\"\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    data_in = json.load(f)\n",
    "\n",
    "k=0\n",
    "if 1==1:\n",
    "    for im in data_in['images']: \n",
    "            k+=1  \n",
    "            if k>1:\n",
    "                 break  \n",
    "            objects = []\n",
    "            file_ = im['file_name']\n",
    "            extension = \".\"+file_.split(\".\")[-1]\n",
    "            masks = [data_in['annotations'][a] for a in range(len(data_in['annotations'])) if data_in['annotations'][a]['image_id'] == im['id']]\n",
    "            classes = [data_in['annotations'][a]['category_id'] for a in range(len(data_in['annotations'])) if data_in['annotations'][a]['image_id'] == im['id']]\n",
    "            class_categories = {data_in['categories'][a]['id']:data_in['categories'][a]['name'] for a in range(len(data_in['categories']))}\n",
    "            temp_img = cv2.imread(input_dir+im[\"file_name\"])\n",
    "            temp_img = cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(f\"./{file_.replace('_png', '.png')}\", temp_img)\n",
    "\n",
    "            for i in range(len(masks)):\n",
    "                segmentation = masks[i]['segmentation']\n",
    "                if isinstance(segmentation, list):\n",
    "                    if len(segmentation) > 0 and isinstance(segmentation[0], list):\n",
    "                        points = segmentation[0]\n",
    "                        h_img, w_img = temp_img.shape[:2]\n",
    "                binary_m = create_mask(points, (h_img, w_img)) # COCO segmentations are polygon points, and must be converted to masks\n",
    "                objects.append({\n",
    "                    'name': class_categories[classes[i]],\n",
    "                    'bbox': mask_to_bbox(binary_m),\n",
    "                    'segmentations': segmentation[0]\n",
    "                })\n",
    "\n",
    "            voc.create_annotation(filename=file_.replace('_png', '.png'), width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format \n",
    "            image_path = file_.replace('_png', '.png')\n",
    "            annotation_filename = file_.replace('_png', '.png').replace(extension, \".xml\")\n",
    "            new_lines = ['<annotation>\\n','\t<folder></folder>\\n']\n",
    "            \n",
    "            with open(annotation_filename, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            del lines[:3]\n",
    "            \n",
    "            modified_lines = new_lines + lines\n",
    "            \n",
    "            with open(annotation_filename, 'w') as file:\n",
    "                file.writelines(modified_lines)\n",
    "\n",
    "            upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "            del temp_img\n",
    "            os.remove(annotation_filename)\n",
    "            os.remove(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export images and VOC annotations in VOC format to Roboflow (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"my_api_key\")\n",
    "upload_project = rf.workspace(\"my_username\").project(\"xmm_om_images_v4-contrast-512-3\")\n",
    "\n",
    "dataset_images_folder = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_stretched/'\n",
    "annotations_voc_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_code_git/xmm_om_images_v4-contrast-512-1/train/'\n",
    "annotations_ = []\n",
    "\n",
    "for annot in os.listdir(annotations_voc_dir):\n",
    "    if annot.endswith('.xml'):\n",
    "        annotations_.append(annot)\n",
    "\n",
    "for image_name in os.listdir(dataset_images_folder):\n",
    "    image_path = os.path.join(dataset_images_folder, image_name)\n",
    "    if os.path.isfile(image_path):\n",
    "        print(image_path.split('/')[-1].replace('.', '_'))\n",
    "        annotations_voc_filename = [annotation for annotation in annotations_ if annotation.startswith(image_path.split('/')[-1].replace('.', '_'))]\n",
    "        if len(annotations_voc_filename):\n",
    "            upload_project.upload(image_path, annotations_voc_dir+annotations_voc_filename[0], overwrite=True)\n",
    "\n",
    "print(\"Image upload complete.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
