{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7320a3-6bf3-493b-8cd5-20f2aeb9500a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# Ensure deterministic behavior (cannot control everything though)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12107aa5-3af9-4eb9-a7de-849d24c1c974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import cv2\n",
    "import numpy as np\n",
    "# import plotly.express as px\n",
    "from matplotlib.path import Path\n",
    "import math\n",
    "\n",
    "def merge_dicts(dict1, dict2): # used to merge train & valid sets into one\n",
    "    for key, value in dict2.items():\n",
    "        if isinstance(dict2[key], dict):\n",
    "            merge_dicts(dict1[key], value)\n",
    "        else:\n",
    "            dict1[key] = value\n",
    "    return dict1\n",
    "\n",
    "input_dir = '/workspace/raid/OM_DeepLearning/XAMI/dog-2/train/'\n",
    "json_file_path = '/workspace/raid/OM_DeepLearning/XAMI/dog-2/train/_annotations.coco.json'\n",
    "\n",
    "valid_path = '/workspace/raid/OM_DeepLearning/XAMI/dog-2/valid/'\n",
    "valid_json_path = '/workspace/raid/OM_DeepLearning/XAMI/dog-2/valid/_annotations.coco.json'\n",
    "\n",
    "test_path = '/workspace/raid/OM_DeepLearning/XAMI/dog-2/test/'\n",
    "test_json_path = '/workspace/raid/OM_DeepLearning/XAMI/dog-2/test/_annotations.coco.json'\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "with open(test_json_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "with open(valid_json_path, 'r') as f:\n",
    "    valid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f2f2a-6e7d-43d4-baed-91ddf15d565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02641f4-7411-40a6-9ff6-ffea079a00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = {} \n",
    "\n",
    "ground_truth_masks, bbox_coords, classes, class_categories = get_coords_and_masks_from_json(input_dir, training_data) # type: ignore\n",
    "\n",
    "# valid_gt_masks, valid_bboxes = get_coords_and_masks_from_json(input_dir, valid_data)\n",
    "# test_gt_masks, test_bboxes = get_coords_and_masks_from_json(input_dir, test_data)\n",
    "\n",
    "image_keys = []\n",
    "for key in ground_truth_masks.keys():\n",
    "    file_name_key = \"_\".join(key.split(\"_\")[:-1])\n",
    "    if file_name_key not in image_keys:\n",
    "        image_keys.append(file_name_key)\n",
    "        # print(file_name_key)\n",
    "\n",
    "image_paths = [input_dir+img_ for img_ in image_keys]\n",
    "image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a077ca-33b4-4b1d-921e-f908846f6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_masks(masks, ax, random_color=False):\n",
    "    for mask in masks:\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "            color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='orange', facecolor=(0,0,0,0), lw=2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc77ccf-4d16-4f72-924b-7e413261c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "def any_sam_model_predictor(any_sam_model, AMG, data_set_gt_masks, model_name,  IMAGE_PATH, use_negative=None, mask_on_negative=False, show_plot=False):\n",
    "    \n",
    "    image_name = IMAGE_PATH.split(\"/\")[-1]\n",
    "    predicted_masks = []\n",
    "    gt_image_masks = np.array([mask for key, mask in data_set_gt_masks.items() if key.startswith(image_name)])\n",
    " \n",
    "    with torch.no_grad():\n",
    "        image_bgr = cv2.imread(IMAGE_PATH)\n",
    "        annotated_image = None\n",
    "\n",
    "        # here also set the negative masked pixels to 0 after pre-processing\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        # image_rgb = 255 - image_rgb\n",
    "\n",
    "        negative_mask = np.where(image_rgb>0, True, False)\n",
    "        negative_mask = torch.from_numpy(negative_mask)  \n",
    "        negative_mask = negative_mask.permute(2, 0, 1)\n",
    "        negative_mask = resize(negative_mask, [1024, 1024], antialias=True) \n",
    "        negative_mask = negative_mask.unsqueeze(0)\n",
    "\n",
    "        if mask_on_negative:\n",
    "            mask_generator = AMG(any_sam_model, negative_mask=negative_mask)\n",
    "        else:\n",
    "            mask_generator = AMG(any_sam_model)\n",
    "            \n",
    "        sam_result = mask_generator.generate(image_rgb)\n",
    "        output_file = './plots/'+image_name+'_'+model_name+'.png'\n",
    "\n",
    "        if mask_on_negative:\n",
    "            sam_result = remove_masks(sam_result=sam_result, mask_on_negative=negative_mask.detach().cpu().numpy(), threshold=50)\n",
    "            output_file = './plots/'+image_name+'_'+model_name+'_segmented_removed_negative.png'\n",
    "\n",
    "        # !!! takes the predicted masks, and removes the ones that are covering more than 50% of the image\n",
    "        predicted_masks = np.array([out_pred['segmentation'] for out_pred in sam_result if np.sum(out_pred['segmentation']) <image_rgb.shape[0]**2/2]) \n",
    "        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "        if detections is not None and detections.mask is not None:\n",
    "            # !!! takes the detection masks and removes the ones that are covering more than 50% of the image\n",
    "            detections.mask = np.array([detmask for detmask in detections.mask if np.sum(detmask) <image_rgb.shape[0]**2/2])\n",
    "\n",
    "        annotated_image = mask_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "        print((annotated_image * (image_rgb>0).astype(int)).shape)\n",
    "        annotated_image = annotated_image * (image_rgb>0).astype(float)\n",
    "        if annotated_image.max() <= 1.0:\n",
    "            annotated_image *= 255\n",
    "        \n",
    "        # Now, convert the type to 'uint8' (unsigned 8-bit integer)\n",
    "        annotated_image = annotated_image.astype(np.uint8)\n",
    "        \n",
    "        # After the conversion, create the PIL image\n",
    "        image = Image.fromarray(annotated_image)\n",
    "        # image.save(output_file)\n",
    "\n",
    "        image_rgb = (image_rgb)*(image_rgb>0).astype(float)\n",
    "        if image_rgb.max() <= 1.0:\n",
    "            image_rgb *= 255\n",
    "        \n",
    "        # Now, convert the type to 'uint8' (unsigned 8-bit integer)\n",
    "        image_rgb = image_rgb.astype(np.uint8)\n",
    "        \n",
    "        # After the conversion, create the PIL image\n",
    "        # image_rgb = Image.fromarray(image_rgb)\n",
    "        # image.save(output_file)\n",
    "        \n",
    "        iou_assoc_loss = compute_loss(gt_image_masks, predicted_masks)\n",
    "        if show_plot:\n",
    "            sv.plot_images_grid(\n",
    "                images=[image_rgb, annotated_image],\n",
    "                grid_size=(1, 2),\n",
    "                titles=[f'source image\\n{image_name.split(\".\")[0]}', \\\n",
    "                        f'segmented image with {model_name}'])\n",
    "    return annotated_image, iou_assoc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcfd8a0-d129-47b4-b6bb-882504f2b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "\n",
    "def dice_loss_numpy(pred, target, area=None, smooth = 1): \n",
    "    pred_flat = pred.flatten()\n",
    "    target_flat = target.flatten()\n",
    "    \n",
    "    intersection = np.sum(pred_flat * target_flat)\n",
    "    union = np.sum(pred_flat) + np.sum(target_flat)\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss = 1 - dice\n",
    "    \n",
    "    return dice_loss\n",
    "        \n",
    "def compute_loss(gt_masks, pred_masks):\n",
    "    if gt_masks.size == 0:\n",
    "        return 0 \n",
    "    if pred_masks.size == 0:\n",
    "        return 1\n",
    "    losses = []\n",
    "    for gt_mask in gt_masks:\n",
    "        mask_losses = []\n",
    "        max_iou = 0.0\n",
    "        mask_loss = 0.5\n",
    "        for pred_mask in pred_masks:\n",
    "            iou = np.sum(pred_mask.flatten() * gt_mask.flatten())\n",
    "            if iou > max_iou:\n",
    "                mask_loss = dice_loss_numpy(pred_mask, gt_mask)\n",
    "        losses.append(mask_loss)\n",
    "    return np.mean(losses) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d5193-a96f-41bd-ac42-8c67ad280d0f",
   "metadata": {},
   "source": [
    "**orig SAM checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e51631-b118-41b7-9577-ea0bc889026d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d5ab9-98b6-4e42-9b8b-a5b63d153aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from segment_anything import sam_model_registry as orig_sam_model_registry, \\\n",
    "                            SamAutomaticMaskGenerator as orig_SamAutomaticMaskGenerator, \\\n",
    "                            SamPredictor as orig_SamPredictor\n",
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "origSAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = orig_sam_model_registry[\"vit_h\"](checkpoint=origSAM_CHECKPOINT_PATH).to(device=device)\n",
    "sam.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b20408-b72f-40d3-a009-fd1ca4e6ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = ['S0802200201_M.png', 'S0673730101_U.png', 'S0673350201_U.png', 'S0304201401_U.png', 'S0100240801_U.png', 'S0302884001_M.png']\n",
    "OM_dir = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/scaled_raw_512/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7adc1-e4a9-4d49-bf9f-118fd6fcf536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for immg in imgs:\n",
    "    annotated_image_orig_sam, annotated_image_orig_sam_loss = any_sam_model_predictor(sam, orig_SamAutomaticMaskGenerator, ground_truth_masks, \n",
    "                                                                                  'orig_SAM', OM_dir+immg, mask_on_negative=None, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea4ded-540d-409a-b0e7-f8f94ee21d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian_solved = '/workspace/raid/OM_DeepLearning/XAMI/gaussian_distrib/'\n",
    "for immg in imgs:\n",
    "    annotated_image_orig_sam, annotated_image_orig_sam_loss = any_sam_model_predictor(sam, orig_SamAutomaticMaskGenerator, ground_truth_masks, \n",
    "                                                                                  'orig_SAM', gaussian_solved+immg, mask_on_negative=None, show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7078f6-ec50-4519-990e-8972a3f1b051",
   "metadata": {},
   "source": [
    "**original MobileSAM checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ae39d-336f-4655-8301-d21a1717c8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/')\n",
    "import mobile_sam\n",
    "from mobile_sam import sam_model_registry as orig_mobile_sam_registry, \\\n",
    "SamAutomaticMaskGenerator as orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "SamPredictor as orig_mobile_SamPredictor\n",
    "\n",
    "orig_mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-master/weights/mobile_sam.pt\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model_orig = orig_mobile_sam_registry[\"vit_t\" ](checkpoint=orig_mobile_sam_checkpoint)\n",
    "mobile_sam_model_orig.to(device);\n",
    "mobile_sam_model_orig.eval();\n",
    "annotated_image_ft_mobile_sam, annotated_image_ft_mobile_sam_loss = any_sam_model_predictor(mobile_sam_model_orig, orig_mobile_SamAutomaticMaskGenerator, \\\n",
    "                                                       ground_truth_masks, 'ft_MobileSAM', image_paths[1], mask_on_negative=False, show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be174c82-47ac-4257-8d84-882d64318d54",
   "metadata": {},
   "source": [
    "**fine-tuned checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffc8e8-d40a-4be4-9e56-240fe26d009f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/raid/OM_DeepLearnin/XAMI/mobile_sam/')\n",
    "import ft_mobile_sam\n",
    "from ft_mobile_sam import sam_model_registry as ft_mobile_sam_registry, \\\n",
    "SamAutomaticMaskGenerator as ft_SamAutomaticMaskGenerator, \\\n",
    "SamPredictor as ft_SamPredictor\n",
    "\n",
    "ft_mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/XAMI/mobile_sam_model_checkpoint.pth\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "ft_mobile_sam_model = ft_mobile_sam_registry[\"vit_t\" ](checkpoint=ft_mobile_sam_checkpoint)\n",
    "ft_mobile_sam_model.to(device);\n",
    "ft_mobile_sam_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32b4bd-1115-465c-bd66-5ef7c617f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = ['raid/OM_DeepLearning/XAMI/dog-2/train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a7f30-db9a-481d-8dea-49136f52f99c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotated_image_ft_mobile_sam, annotated_image_ft_mobile_sam_loss = amg_predict(ft_mobile_sam_model, ft_SamAutomaticMaskGenerator, \\\n",
    "                                                       ground_truth_masks, 'MobileSAM', image_paths[1], mask_on_negative=False, show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b47ce1-46ac-4548-9d80-1c8a2ded8f42",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2b31a-da47-4e1e-b4ae-c9963d781635",
   "metadata": {},
   "source": [
    "**fine-tune MobileSAM AutoMaskGenerator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2a6c9-27ff-4071-9053-30542752c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_masks(sam_result, mask_on_negative, threshold, remove_big_masks=False, big_masks_threshold=None, img_shape=None):\n",
    "    '''\n",
    "    Given a segmentation result, this function removes the masks \n",
    "    if the intersection with the negative pixels gives a number is > than a threshold\n",
    "    '''\n",
    "    big_masks_threshold = img_shape[0]**2/5 if big_masks_threshold is None else big_masks_threshold\n",
    "    bad_indices = np.array([],  dtype=int) \n",
    "    print(sam_result[0]['segmentation'].shape, mask_on_negative.shape)\n",
    "    for segm_index in range(len(sam_result)):\n",
    "        count = np.sum((sam_result[segm_index]['segmentation'] == 1) & (mask_on_negative == 1))            \n",
    "        # remove masks on negative pixels given threshold\n",
    "        if count > threshold:\n",
    "            bad_indices = np.append(bad_indices, segm_index)\n",
    "        \n",
    "        # remove very big (>70) masks\n",
    "        if remove_big_masks and img_shape is not None and np.sum(sam_result[segm_index]['segmentation']) > big_masks_threshold:\n",
    "            print(f\"Removing mask {segm_index} with area {np.sum(sam_result[segm_index]['segmentation'])}\")\n",
    "            bad_indices = np.append(bad_indices, segm_index)   \n",
    "    sam_result = np.delete(sam_result, bad_indices)\n",
    "    return sam_result\n",
    "\n",
    "def amg_predict(any_sam_model, AMG, data_set_gt_masks, model_name,  IMAGE_PATH, use_negative=None, mask_on_negative=False, show_plot=False):\n",
    "    \n",
    "    image_name = IMAGE_PATH.split(\"/\")[-1]\n",
    "    predicted_masks = []\n",
    "    gt_image_masks = np.array([mask for key, mask in data_set_gt_masks.items() if key.startswith(image_name)])\n",
    " \n",
    "    # with torch.no_grad():\n",
    "    if True:\n",
    "        image_bgr = cv2.imread(IMAGE_PATH)\n",
    "        annotated_image = None\n",
    "\n",
    "        # here also set the negative masked pixels to 0 after pre-processing\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        negative_mask = np.where(image_rgb>0, True, False)\n",
    "        negative_mask = torch.from_numpy(negative_mask)  \n",
    "        negative_mask = negative_mask.permute(2, 0, 1)\n",
    "        negative_mask = resize(negative_mask, [1024, 1024], antialias=True) \n",
    "        negative_mask = negative_mask.unsqueeze(0)\n",
    "\n",
    "        if mask_on_negative:\n",
    "            mask_generator = AMG(any_sam_model, negative_mask=negative_mask)\n",
    "        else:\n",
    "            mask_generator = AMG(any_sam_model)\n",
    "            \n",
    "        sam_result = mask_generator.generate(image_rgb)\n",
    "        output_file = './plots/'+image_name+'_'+model_name+'.png'\n",
    "\n",
    "        if mask_on_negative:\n",
    "            print(image_rgb.shape)\n",
    "            img_negative_mask = np.where(image_rgb>0, 1, 0) \n",
    "            img_negative_mask = np.mean(img_negative_mask, axis=2) # to make it 2D\n",
    "            sam_result = remove_masks(sam_result=sam_result, mask_on_negative=img_negative_mask, threshold=50, img_shape=image_rgb.shape)\n",
    "            output_file = './plots/'+image_name+'_'+model_name+'_segmented_removed_negative.png'\n",
    "\n",
    "        # !!! takes the predicted masks, and removes the ones that are covering more than 50% of the image\n",
    "        predicted_masks = np.array([out_pred['segmentation'] for out_pred in sam_result if np.sum(out_pred['segmentation']) <image_rgb.shape[0]**2/2]) \n",
    "        mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "        detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "        if detections is not None and detections.mask is not None:\n",
    "            # !!! takes the detection masks and removes the ones that are covering more than 50% of the image (for detections)\n",
    "            detections.mask = np.array([detmask for detmask in detections.mask if np.sum(detmask) <image_rgb.shape[0]**2/2])\n",
    "\n",
    "        annotated_image = mask_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "        image = Image.fromarray(annotated_image)\n",
    "        # image.save(output_file)\n",
    "\n",
    "        iou_assoc_loss = compute_loss(gt_image_masks, predicted_masks)\n",
    "        \n",
    "        if show_plot:\n",
    "            sv.plot_images_grid(\n",
    "                images=[image_bgr, annotated_image],\n",
    "                grid_size=(1, 2),\n",
    "                titles=[f'source image\\n{image_name.split(\".\")[0]}', \\\n",
    "                        f'segmented image with {model_name}'])\n",
    "    return annotated_image, iou_assoc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663858d-2f4b-4ae2-b12e-026b6a6e0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_image_ft_mobile_sam, annotated_image_ft_mobile_sam_loss = any_sam_model_predictor(ft_mobile_sam_model, ft_SamAutomaticMaskGenerator, \\\n",
    "                                                       ground_truth_masks, 'MobileSAM', image_paths[0], mask_on_negative=True, show_plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
