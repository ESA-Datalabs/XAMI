{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08a5eb-cf5f-4b9c-bf97-94e34c5b061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "# Ensure deterministic behavior (cannot control everything though)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219539e-4dc4-4e78-8866-6a0c46f1ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = '/workspace/raid/OM_DeepLearning/MobileSAM-master/MobileSAMv2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6f101-eab2-4ffa-ae57-5b24d19ccfdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-master/MobileSAMv2/')\n",
    "\n",
    "from mobilesamv2.promt_mobilesamv2 import ObjectAwareModel\n",
    "from mobilesamv2 import sam_model_registry, SamPredictor\n",
    "from typing import Any, Dict, Generator,List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_model():\n",
    "    Prompt_guided_path=dir_+'PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt'\n",
    "    obj_model_path=dir_+'weight/ObjectAwareModel.pt'\n",
    "    ObjAwareModel = ObjectAwareModel(obj_model_path)\n",
    "    PromptGuidedDecoder=sam_model_registry['PromptGuidedDecoder'](Prompt_guided_path)\n",
    "    mobilesamv2 = sam_model_registry['vit_h']()\n",
    "    mobilesamv2.prompt_encoder=PromptGuidedDecoder['PromtEncoder']\n",
    "    mobilesamv2.mask_decoder=PromptGuidedDecoder['MaskDecoder']\n",
    "    return mobilesamv2,ObjAwareModel\n",
    "    \n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:\n",
    "    assert len(args) > 0 and all(\n",
    "        len(a) == len(args[0]) for a in args\n",
    "    ), \"Batched iteration must have inputs of all the same size.\"\n",
    "    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\n",
    "    for b in range(n_batches):\n",
    "        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]\n",
    "\n",
    "encoder_path={'efficientvit_l2':dir_+'weight/l2.pt',\n",
    "            'tiny_vit':dir_+'weight/mobile_sam.pt',\n",
    "            'sam_vit_h':dir_+'weight/sam_vit_h.pt',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ebf55-1212-4229-abbb-f0445f1030d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Load bboxes from source detection & deblending\n",
    "# import json\n",
    "# import time\n",
    "\n",
    "# # image_path = '/workspace/raid/OM_DeepLearning/XMM_OM_code/scaled_raw/clahe_S0720251301_L.png'\n",
    "\n",
    "# def remove_boxes_near_corners(boxes, img_width, img_height, distance):\n",
    "#     \"\"\"\n",
    "#     Remove bounding boxes that are within a certain distance from the corners of the image.\n",
    "\n",
    "#     Args:\n",
    "#         boxes (list): List of bounding boxes. Each box is a tuple (x1, y1, x2, y2).\n",
    "#         img_width (int): Width of the image.\n",
    "#         img_height (int): Height of the image.\n",
    "#         distance (int): The distance from the corners.\n",
    "\n",
    "#     Returns:\n",
    "#         list: The list of remaining bounding boxes.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     corners = [(0, 0), (img_width, 0), (0, img_height), (img_width, img_height)]\n",
    "#     new_boxes = []\n",
    "#     for box in boxes:\n",
    "#         x1, y1, x2, y2 = box\n",
    "#         box_center = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "#         if all(np.sqrt((cx - box_center[0]) ** 2 + (cy - box_center[1]) ** 2) > distance for cx, cy in corners):\n",
    "#             new_boxes.append(box)\n",
    "#     return new_boxes\n",
    "    \n",
    "# with open('extracted_sources_bboxes_points.json', 'r') as f:\n",
    "#     extracted_bboxes_points = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47dd481-4518-4e36-a941-52b3aefa7010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracted_bboxes_points['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a012b4-cc33-48e8-a4ea-55fe8e144cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# import random\n",
    "\n",
    "# def generate_random_data(batch_size, num_points, image_size=(256, 256), num_boxes=2):\n",
    "#     batched_input = []\n",
    "\n",
    "#     for _ in range(batch_size):\n",
    "#         image = torch.randn((3, image_size[0], image_size[1]))  # 3xHxW format\n",
    "#         original_size = (random.randint(100, 500), random.randint(100, 500))\n",
    "#         point_coords = torch.randn((batch_size, num_points, 2))  # BxNx2\n",
    "#         point_labels = torch.randint(0, 2, (batch_size, num_points))  # BxN\n",
    "#         boxes = torch.randn((num_boxes, 4))  # Bx4\n",
    "#         mask_inputs = torch.randn((1, 1, image_size[0], image_size[1]))  # Bx1xHxW\n",
    "\n",
    "#         input_dict = {\n",
    "#             'image': image,\n",
    "#             'original_size': original_size,\n",
    "#             'point_coords': point_coords,\n",
    "#             'point_labels': point_labels,\n",
    "#             'boxes': boxes,\n",
    "#             'mask_inputs': mask_inputs,\n",
    "#         }\n",
    "\n",
    "#         batched_input.append(input_dict)\n",
    "\n",
    "#     return batched_input\n",
    "\n",
    "# batch_size = 2\n",
    "# num_points = 5\n",
    "# random_data = generate_random_data(batch_size, num_points)\n",
    "\n",
    "# import numpy as np\n",
    "# batched_input = np.array([random_data, random_data])\n",
    "\n",
    "# from torchview import draw_graph\n",
    "\n",
    "# model_graph = draw_graph(mobilesamv2, input_data = random_data, depth= 3, \\\n",
    "#                          device = 'cuda:7', save_graph=True, filename='./view_sam', \\\n",
    "#                         hide_module_functions=False, hide_inner_tensors=True, roll=True)\n",
    "# # model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c1c1f-c33d-4773-ace9-716037c58885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    ObjectAwareModel_path = dir_+'PromptGuidedDecoder/ObjectAwareModel.pt'\n",
    "    Prompt_guided_Mask_Decoder_path = dir_+'PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt'\n",
    "    encoder_path = dir_\n",
    "    img_path = '/workspace/raid/OM_DeepLearning/XMM_OM_dataset/zscaled_512_rescaled_SAM_stats/'\n",
    "    imgsz = 512\n",
    "    iou = 0.9\n",
    "    conf = 0.4\n",
    "    retina = False\n",
    "    output_dir = '/workspace/raid/OM_DeepLearning/new_temp/'\n",
    "    encoder_type = 'tiny_vit'  # choose from ['tiny_vit','sam_vit_h','mobile_sam','efficientvit_l2','efficientvit_l1','efficientvit_l0']\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# import pdb;pdb.set_trace()\n",
    "output_dir=args.output_dir  \n",
    "mobilesamv2, ObjAwareModel=create_model()\n",
    "image_encoder=sam_model_registry[args.encoder_type](encoder_path[args.encoder_type])\n",
    "mobilesamv2.image_encoder=image_encoder\n",
    "device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobilesamv2.to(device=device)\n",
    "mobilesamv2.eval()\n",
    "predictor = SamPredictor(mobilesamv2)\n",
    "image_files= os.listdir(args.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90dc26-6081-45b9-80b6-83f964f87449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from mobilesamv2.promt_mobilesamv2 import ObjectAwareModel\n",
    "from mobilesamv2 import sam_model_registry, SamPredictor\n",
    "from typing import Any, Dict, Generator,List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ObjectAwareModel_path\", type=str, default='./PromptGuidedDecoder/ObjectAwareModel.pt', help=\"ObjectAwareModel path\")\n",
    "    parser.add_argument(\"--Prompt_guided_Mask_Decoder_path\", type=str, default='./PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt', help=\"Prompt_guided_Mask_Decoder path\")\n",
    "    parser.add_argument(\"--encoder_path\", type=str, default=\"./\", help=\"select your own path\")\n",
    "    parser.add_argument(\"--img_path\", type=str, default=\"./test_images/\", help=\"path to image file\")\n",
    "    parser.add_argument(\"--imgsz\", type=int, default=1024, help=\"image size\")\n",
    "    parser.add_argument(\"--iou\",type=float,default=0.9,help=\"yolo iou\")\n",
    "    parser.add_argument(\"--conf\", type=float, default=0.4, help=\"yolo object confidence threshold\")\n",
    "    parser.add_argument(\"--retina\",type=bool,default=True,help=\"draw segmentation masks\",)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./\", help=\"image save path\")\n",
    "    parser.add_argument(\"--encoder_type\", choices=['tiny_vit','sam_vit_h','mobile_sam','efficientvit_l2','efficientvit_l1','efficientvit_l0'], help=\"choose the model type\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_model():\n",
    "    Prompt_guided_path=dir_+'PromptGuidedDecoder/Prompt_guided_Mask_Decoder.pt'\n",
    "    obj_model_path=dir_+'weight/ObjectAwareModel.pt'\n",
    "    ObjAwareModel = ObjectAwareModel(obj_model_path)\n",
    "    PromptGuidedDecoder=sam_model_registry['PromptGuidedDecoder'](Prompt_guided_path)\n",
    "    mobilesamv2 = sam_model_registry['vit_h']()\n",
    "    mobilesamv2.prompt_encoder=PromptGuidedDecoder['PromtEncoder']\n",
    "    mobilesamv2.mask_decoder=PromptGuidedDecoder['MaskDecoder']\n",
    "    return mobilesamv2,ObjAwareModel\n",
    "    \n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    # ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:\n",
    "    assert len(args) > 0 and all(\n",
    "        len(a) == len(args[0]) for a in args\n",
    "    ), \"Batched iteration must have inputs of all the same size.\"\n",
    "    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\n",
    "    for b in range(n_batches):\n",
    "        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]\n",
    "\n",
    "\n",
    "encoder_path={'efficientvit_l2':dir_+'weight/l2.pt',\n",
    "            'tiny_vit':dir_+'weight/mobile_sam.pt',\n",
    "            'sam_vit_h':dir_+'weight/sam_vit_h.pt',}\n",
    "\n",
    "def main(args):\n",
    "    # import pdb;pdb.set_trace()\n",
    "    output_dir=args.output_dir  \n",
    "    mobilesamv2, ObjAwareModel=create_model()\n",
    "    image_encoder=sam_model_registry[args.encoder_type](encoder_path[args.encoder_type])\n",
    "    mobilesamv2.image_encoder=image_encoder\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mobilesamv2.to(device=device)\n",
    "    mobilesamv2.eval()\n",
    "    predictor = SamPredictor(mobilesamv2)\n",
    "    image_files= os.listdir(args.img_path)\n",
    "    for image_name in image_files[:10]:\n",
    "        print('*****', image_name)\n",
    "        image = cv2.imread(args.img_path + image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        obj_results = ObjAwareModel(image,device=device,retina_masks=args.retina,imgsz=args.imgsz,conf=args.conf,iou=args.iou)\n",
    "        predictor.set_image(image)\n",
    "        input_boxes1 = obj_results[0].boxes.xyxy\n",
    "        input_boxes = input_boxes1.cpu().numpy()\n",
    "        input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "        input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "        sam_mask=[]\n",
    "        image_embedding=predictor.features\n",
    "        image_embedding=torch.repeat_interleave(image_embedding, 720, dim=0)\n",
    "        prompt_embedding=mobilesamv2.prompt_encoder.get_dense_pe()\n",
    "        prompt_embedding=torch.repeat_interleave(prompt_embedding, 720, dim=0)\n",
    "        for (boxes,) in batch_iterator(320, input_boxes):\n",
    "            with torch.no_grad():\n",
    "                # Create a black image of size 1024x1024\n",
    "                image1 = np.zeros((1024, 1024, 3), dtype=np.uint8)\n",
    "                \n",
    "                for bbox in boxes:\n",
    "                    # Each bbox is a tuple or list in the format (x1, y1, x2, y2)\n",
    "                    x1, y1, x2, y2 = bbox.detach().cpu().numpy()\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                \n",
    "                    # Draw rectangle (bounding box) on the image\n",
    "                    cv2.rectangle(image1, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green rectangle with thickness 2\n",
    "                \n",
    "                # Convert the BGR image to RGB\n",
    "                image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Display the image\n",
    "                plt.imshow(image1)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "                image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "                prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "                sparse_embeddings, dense_embeddings = mobilesamv2.prompt_encoder(\n",
    "                    points=None,\n",
    "                    boxes=boxes,\n",
    "                    masks=None,)\n",
    "                low_res_masks, _ = mobilesamv2.mask_decoder(\n",
    "                    image_embeddings=image_embedding,\n",
    "                    image_pe=prompt_embedding,\n",
    "                    sparse_prompt_embeddings=sparse_embeddings,\n",
    "                    dense_prompt_embeddings=dense_embeddings,\n",
    "                    multimask_output=False,\n",
    "                    simple_type=True,\n",
    "                )\n",
    "                low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "                sam_mask_pre = (low_res_masks > mobilesamv2.mask_threshold)*1.0\n",
    "                sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "                # print(sam_mask[0].shape)\n",
    "        sam_mask=torch.cat(sam_mask)\n",
    "        # print(sam_mask.shape)\n",
    "        annotation = sam_mask\n",
    "        areas = torch.sum(annotation, dim=(1, 2))\n",
    "        sorted_indices = torch.argsort(areas, descending=True)\n",
    "        show_img = annotation[sorted_indices]\n",
    "        plt.figure(figsize=(5,5))\n",
    "        background=np.ones_like(image)*255\n",
    "        plt.imshow(background)\n",
    "        # print('torch', show_img.shape)\n",
    "        show_anns(show_img)\n",
    "        plt.axis('off')\n",
    "        plt.show() \n",
    "        plt.savefig(\"{}\".format(output_dir+image_name), bbox_inches='tight', pad_inches = 0.0) \n",
    "        plt.close()\n",
    "\n",
    "args = Args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04187c22-3666-487c-8195-be7c72fafdac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(obj_results[0].masks.data[0].permute(1,2,0).detach().cpu().numpy()*255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2a151-a1d2-484b-9477-a454ff474169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj_results[0].boxes.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949be5e-aa2b-4ba3-b096-527c1083d702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output = sam_model(sample_batched_input, multimask_output=True)\n",
    "\n",
    "# # Visualize the computation graph using torchview\n",
    "# draw_graph(sam_model, input_size=(1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518f0c9-fa16-48b2-9a4c-bd909b4cf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd MobileSAMv2\n",
    "bash ./experiments/mobilesamv2.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
