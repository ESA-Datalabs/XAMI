{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa5306-1bef-4759-b000-d8299f56e75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from inference.YoloSamPipeline import YoloSam\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511383d3-5f24-473b-ae9b-0d3e79766f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yolo_path = './yolov8-segm-0/yolov8n-seg/weights/best.pt'\n",
    "\n",
    "device_id = 3\n",
    "torch.cuda.set_device(device_id)\n",
    "\n",
    "# the checkpoint and model_type (vit_h, vit_t, etc.) must be compatible\n",
    "yolo_sam_pipeline = YoloSam(\n",
    "    device=f'cuda:{device_id}', \n",
    "    yolo_checkpoint=yolo_path, \n",
    "    sam_checkpoint=\"./output_sam/ft_mobile_sam_final_2024-05-05 09:57:26.423152.pth\", \n",
    "    # sam_checkpoint=\"./output_sam/ft_mobile_sam_final_2024-04-27 00:02:11.627528_last.pth\", \n",
    "    model_type='vit_t',\n",
    "    efficient_vit_enc=None,\n",
    "    yolo_conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97417c45-db3c-4538-9e9e-6c17cf67148a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "masks = yolo_sam_pipeline.run_predict('../XMM_OM_dataset/zscaled_512_stretched/S0782961401_L.png', show_masks=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ced51d-2c1c-4871-852c-55c189d699c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "dir_files = os.listdir('../XMM_OM_dataset/zscaled_512_stretched')\n",
    "inference_times = []\n",
    "num_empty_images = 0.0\n",
    "num_images = len(dir_files)\n",
    "for file_ in dir_files:\n",
    "    try:\n",
    "        _, _, inf_time, empty_image = yolo_sam_pipeline.run_predict('../XMM_OM_dataset/zscaled_512_stretched/'+file_) \n",
    "        if empty_image==0:\n",
    "            inference_times.append(inf_time)\n",
    "        num_empty_images += empty_image\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        num_images -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390a37a-67ae-4138-a464-c56e9106b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images-num_empty_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49eb45-e3c3-4b02-b017-2e569ea09f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_empty_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c81524-0300-4b14-99d4-d6362dbca4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(inference_times), np.std(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e39d0-a0f5-4eb8-b048-eaff226422c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(inference_times), np.min(inference_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f6dc6-b7cd-4a6c-a32f-8438af46f8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "masks = yolo_sam_pipeline.run_predict('..//XMM_OM_dataset/zscaled_512_stretched/S0504550601_U.png') \n",
    "# masks = yolo_sam_pipeline.run_predict('./Euclid-Grism-mode-caption1-768x768.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d3f46-bef9-46cd-a9d2-a80d7bb49e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "from dataset import dataset_utils, voc_annotate_and_Roboflow_export\n",
    "\n",
    "def export_image_det_to_Roboflow(input_dir, filename, masks, obj_results):\n",
    "    class_names = obj_results[0].names\n",
    "    class_labels = obj_results[0].boxes.data[:, -1].int().tolist()\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(masks)):\n",
    "        # masks[i]: [ 1, H, W]\n",
    "        mask_np = masks[i].detach().cpu().numpy()\n",
    "        polygon = voc_annotate_and_Roboflow_export.binary_image_to_polygon(mask_np[0])\n",
    "        bbox = dataset_utils.mask_to_bbox(mask_np)\n",
    "        if class_names[class_labels[i]] != 'read-out-streak':\n",
    "            objects.append({\n",
    "                'name': class_names[class_labels[i]],\n",
    "                'bbox': bbox,\n",
    "                'segmentations': polygon[0]\n",
    "            })\n",
    "    if len(objects)>0:\n",
    "        voc_annotate_and_Roboflow_export.create_annotation_SAM(\n",
    "            filename=filename, \n",
    "            width=512, \n",
    "            height=512, \n",
    "            depth=3, \n",
    "            objects=objects, \n",
    "            offset=1.2) # generating xml file for VOC format\n",
    "        image_path = input_dir+filename\n",
    "        annotation_filename = filename.replace(\".png\", \".xml\")\n",
    "        upload_project.upload(image_path, annotation_filename, overwrite=True)\n",
    "        os.remove(annotation_filename)\n",
    "    else:\n",
    "        print(\"No objects after label filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33784695-7c40-4b44-98c1-54e7bc3354a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2 \n",
    "from dataset import dataset_utils, voc_annotate_and_Roboflow_export\n",
    "\n",
    "# Optional Roboflow export in VOC format given filenames\n",
    "export_to_Roboflow = False\n",
    "import time\n",
    "\n",
    "# best_model = mobile_sam_model.cpu()\n",
    "if export_to_Roboflow:\n",
    "    # Initialize Roboflow client\n",
    "    rf = Roboflow(api_key=\"api_key\")\n",
    "    upload_project = rf.workspace(\"username\").project(\"project\") # error if the project doesn't exist\n",
    "\n",
    "new_images_dir = '../XMM_OM_dataset/zscaled_512_stretched/'\n",
    "new_image_files =  os.listdir(new_images_dir)\n",
    "# best_model.eval()\n",
    "    \n",
    "with torch.no_grad(): \n",
    "    # eg_img = 'S0018141301_M.png'\n",
    "    for image_name in new_image_files[200:1000]: # [3695:3730]\n",
    "        print('Image', new_images_dir+image_name)\n",
    "        image = cv2.imread(new_images_dir + image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            sam_mask_pre, obj_results, _ = yolo_sam_pipeline.run_predict(new_images_dir + image_name) \n",
    "            # if export_to_Roboflow:\n",
    "                # export_image_det_to_Roboflow(new_images_dir, image_name, sam_mask_pre, obj_results)\n",
    "                \n",
    "        except Exception as e: # most likely the image had no annotations\n",
    "            print(e)\n",
    "            if export_to_Roboflow:\n",
    "                upload_project.upload(new_images_dir+image_name)\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py311",
   "language": "python",
   "name": "env_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
