{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the YOLOv8 pretrained model\n",
    "\n",
    "- The model is pretrained (in another notebook)  using a Roboflow dataset version on OM images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "eotMLol5O5G0",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from typing import Any, Dict, Generator,List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "import dataset_utils, predictor_utils, loss#, data_mounting\n",
    "reload(dataset_utils), reload(predictor_utils), reload(loss)#, reload(data_mounting)\n",
    "from dataset_utils import *\n",
    "from predictor_utils import *\n",
    "from loss import *\n",
    "# from data_mounting import *\n",
    "\n",
    "import torch.autograd.profiler as profiler\n",
    "device_id = 3\n",
    "torch.cuda.set_device(device_id) # ‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = True\n",
    "\n",
    "if use_wandb:\n",
    "    from datetime import datetime\n",
    "    # !pip install wandb\n",
    "    # !wandb login --relogin\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"yolov8-segm-ft_no_stars-n\", name=f\"200_epochs-no_stars-n-SAM-fewer {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dataset (YOLOv8 format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yolo_dataset_path = './xmm_om_images_512_SG_SR_CR_only-even_fewer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(yolo_dataset_path+\"data.yaml\", 'r') as stream:\n",
    "    yam_data = yaml.safe_load(stream) # dict with keys 'names', 'nc', 'roboflow', 'test', 'train', 'val'\n",
    "yam_data['names']\n",
    "\n",
    "classes = {i:name for i, name in enumerate(yam_data['names'])}\n",
    "train_path = yam_data['train']\n",
    "val_path = yam_data['val']\n",
    "test_path = yam_data['test']\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get masks from dataset (in YOLOv8 format) given an image file\n",
    "\n",
    "def get_label_file_path(dataset_path, image_location):\n",
    "    dataset_path = '/'.join(dataset_path.split('/')[:-2])+'/'+'labels'+'/'\n",
    "    label_file_path = os.path.join(dataset_path, image_location)\n",
    "    label_loc = '.'.join(image_location.split('.')[:-1]) + '.txt'\n",
    "    label_file_path = dataset_path+label_loc\n",
    "    return label_file_path\n",
    "\n",
    "def read_annotations(label_file_path):\n",
    "    annotations = []\n",
    "    with open(label_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            segmentation_points = [float(p) for p in parts[1:]]\n",
    "            annotations.append({\n",
    "                'class_id': class_id,\n",
    "                'segmentation_points': segmentation_points\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "def get_masks_from_image(yolo_dataset_path, image_location):\n",
    "    label_file_path = get_label_file_path(yolo_dataset_path, image_location)\n",
    "    annotations = read_annotations(label_file_path)\n",
    "    masks = [create_mask_0_1(annot['segmentation_points'], (512, 512)) for annot in annotations]\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters docs: https://docs.ultralytics.com/usage/cfg/#train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmQtdzgx2Noc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_masks(masks, ax, random_color=False):\n",
    "    for mask in masks:\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "                color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couple YOLO bboxes with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:\n",
    "    assert len(args) > 0 and all(\n",
    "        len(a) == len(args[0]) for a in args\n",
    "    ), \"Batched iteration must have inputs of all the same size.\"\n",
    "    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\n",
    "    for b in range(n_batches):\n",
    "        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**load SAM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/')\n",
    "from ft_mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/weights/sam_vit_l_0b3195.pth\"\n",
    "\n",
    "yolov8_pretrained_model = YOLO('./yolov8-segm-ft_no_stars-n/200_epochs-no_stars-n-even_fewer_obj/weights/best.pt');\n",
    "yolov8_pretrained_model.to(f'cuda:{device_id}');\n",
    "\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_l\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device)\n",
    "predictor = SamPredictor(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = yolo_dataset_path+'train/images/'\n",
    "valid_dir = yolo_dataset_path+'valid/images/'\n",
    "train_image_files = os.listdir(train_dir)\n",
    "valid_image_files = os.listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    params_to_train = ['mask_tokens', 'output_upscaling', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
    "    # if 'mask_decoder' in name: # and any(s in name for s in params_to_train):\n",
    "    if True:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_requires_grad(model, show=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and show:\n",
    "            print(\"‚úÖ Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"‚ùå Param\", name, \" doesn't require grad.\")\n",
    "\n",
    "print(f\"üöÄ The model has {sum(p.numel() for p in mobile_sam_model.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "check_requires_grad(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import loss\n",
    "reload(loss)\n",
    "from loss import *\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils, voc_annotate_and_Roboflow_export\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "reload(voc_annotate_and_Roboflow_export)\n",
    "\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "from voc_annotate_and_Roboflow_export import * \n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 12\n",
    "train_num_batches = len(train_image_files) // batch_size\n",
    "valid_num_batches = len(valid_image_files) // batch_size\n",
    "\n",
    "lr=3e-4\n",
    "wd=0.00005\n",
    "parameters_to_optimize = [param for param in mobile_sam_model.mask_decoder.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(parameters_to_optimize, lr=lr, weight_decay=wd) #betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "def run_epoch(phase, image_files, images_dir, num_batches, model_, optimizer=None, train_encoders=False):\n",
    "    assert phase in ['train', 'val'], \"Phase must be 'train' or 'val'\"\n",
    "    \n",
    "    if phase == 'train':\n",
    "        model_.train()  \n",
    "    else:\n",
    "        model_.eval() \n",
    "\n",
    "    epoch_sam_loss = []\n",
    "    epoch_yolo_loss = []\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=f\"{phase.capitalize()} Batch\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_files = image_files[start_idx:end_idx]\n",
    "\n",
    "        batch_losses_sam = []\n",
    "        batch_losses_yolo = []\n",
    "\n",
    "        for image_name in batch_files:\n",
    "            image_path = images_dir + image_name\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            obj_results = yolov8_pretrained_model.predict(image_path, verbose=False, conf=0.2) \n",
    "            if train_encoders:\n",
    "                predictor.set_image(image)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    obj_results = yolov8_pretrained_model.predict(image_path, verbose=False, conf=0.2) \n",
    "                    predictor.set_image(image)\n",
    "            # sets a specific mean for each image\n",
    "            image_T = np.transpose(image, (2, 1, 0))\n",
    "            mean_ = np.mean(image_T[image_T>0])\n",
    "            std_ = np.std(image_T[image_T>0]) \n",
    "            pixel_mean = torch.as_tensor([mean_, mean_, mean_], dtype=torch.float, device=device)\n",
    "            pixel_std = torch.as_tensor([std_, std_, std_], dtype=torch.float, device=device)\n",
    "    \n",
    "            mobile_sam_model.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "            mobile_sam_model.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).unsqueeze(-1).unsqueeze(-1), False) # not in SAM\n",
    "                \n",
    "            gt_masks = get_masks_from_image(images_dir, image_name)  \n",
    "            if len(obj_results[0]) == 0 or len(gt_masks) == 0:\n",
    "                continue\n",
    "\n",
    "            input_boxes1 = obj_results[0].boxes.xyxy\n",
    "            expand_by = 2.5\n",
    "            enlarged_bbox = input_boxes1.clone() \n",
    "            enlarged_bbox[:, :2] -= expand_by  \n",
    "            enlarged_bbox[:, 2:] += expand_by  \n",
    "            input_boxes1 = enlarged_bbox\n",
    "            input_boxes = input_boxes1.cpu().numpy()\n",
    "            input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "            input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "            sam_mask, yolo_masks = [], []\n",
    "            \n",
    "            if train_encoders:\n",
    "                image_embedding=predictor.features\n",
    "                prompt_embedding=model_.prompt_encoder.get_dense_pe()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    image_embedding=predictor.features\n",
    "                    prompt_embedding=model_.prompt_encoder.get_dense_pe()\n",
    "                \n",
    "            non_resized_masks = obj_results[0].masks.data.cpu().numpy()\n",
    "            \n",
    "            for i in range(len(non_resized_masks)):\n",
    "                    yolo_masks.append(cv2.resize(non_resized_masks[i], image.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)) \n",
    "\n",
    "            for (boxes,) in batch_iterator(320, input_boxes): \n",
    "                if train_encoders:\n",
    "                    image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "                    prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "                    sparse_embeddings, dense_embeddings = mobile_sam_model.prompt_encoder(\n",
    "                        points=None,\n",
    "                        boxes=boxes,\n",
    "                        masks=None,)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "                        prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "                        sparse_embeddings, dense_embeddings = mobile_sam_model.prompt_encoder(\n",
    "                            points=None,\n",
    "                            boxes=boxes,\n",
    "                            masks=None,)\n",
    "\n",
    "                if phase == 'val':\n",
    "                    with torch.no_grad():\n",
    "                        low_res_masks, _ = model_.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                else:\n",
    "                    low_res_masks, _ = model_.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "                threshold_masks = torch.sigmoid(low_res_masks - model_.mask_threshold) \n",
    "                # threshold_masks = normalize(threshold(low_res_masks, 0.0, 0)).to(device)\n",
    "                sam_mask_pre = (threshold_masks > 0.5)*1.0\n",
    "                sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "\n",
    "                # reshape gt_masks to same shape as predicted masks\n",
    "                gt_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in gt_masks], dim=0).to(device)\n",
    "                yolo_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in yolo_masks], dim=0).to(device)\n",
    "                segm_loss_sam = segm_loss_match(threshold_masks, low_res_masks, gt_masks_tensor)\n",
    "                segm_loss_yolo = segm_loss_match(yolo_masks_tensor, low_res_masks, gt_masks_tensor)\n",
    "                batch_losses_sam.append(segm_loss_sam)\n",
    "                batch_losses_yolo.append(segm_loss_yolo)\n",
    "                del sparse_embeddings, dense_embeddings, low_res_masks, gt_masks, \n",
    "                del yolo_masks_tensor, segm_loss_sam, segm_loss_yolo\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if phase == 'val':\n",
    "                    fig, axes = plt.subplots(1, 4, figsize=(18, 6)) \n",
    "                    \n",
    "                    # Plot 1: GT Masks\n",
    "                    axes[0].imshow(image)\n",
    "                    axes[0].set_title('GT Masks')\n",
    "                    show_masks(gt_masks_tensor.squeeze(1).squeeze(1).detach().cpu().numpy(), axes[0], random_color=True)\n",
    "                    \n",
    "                    # Plot 2: YOLO Masks\n",
    "                    axes[1].imshow(image)\n",
    "                    axes[1].set_title('YOLOv8n predicted Masks')\n",
    "                    show_masks(yolo_masks, axes[1], random_color=True)\n",
    "                    \n",
    "                    # Plot 3: Bounding Boxes\n",
    "                    image1 = cv2.resize(image, (1024, 1024))\n",
    "                    for bbox in boxes:\n",
    "                        x1, y1, x2, y2 = bbox.detach().cpu().numpy()\n",
    "                        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                        cv2.rectangle(image1, (x1, y1), (x2, y2), (0, 255, 0), 2) \n",
    "                    image1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "                    axes[2].imshow(image1_rgb)\n",
    "                    axes[2].set_title('YOLOv8n predicted Bboxes')\n",
    "                    \n",
    "                    # Plot 4: SAM Masks\n",
    "                    sam_masks_numpy = sam_mask[0].detach().cpu().numpy()\n",
    "                    axes[3].imshow(image)\n",
    "                    show_masks(sam_masks_numpy, axes[3], random_color=True)\n",
    "                    axes[3].set_title('MobileSAM predicted masks')\n",
    "                    plt.tight_layout() \n",
    "                    # plt.savefig(f'./plots/combined_plots.png')\n",
    "                    plt.show()\n",
    "\n",
    "        mean_loss_sam = torch.mean(torch.stack(batch_losses_sam))\n",
    "        mean_loss_yolo = torch.mean(torch.stack(batch_losses_yolo))\n",
    "        epoch_sam_loss.append(mean_loss_sam.item())\n",
    "        epoch_yolo_loss.append(mean_loss_yolo.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            mean_loss_sam.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch}, {phase.capitalize()} Segmentation loss SAM: {np.mean(epoch_sam_loss)}. YOLO: {np.mean(epoch_yolo_loss)}')\n",
    "    return np.mean(epoch_sam_loss), np.mean(epoch_yolo_loss), model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 20\n",
    "n_epochs_stop = 5 #+ num_epochs/10\n",
    "epoch_sam_loss_train_list, epoch_sam_loss_val_list, epoch_yolo_loss_train_list, epoch_yolo_loss_val_list = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_sam_loss_train, epoch_yolo_loss_train, model = run_epoch('train', train_image_files, train_dir, train_num_batches, mobile_sam_model, optimizer, train_encoders=True)\n",
    "    epoch_sam_loss_val, epoch_yolo_loss_val, model = run_epoch('val', valid_image_files, valid_dir, valid_num_batches, mobile_sam_model)\n",
    "\n",
    "    epoch_sam_loss_train_list.append(epoch_sam_loss_train)\n",
    "    epoch_sam_loss_val_list.append(epoch_sam_loss_val)\n",
    "    epoch_yolo_loss_train_list.append(epoch_yolo_loss_train)\n",
    "    epoch_yolo_loss_val_list.append(epoch_yolo_loss_val)\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({'epoch train SAM loss': epoch_sam_loss_train, 'epoch valid SAM loss': epoch_sam_loss_val})\n",
    "        wandb.log({'epoch train YOLO loss': epoch_yolo_loss_train, 'epoch valid YOLO loss': epoch_yolo_loss_val})\n",
    "\n",
    "    if epoch_sam_loss_val < best_valid_loss:\n",
    "        best_valid_loss = epoch_sam_loss_val\n",
    "        best_model = model\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print(\"Early stopping initiated.\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "torch.save(best_model.state_dict(), f'ft_mobile_sam_final.pth')\n",
    "if use_wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(epoch_sam_loss_train_list))), epoch_sam_loss_train_list)\n",
    "plt.plot(list(range(len(epoch_sam_loss_val_list))), epoch_sam_loss_val_list)\n",
    "plt.plot(list(range(len(epoch_yolo_loss_train_list))), epoch_yolo_loss_train_list)\n",
    "plt.plot(list(range(len(epoch_yolo_loss_val_list))), epoch_yolo_loss_val_list)\n",
    "\n",
    "plt.title('Mean epoch loss \\n YOLO-SAM')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('loss_yolo_SAM.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow client\n",
    "rf = Roboflow(api_key=\"my_api_key\")\n",
    "upload_project = rf.workspace(\"orij\").project(\"xmm_om_images_512_sg_sr_cr_only\") # error if the project doesn't exist\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "def export_image_det_to_Roboflow(input_dir, filename, masks, obj_results):\n",
    "    class_names = obj_results[0].names\n",
    "    class_labels = obj_results[0].boxes.data[:, -1].int().tolist()\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(masks)):\n",
    "        # masks[i]: [ 1, H, W]\n",
    "        mask_np = masks[i].detach().cpu().numpy()\n",
    "        polygon = binary_image_to_polygon(mask_np[0])\n",
    "        bbox = mask_to_bbox(mask_np)\n",
    "        if class_names[class_labels[i]] != 'star' and class_names[class_labels[i]] != 'other': # ignore stars and 'other' label\n",
    "            objects.append({\n",
    "                'name': class_names[class_labels[i]],\n",
    "                'bbox': bbox,\n",
    "                'segmentations': polygon[0]\n",
    "            })\n",
    "    if len(objects)>0:\n",
    "        create_annotation_SAM(filename=filename, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "        image_path = input_dir+filename\n",
    "        annotation_filename = filename.replace(\".png\", \".xml\")\n",
    "        upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "        os.remove(annotation_filename)\n",
    "    else:\n",
    "        print(\"No objects after label filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional Roboflow export in VOC format given filenames\n",
    "export_to_Roboflow = True\n",
    "\n",
    "if export_to_Roboflow:\n",
    "    new_images_dir = '../XMM_OM_dataset/zscaled_512_stretched/'\n",
    "    new_image_files =  os.listdir(new_images_dir)\n",
    "    best_model.eval()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for image_name in new_image_files[600:620]:\n",
    "                print('Image', new_images_dir+image_name)\n",
    "                image = cv2.imread(new_images_dir + image_name)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                obj_results = yolov8_pretrained_model.predict(new_images_dir + image_name, conf=0.2)  \n",
    "                predictor.set_image(image)\n",
    "            \n",
    "                if len(obj_results[0]) == 0:\n",
    "                    print(f\"No masks for {image_name}.\")\n",
    "                    plt.imshow(image)\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "                    continue\n",
    "        \n",
    "                input_boxes1 = obj_results[0].boxes.xyxy\n",
    "                expand_by = 2.5\n",
    "                enlarged_bbox = input_boxes1.clone() \n",
    "                enlarged_bbox[:, :2] -= expand_by  \n",
    "                enlarged_bbox[:, 2:] += expand_by  \n",
    "                input_boxes1 = enlarged_bbox\n",
    "        \n",
    "                input_boxes = input_boxes1.cpu().numpy()\n",
    "                input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "                input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "                sam_mask, yolo_masks = [], []\n",
    "                image_embedding=predictor.features\n",
    "                prompt_embedding=best_model.prompt_encoder.get_dense_pe()\n",
    "                non_resized_masks = obj_results[0].masks.data.cpu().numpy()\n",
    "                for i in range(len(non_resized_masks)):\n",
    "                        yolo_masks.append(cv2.resize(non_resized_masks[i], image.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)) \n",
    "            \n",
    "                for (boxes,) in batch_iterator(320, input_boxes): \n",
    "                    with torch.no_grad():\n",
    "                        image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "                        prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "                        sparse_embeddings, dense_embeddings = best_model.prompt_encoder(\n",
    "                            points=None,\n",
    "                            boxes=boxes,\n",
    "                            masks=None,)\n",
    "                        low_res_masks, _ = best_model.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                        low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "                        threshold_masks = torch.sigmoid(low_res_masks - best_model.mask_threshold) \n",
    "                        sam_mask_pre = (threshold_masks > 0.5)*1.0\n",
    "                        sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "        \n",
    "                        yolo_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in yolo_masks], dim=0)\n",
    "                        export_image_det_to_Roboflow(new_images_dir, image_name, sam_mask_pre, obj_results)\n",
    "\n",
    "                        fig, axes = plt.subplots(1, 3, figsize=(18, 6)) \n",
    "                        \n",
    "                        # Plot 1: YOLO Masks\n",
    "                        axes[0].imshow(image)\n",
    "                        axes[0].set_title('YOLOv8n predicted Masks')\n",
    "                        show_masks(yolo_masks, axes[0], random_color=True)\n",
    "                        \n",
    "                        # Plot 2: Bounding Boxes\n",
    "                        image1 = cv2.resize(image, (1024, 1024))\n",
    "                        for bbox in boxes:\n",
    "                            x1, y1, x2, y2 = bbox.detach().cpu().numpy()\n",
    "                            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                            cv2.rectangle(image1, (x1, y1), (x2, y2), (0, 255, 0), 2) \n",
    "                        image1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "                        axes[1].imshow(image1_rgb)\n",
    "                        axes[1].set_title('YOLOv8n predicted Bboxes')\n",
    "                        \n",
    "                        # Plot 3: SAM Masks\n",
    "                        sam_masks_numpy = sam_mask[0].detach().cpu().numpy()\n",
    "                        axes[2].imshow(image)\n",
    "                        show_masks(sam_masks_numpy, axes[2], random_color=True)\n",
    "                        axes[2].set_title('MobileSAM predicted masks')\n",
    "                        plt.tight_layout() \n",
    "                        # plt.savefig(f'./plots/combined_plots.png')\n",
    "                        plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_py311_cloned_dsa",
   "language": "python",
   "name": "env_py311_cloned"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "218c61d150744eeca7f78d41f77a8a10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "2c0c7724c463489c93f58b229a8c2842": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41e1477131044f09b6c212658406bc21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5088d43921aa4379b718092288a895b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_91f18cd4b3ca46a39b5d968e8d1c1919",
       "max": 1,
       "style": "IPY_MODEL_ab5af91b9d4d4914ad58b7953fad3100"
      }
     },
     "52a8a6fab07d4523b088f4d6d1e9ff48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ca80e26fbca43ec9df3e0590bfa63ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "72a0c6b32bcb4576a9fb285b8c7ed86f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "91f18cd4b3ca46a39b5d968e8d1c1919": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ab5af91b9d4d4914ad58b7953fad3100": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b40c5382e77d425ba892833662beb6fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_2c0c7724c463489c93f58b229a8c2842",
       "style": "IPY_MODEL_218c61d150744eeca7f78d41f77a8a10"
      }
     },
     "cabfd35c1bdc4614acea0682a077c6fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_fc869d37c5214075afef71dc8e8083f3",
       "max": 1,
       "style": "IPY_MODEL_e92c7e8dee114184a4bd6e4935612355",
       "value": 1
      }
     },
     "ddfacf8bf69b4ee3854f2a043f4c739c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b40c5382e77d425ba892833662beb6fb",
        "IPY_MODEL_5088d43921aa4379b718092288a895b1"
       ],
       "layout": "IPY_MODEL_41e1477131044f09b6c212658406bc21"
      }
     },
     "e92c7e8dee114184a4bd6e4935612355": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "eaf1ff0dca8443b99d89c9991a128452": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_6ca80e26fbca43ec9df3e0590bfa63ff",
       "style": "IPY_MODEL_72a0c6b32bcb4576a9fb285b8c7ed86f",
       "value": "0.100 MB of 0.100 MB uploaded\r"
      }
     },
     "fc869d37c5214075afef71dc8e8083f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
