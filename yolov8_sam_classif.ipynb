{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the YOLOv8 pretrained model\n",
    "\n",
    "- The model is pretrained (in another notebook)  using a Roboflow dataset version on OM images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "eotMLol5O5G0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch import cuda\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy.ma as ma\n",
    "import json\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from typing import Any, Dict, Generator,List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *\n",
    "\n",
    "import predictor_utils\n",
    "reload(predictor_utils)\n",
    "from predictor_utils import *\n",
    "\n",
    "import loss\n",
    "reload(loss)\n",
    "from loss import *\n",
    "\n",
    "import torch.autograd.profiler as profiler\n",
    "device_id = 3\n",
    "torch.cuda.set_device(device_id) # ❗️❗️❗️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dataset (YOLOv8 format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yolo_dataset_path = './xmm_om_images_512_no_stars-4-YOLO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(yolo_dataset_path+\"data.yaml\", 'r') as stream:\n",
    "    yam_data = yaml.safe_load(stream) # dict with keys 'names', 'nc', 'roboflow', 'test', 'train', 'val'\n",
    "yam_data['names']\n",
    "\n",
    "classes = {i:name for i, name in enumerate(yam_data['names'])}\n",
    "train_path = yam_data['train']\n",
    "val_path = yam_data['val']\n",
    "test_path = yam_data['test']\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get masks from dataset (in YOLOv8 format) given an image file\n",
    "\n",
    "def get_label_file_path(dataset_path, image_location):\n",
    "    dataset_path = '/'.join(dataset_path.split('/')[:-2])+'/'+'labels'+'/'\n",
    "    label_file_path = os.path.join(dataset_path, image_location)\n",
    "    label_loc = '.'.join(image_location.split('.')[:-1]) + '.txt'\n",
    "    label_file_path = dataset_path+label_loc\n",
    "    return label_file_path\n",
    "\n",
    "def read_annotations(label_file_path):\n",
    "    annotations = []\n",
    "    with open(label_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            segmentation_points = [float(p) for p in parts[1:]]\n",
    "            annotations.append({\n",
    "                'class_id': class_id,\n",
    "                'segmentation_points': segmentation_points\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "def get_masks_from_image(yolo_dataset_path, image_location):\n",
    "    label_file_path = get_label_file_path(yolo_dataset_path, image_location)\n",
    "    annotations = read_annotations(label_file_path)\n",
    "    masks = [create_mask_0_1(annot['segmentation_points'], (512, 512)) for annot in annotations]\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters docs: https://docs.ultralytics.com/usage/cfg/#train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmQtdzgx2Noc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_masks(masks, ax, random_color=False):\n",
    "    for mask in masks:\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "                color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)\n",
    "\n",
    "yolov8_pretrained_model = YOLO('./yolov8-segm-fine-tuning/200_epochs-2/weights/best.pt');\n",
    "yolov8_pretrained_model.to(f'cuda:{device_id}');\n",
    "# yolov8_pretrained_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couple YOLO bboxes with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:\n",
    "    assert len(args) > 0 and all(\n",
    "        len(a) == len(args[0]) for a in args\n",
    "    ), \"Batched iteration must have inputs of all the same size.\"\n",
    "    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\n",
    "    for b in range(n_batches):\n",
    "        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**load SAM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append('/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/')\n",
    "from ft_mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "mobile_sam_checkpoint = \"/workspace/raid/OM_DeepLearning/MobileSAM-fine-tuning/weights/mobile_sam.pt\"\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "mobile_sam_model = sam_model_registry[\"vit_t\"](checkpoint=mobile_sam_checkpoint)\n",
    "mobile_sam_model.to(device)\n",
    "predictor = SamPredictor(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = yolo_dataset_path+'train/images/'\n",
    "valid_dir = yolo_dataset_path+'valid/images/'\n",
    "train_image_files = os.listdir(train_dir)\n",
    "valid_image_files = os.listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "\n",
    "def export_image_det_to_Roboflow(input_dir, filename, masks, obj_results):\n",
    "    class_names = obj_results[0].names\n",
    "    class_labels = obj_results[0].boxes.data[:, -1].int().tolist()\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(masks)):\n",
    "        # masks[i]: [ 1, H, W]\n",
    "        mask_np = masks[i].detach().cpu().numpy()\n",
    "        polygon = binary_image_to_polygon(mask_np[0])\n",
    "        print(polygon[0].shape)\n",
    "        bbox = mask_to_bbox(mask_np)\n",
    "        if class_names[class_labels[i]] != 'star' and class_names[class_labels[i]] != 'other': # ignore stars and 'other' label\n",
    "            objects.append({\n",
    "                'name': class_names[class_labels[i]],\n",
    "                'bbox': bbox,\n",
    "                'segmentations': polygon[0]\n",
    "            })\n",
    "    if len(objects)>0:\n",
    "        create_annotation_SAM(filename=filename, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "        image_path = input_dir+filename\n",
    "        annotation_filename = filename.replace(\".png\", \".xml\")\n",
    "        upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "        os.remove(annotation_filename)\n",
    "    else:\n",
    "        print(\"No objects after label filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in mobile_sam_model.named_parameters():\n",
    "    params_to_train = ['mask_tokens', 'output_upscaling', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
    "    if 'mask_decoder' in name and any(s in name for s in params_to_train):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_requires_grad(model, show=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and show:\n",
    "            print(\"✅ Param\", name, \" requires grad.\")\n",
    "        elif param.requires_grad == False:\n",
    "            print(\"❌ Param\", name, \" doesn't require grad.\")\n",
    "\n",
    "print(f\"🚀 The model has {sum(p.numel() for p in mobile_sam_model.parameters() if p.requires_grad)} trainable parameters.\\n\")\n",
    "check_requires_grad(mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import loss\n",
    "reload(loss)\n",
    "from loss import *\n",
    "from importlib import reload\n",
    "import astronomy_utils, predictor_utils, voc_annotate_and_Roboflow_export\n",
    "reload(astronomy_utils)\n",
    "reload(predictor_utils)\n",
    "reload(voc_annotate_and_Roboflow_export)\n",
    "\n",
    "from predictor_utils import *\n",
    "from astronomy_utils import *\n",
    "from voc_annotate_and_Roboflow_export import * \n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 8\n",
    "train_num_batches = len(train_image_files) // batch_size\n",
    "valid_num_batches = len(valid_image_files) // batch_size\n",
    "\n",
    "lr=3e-4\n",
    "wd=0.0\n",
    "parameters_to_optimize = [param for param in mobile_sam_model.mask_decoder.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(parameters_to_optimize, lr=lr, weight_decay=wd) #betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "def run_epoch(phase, image_files, images_dir, num_batches, model_, optimizer=None):\n",
    "    assert phase in ['train', 'val'], \"Phase must be 'train' or 'val'\"\n",
    "    \n",
    "    if phase == 'train':\n",
    "        model_.train()  \n",
    "    else:\n",
    "        model_.eval() \n",
    "\n",
    "    epoch_sam_loss = []\n",
    "    epoch_yolo_loss = []\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=f\"{phase.capitalize()} Batch\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_files = image_files[start_idx:end_idx]\n",
    "\n",
    "        batch_losses_sam = []\n",
    "        batch_losses_yolo = []\n",
    "\n",
    "        for image_name in batch_files:\n",
    "            image_path = images_dir + image_name\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            with torch.no_grad():\n",
    "                obj_results = yolov8_pretrained_model.predict(image_path, verbose=False, conf=0.2) \n",
    "                predictor.set_image(image)\n",
    "                \n",
    "            gt_masks = get_masks_from_image(images_dir, image_name)  \n",
    "            if len(obj_results[0]) == 0 or len(gt_masks) == 0:\n",
    "                continue\n",
    "      \n",
    "            input_boxes1 = obj_results[0].boxes.xyxy\n",
    "            input_boxes = input_boxes1.cpu().numpy()\n",
    "            input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "            input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "            sam_mask, yolo_masks = [], []\n",
    "            with torch.no_grad():\n",
    "                image_embedding=predictor.features\n",
    "                prompt_embedding=model_.prompt_encoder.get_dense_pe()\n",
    "                \n",
    "            non_resized_masks = obj_results[0].masks.data.cpu().numpy()\n",
    "            \n",
    "            for i in range(len(non_resized_masks)):\n",
    "                    yolo_masks.append(cv2.resize(non_resized_masks[i], image.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)) \n",
    "\n",
    "            for (boxes,) in batch_iterator(320, input_boxes): \n",
    "                with torch.no_grad():\n",
    "                    image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "                    prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "                    sparse_embeddings, dense_embeddings = mobile_sam_model.prompt_encoder(\n",
    "                        points=None,\n",
    "                        boxes=boxes,\n",
    "                        masks=None,)\n",
    "                    \n",
    "                if phase == 'val':\n",
    "                    with torch.no_grad():\n",
    "                        low_res_masks, _ = model_.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                else:\n",
    "                    low_res_masks, _ = model_.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "                # threshold_mask = torch.sigmoid(low_res_masks - model_.mask_threshold)\n",
    "                threshold_masks = normalize(threshold(low_res_masks, 0.0, 0)).to(device)\n",
    "                # plt.imshow(threshold_masks[0][0].detach().cpu().numpy())\n",
    "                # plt.show()\n",
    "                # plt.close()\n",
    "                sam_mask_pre = (low_res_masks > model_.mask_threshold)*1.0\n",
    "                sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "\n",
    "                # reshape gt_masks to same shape as predicted masks\n",
    "                gt_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in gt_masks], dim=0).to(device)\n",
    "                yolo_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in yolo_masks], dim=0).to(device)\n",
    "                segm_loss_sam = segm_loss_match(threshold_masks, gt_masks_tensor)\n",
    "                segm_loss_yolo = segm_loss_match(yolo_masks_tensor, gt_masks_tensor)\n",
    "                batch_losses_sam.append(segm_loss_sam)\n",
    "                batch_losses_yolo.append(segm_loss_yolo)\n",
    "                del sparse_embeddings, dense_embeddings, low_res_masks, gt_masks, \n",
    "                del gt_masks_tensor, yolo_masks_tensor, segm_loss_sam, segm_loss_yolo\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if phase == 'val':\n",
    "                    fig, axes = plt.subplots(1, 3, figsize=(18, 6)) \n",
    "                    \n",
    "                    # Plot 1: YOLO Masks\n",
    "                    axes[0].imshow(image)\n",
    "                    axes[0].set_title('YOLOv8n predicted Masks')\n",
    "                    show_masks(yolo_masks, axes[0], random_color=True)\n",
    "                    \n",
    "                    # Plot 2: Bounding Boxes\n",
    "                    image1 = cv2.resize(image, (1024, 1024))\n",
    "                    for bbox in boxes:\n",
    "                        x1, y1, x2, y2 = bbox.detach().cpu().numpy()\n",
    "                        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                        cv2.rectangle(image1, (x1, y1), (x2, y2), (0, 255, 0), 2) \n",
    "                    image1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "                    axes[1].imshow(image1_rgb)\n",
    "                    axes[1].set_title('YOLOv8n predicted Bboxes')\n",
    "                    \n",
    "                    # Plot 3: SAM Masks\n",
    "                    sam_masks_numpy = sam_mask[0].detach().cpu().numpy()\n",
    "                    axes[2].imshow(image)\n",
    "                    show_masks(sam_masks_numpy, axes[2], random_color=True)\n",
    "                    axes[2].set_title('MobileSAM predicted masks')\n",
    "                    plt.tight_layout() \n",
    "                    plt.savefig(f'./plots/combined_plots{i}.png')\n",
    "                    plt.show()\n",
    "\n",
    "        mean_loss_sam = torch.mean(torch.stack(batch_losses_sam))\n",
    "        mean_loss_yolo = torch.mean(torch.stack(batch_losses_yolo))\n",
    "        epoch_sam_loss.append(mean_loss_sam.item())\n",
    "        epoch_yolo_loss.append(mean_loss_yolo.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            mean_loss_sam.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # print(f\"{phase.capitalize()} Batch {batch_idx + 1}/{num_batches}, Segmentation loss SAM: {mean_loss_sam.item()}, YOLO: {mean_loss_yolo.item()}\")\n",
    "\n",
    "    print(f'Epoch {epoch}, {phase.capitalize()} Segmentation loss SAM: {np.mean(epoch_sam_loss)}. YOLO: {np.mean(epoch_yolo_loss)}')\n",
    "    return np.mean(epoch_sam_loss), np.mean(epoch_yolo_loss)\n",
    "    \n",
    "num_epochs = 1\n",
    "epoch_sam_loss_train, epoch_sam_loss_val, epoch_yolo_loss_train, epoch_yolo_loss_val = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_sam_loss_train, epoch_yolo_loss_train = run_epoch('train', train_image_files, train_dir, train_num_batches, mobile_sam_model, optimizer)\n",
    "    epoch_sam_loss_val, epoch_yolo_loss_val = run_epoch('val', valid_image_files, valid_dir, valid_num_batches, mobile_sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mobile_sam_model.state_dict(), f'sam_checkpoint_with_yolo.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional Roboflow export in VOC format given filenames\n",
    "export_to_Roboflow = False\n",
    "\n",
    "if export_to_Roboflow:\n",
    "    new_images_dir = '../XMM_OM_dataset/zscaled_512_stretched/'\n",
    "    new_image_files =  os.listdir(new_images_dir)\n",
    "    mobile_sam_model.eval()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for image_name in new_image_files[600:610]:\n",
    "                print('*****', new_images_dir, image_name)\n",
    "                image = cv2.imread(new_images_dir + image_name)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                obj_results = yolov8_pretrained_model.predict(new_images_dir + image_name, conf=0.2)  \n",
    "                predictor.set_image(image)\n",
    "            \n",
    "                if len(obj_results[0]) == 0:\n",
    "                    print(f\"No masks for {image_name}.\")\n",
    "                    plt.imshow(image)\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "                    continue\n",
    "        \n",
    "                input_boxes1 = obj_results[0].boxes.xyxy\n",
    "                input_boxes = input_boxes1.cpu().numpy()\n",
    "                input_boxes = predictor.transform.apply_boxes(input_boxes, predictor.original_size)\n",
    "                input_boxes = torch.from_numpy(input_boxes).to(device)\n",
    "                sam_mask, yolo_masks = [], []\n",
    "                image_embedding=predictor.features\n",
    "                prompt_embedding=mobile_sam_model.prompt_encoder.get_dense_pe()\n",
    "                non_resized_masks = obj_results[0].masks.data.cpu().numpy()\n",
    "                for i in range(len(non_resized_masks)):\n",
    "                        yolo_masks.append(cv2.resize(non_resized_masks[i], image.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)) \n",
    "            \n",
    "                for (boxes,) in batch_iterator(320, input_boxes): \n",
    "                    with torch.no_grad():\n",
    "                        image_embedding=image_embedding[0:boxes.shape[0],:,:,:]\n",
    "                        prompt_embedding=prompt_embedding[0:boxes.shape[0],:,:,:]\n",
    "                        sparse_embeddings, dense_embeddings = mobile_sam_model.prompt_encoder(\n",
    "                            points=None,\n",
    "                            boxes=boxes,\n",
    "                            masks=None,)\n",
    "                        low_res_masks, _ = mobile_sam_model.mask_decoder(\n",
    "                            image_embeddings=image_embedding,\n",
    "                            image_pe=prompt_embedding,\n",
    "                            sparse_prompt_embeddings=sparse_embeddings,\n",
    "                            dense_prompt_embeddings=dense_embeddings,\n",
    "                            multimask_output=False,\n",
    "                        )\n",
    "                        low_res_masks=predictor.model.postprocess_masks(low_res_masks, predictor.input_size, predictor.original_size)\n",
    "                        sam_mask_pre = (low_res_masks > mobile_sam_model.mask_threshold)*1.0\n",
    "                        sam_mask.append(sam_mask_pre.squeeze(1))\n",
    "        \n",
    "                        yolo_masks_tensor = torch.stack([torch.from_numpy(mask).unsqueeze(0) for mask in yolo_masks], dim=0)\n",
    "                        export_image_det_to_Roboflow(new_images_dir, image_name, sam_mask_pre, obj_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_image_det_to_Roboflow(input_dir, filename, masks, obj_results):\n",
    "    class_names = obj_results[0].names\n",
    "    class_labels = obj_results[0].boxes.data[:, -1].int().tolist()\n",
    "    \n",
    "    objects = []\n",
    "    print(masks.shape)\n",
    "    for i in range(len(masks)):\n",
    "        # masks[i]: [ 1, H, W]\n",
    "        mask_np = masks[i].detach().cpu().numpy()\n",
    "        plt.imshow(mask_np[0])\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        polygon = binary_image_to_polygon(mask_np[0])\n",
    "        print(len(polygon))\n",
    "        bbox = mask_to_bbox(mask_np)\n",
    "        if class_names[class_labels[i]] != 'star' and class_names[class_labels[i]] != 'other': # ignore stars and 'other' label\n",
    "            objects.append({\n",
    "                'name': class_names[class_labels[i]],\n",
    "                'bbox': bbox,\n",
    "                'segmentations': polygon[0]\n",
    "            })\n",
    "    if len(objects)>0:\n",
    "        create_annotation_SAM(filename=filename, width=512, height=512, depth=3, objects=objects) # generating xml file for VOC format\n",
    "        image_path = input_dir+filename\n",
    "        annotation_filename = filename.replace(\".png\", \".xml\")\n",
    "        upload_project.upload(image_path, annotation_filename, overwrite=False)\n",
    "        os.remove(annotation_filename)\n",
    "    else:\n",
    "        print(\"No objects after label filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(epoch_sam_loss_train))), epoch_sam_loss_train)\n",
    "plt.plot(list(range(len(epoch_sam_loss_val))), epoch_sam_loss_val)\n",
    "plt.title('SAM vs. YOLO mask loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('sam_vs_yolo_masks_loss.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# import os\n",
    "# import tarfile\n",
    "\n",
    "# second_directory_path = '../XMM_OM_dataset/zscaled_512_stretched/'\n",
    "# archive_name = 'imgs_512_50.tar.gz'\n",
    "\n",
    "# files_to_archive = []\n",
    "# for file in os.listdir(second_directory_path):\n",
    "#         files_to_archive.append(os.path.join(second_directory_path, file))\n",
    "\n",
    "# files_to_archive = files_to_archive[500:550]\n",
    "# # Create a tar.gz archive of the filtered files\n",
    "# with tarfile.open(archive_name, \"w:gz\") as tar:\n",
    "#     for file_path in files_to_archive:\n",
    "#         tar.add(file_path, arcname=os.path.basename(file_path))\n",
    "\n",
    "# print(f\"Archive {archive_name} created with {len(files_to_archive)} files.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_py311_cloned_dsa",
   "language": "python",
   "name": "env_py311_cloned"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
